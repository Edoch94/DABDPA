[
  {
    "objectID": "00_01_intro.html",
    "href": "00_01_intro.html",
    "title": "2  Introduction to Big data",
    "section": "",
    "text": "3 Who generates Big Data"
  },
  {
    "objectID": "00_01_intro.html#example-of-big-data-at-work",
    "href": "00_01_intro.html#example-of-big-data-at-work",
    "title": "2  Introduction to Big data",
    "section": "3.1 Example of Big Data at work",
    "text": "3.1 Example of Big Data at work\n\n\nBigdata example"
  },
  {
    "objectID": "00_01_intro.html#bottleneck",
    "href": "00_01_intro.html#bottleneck",
    "title": "2  Introduction to Big data",
    "section": "5.1 Bottleneck",
    "text": "5.1 Bottleneck\n\nProcessors process data\nHard drives store data\nWe need to transfer data from the disk to the processor"
  },
  {
    "objectID": "00_01_intro.html#solution",
    "href": "00_01_intro.html#solution",
    "title": "2  Introduction to Big data",
    "section": "5.2 Solution",
    "text": "5.2 Solution\n\nTransfer the processing power to the data\nMultiple distributed disks: each one holding a portion of a large dataset\nProcess in parallel different file portions from different disks"
  },
  {
    "objectID": "02_architectures.html",
    "href": "02_architectures.html",
    "title": "3  Big data architectures",
    "section": "",
    "text": "4 Definition of Big data architecture\nThe most frequently used big data architecture is the Lambda Architecture. The lambda architecture was proposed by Nathan Marz in 2011."
  },
  {
    "objectID": "02_architectures.html#definitions",
    "href": "02_architectures.html#definitions",
    "title": "3  Big data architectures",
    "section": "5.1 Definitions",
    "text": "5.1 Definitions\n\n\n\n\n\n\nFrom Nathan Marz\n\n\n\nThe past decade has seen a huge amount of innovation in scalable data systems. These include large-scale computation systems like Hadoop and databases such as Cassandra and Riak. These systems can handle very large amounts of data, but with serious trade-offs.Hadoop, for example, can parallelize large-scale batch computations on very large amounts of data, but the computations have high latency. You don’t use Hadoop for anything where you need low-latency results.\nNoSQL databases like Cassandra achieve their scalability by offering you a much more limited data model than you’re used to with something like SQL. Squeezing your application into these limited data models can be very complex. And because the databases are mutable, they’re not human-fault tolerant.\nThese tools on their own are not a panacea. But when intelligently used in conjunction with one another, you can produce scalable systems for arbitrary data problems with human-fault tolerance and a minimum of complexity. This is the Lambda Architecture you’ll learn throughout the book.\n\n\n\n\n\n\n\n\nFrom What is Lambda Architecture? article in Databricks website\n\n\n\nLambda architecture is a way of processing massive quantities of data (i.e. “Big Data”) that provides access to batch-processing and stream-processing methods with a hybrid approach.\nLambda architecture is used to solve the problem of computing arbitrary functions.\n\n\n\n\n\n\n\n\nFrom Lambda architecture article in Wikipedia\n\n\n\nLambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.\nThis approach to architecture attempts to balance latency, throughput, and fault tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation.\nLambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data."
  },
  {
    "objectID": "02_architectures.html#requirements",
    "href": "02_architectures.html#requirements",
    "title": "3  Big data architectures",
    "section": "5.2 Requirements",
    "text": "5.2 Requirements\nFault-tolerant against both hardware failures and human errors Support variety of use cases that include low latency querying as well as updates Linear scale-out capabilities Extensible, so that the system is manageable and can accommodate newer features easily"
  },
  {
    "objectID": "02_architectures.html#queries",
    "href": "02_architectures.html#queries",
    "title": "3  Big data architectures",
    "section": "5.3 Queries",
    "text": "5.3 Queries\n\\[\n\\textbf{query} = \\textbf{function}(\\textbf{all data})\n\\]\nSome query properties\n\nLatency: the time it takes to run a query\nTimeliness: how up to date the query results are (freshness and consistency)\nAccuracy: tradeoff between performance and scalability (approximations)\n\nIt is based on two data paths:\n\nCold path (batch layer)\n\nIt stores all of the incoming data in its raw form and performs batch processing on the data\nThe result of this processing is stored as batch views\n\nHot path (speed layer)\n\nIt analyzes data in real time\nThis path is designed for low latency, at the expense of accuracy"
  },
  {
    "objectID": "02_architectures.html#basic-structure",
    "href": "02_architectures.html#basic-structure",
    "title": "3  Big data architectures",
    "section": "5.4 Basic structure",
    "text": "5.4 Basic structure\n\n\nGeneral Lambda architecture\n\n\n\n\nAll data entering the system is dispatched to both the batch layer and the speed layer for processing\nThe batch layer has two functions:\n\nmanaging the master dataset(an immutable, append-only set of raw data), and\nto pre-compute the batch views\n\nThe serving layer indexes the batch views so that they can be queried in low-latency, ad-hoc way\nThe speed layer compensates for the high latency of updates to the serving layer and deals with recent data only\nAny incoming query can be answered by merging results from batch views and real-time views (e.g., the query looks at the serving layer for days until today, and looks at the speed layer for today’s data)."
  },
  {
    "objectID": "02_architectures.html#detailed-view",
    "href": "02_architectures.html#detailed-view",
    "title": "3  Big data architectures",
    "section": "5.5 Detailed view",
    "text": "5.5 Detailed view\n\n\nMore detailed of Lambda architecture\n\n\n\nStructure similar to the one described before\n\nData stream\nBatch layer\n\nimmutable data\nprecompute views\n\nReal-time layer\n\nprocess stream\nincrement views\n\nServing layer\n\n\n5.5.1 Possible instances\n\n\nMore detailed of Lambda architecture\n\n\n\nIn general, the technologies used are\n\nData stream: Kafka\nBatch layer:\n\nimmutable data: Hadoop HDFS\nprecompute views: Hadoop, Spark\nviews: Hive (it is a distributed relational database; SQL-like query language can be used)\n\nReal-time layer:\n\nprocess stream and increment views:\n\nSpark (it has a module available for managing stream data)\nApache Storm (pros: true real-time; cons: sometimes it approximates)\nFlink (used stream data analysis)\n\nviews: HBase, Cassandra, MongoDB\n\nServing layer\n\nIn general: choose the most suitable technology, but also be able to adapt on what’s available."
  },
  {
    "objectID": "03b_HDFS_clc.html",
    "href": "03b_HDFS_clc.html",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "",
    "text": "5 HDFS\nThe content of a HDFS file can be accessed by means of\nThe Hadoop programs are executed (submitted to the cluster) by using the hadoop command. It is a command line program, characterized by a set of parameters, such as"
  },
  {
    "objectID": "03b_HDFS_clc.html#user-folder",
    "href": "03b_HDFS_clc.html#user-folder",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "5.1 User folder",
    "text": "5.1 User folder\nEach user of the Hadoop cluster has a personal folder in the HDFS file system. The default folder of a user is\n\n/user/username"
  },
  {
    "objectID": "03b_HDFS_clc.html#command-line",
    "href": "03b_HDFS_clc.html#command-line",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "5.2 Command line",
    "text": "5.2 Command line\nThe hdfs command can be executed in a Linux shell to read/write/modify/delete the content of the distributed file system. The parameters/arguments of hdfs command are used to specify the operation to execute.\n\n5.2.1 Content of a folder\nTo list the content of a folder of the HDFS file system, use\n\nhdfs dfs -ls folder\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -ls /user/garza shows the content (list of files and folders) of the /user/garza folder.\nThe command hdfs dfs -ls . shows the content of the current folder (i.e., the content of /user/current_username).\nNotice that the mapping between the local linux user and the user of the cluster is based on\n\nA Kerberos ticket if Kerberos is active\nOtherwise the local linux user is considered\n\n\n\n\n\n\n5.2.2 Content of a file\nTo show the content of a file in the HDFS file system, use\n\nhdfs dfs -cat file_name\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -cat /user/garza/document.txt shows the content of the /user/garza/document.txt file stored in HDFS.\n\n\n\n\n\n5.2.3 Copy a file from local to HDFS\nTo copy a file from the local file system to the HDFS file system, use\n\nhdfs dfs -put local_file HDFS_path\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -put /data/document.txt /user/garza/ copies the local file /data/document.txt in the folder /user/garza in HDFS.\n\n\n\n\n\n5.2.4 Copy a file from HDFS to local\nTo copy a file from the HDFS file system to the local file system, use\n\nhdfs dfs -get HDFS_path local_file\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -get /user/garza/document.txt /data/ copies the HDFS file /user/garza/document.txt in the local file system folder /data/.\n\n\n\n\n\n5.2.5 Delete a file\nTo delete a file from the HDFS file system, use\n\nhdfs dfs -rm HDFS_path\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -rm /user/garza/document.txt delete from HDFS the file /user/garza/document.txt\n\n\n\n\n\n5.2.6 Other commands\nThere are many other linux-like commands, for example\n\nrmdir\ndu\ntail\n\nSee the HDFS commands guide for a complete list."
  },
  {
    "objectID": "03b_HDFS_clc.html#example-5",
    "href": "03b_HDFS_clc.html#example-5",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "6.1 Example",
    "text": "6.1 Example\nThe following command executes/submits a MapReduce application\nhadoop jar MyApplication.jar \\\nit.polito.bigdata.hadoop.DriverMyApplication \\\n1 inputdatafolder/ outputdatafolder/\n\nIt executes/submits the application contained in MyApplication.jar\nThe Driver Class is it.polito.bigdata.hadoop.DriverMyApplication\nThe application has three arguments\n\nNumber of reducers (1)\nInput data folder (inputdatafolder/)\nOutput data folder (outputdatafolder/)"
  },
  {
    "objectID": "03_intro_hadoop.html",
    "href": "03_intro_hadoop.html",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "",
    "text": "6 Motivations of Hadoop and Big data frameworks\nIt is scalable fault-tolerant distributed system for Big Data\nIt borrowed concepts/ideas from the systems designed at Google (Google File System for Google’s MapReduce). It is open source project under the Apache license, but there are also many commercial implementations (e.g., Cloudera, Hortonworks, MapR).\nThe MapReduce programming paradigm is based on the basic concepts of Functional programming. Actually, MapReduce “implements” a subset of functional programming, and, because of this, the programming model appears quite limited and strict: everything is based on two “functions” with predefined signatures, that are Map and Reduce."
  },
  {
    "objectID": "03_intro_hadoop.html#data-volumes",
    "href": "03_intro_hadoop.html#data-volumes",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "6.1 Data volumes",
    "text": "6.1 Data volumes\n\nThe amount of data increases every day\nSome numbers (∼2012):\n\nData processed by Google every day: 100+ PB\nData processed by Facebook every day: 10+ PB\n\nTo analyze them, systems that scale with respect to the data volume are needed\n\n\n\n\n\n\n\nExample: Google\n\n\n\n\n\nConsider this situation: you have to analyze 10 billion web pages, and the average size of a webpage is 20KB. So\n\nThe total size of the collection: 10 billion x 20KBs = 200TB\nAssuming the usage of HDD hard disk (read bandwidth: 150MB/sec), the time needed to read all web pages (without analyzing them) is equal to 2 million seconds (i.e., more than 15 days).\nAssuming the usage of SSD hard disk (read bandwidth: 550MB/sec), the time needed to read all web pages (without analyzing them) is equal to 2 million seconds (i.e., more than 4 days).\nA single node architecture is not adequate"
  },
  {
    "objectID": "03_intro_hadoop.html#failures",
    "href": "03_intro_hadoop.html#failures",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "6.2 Failures",
    "text": "6.2 Failures\nFailures are part of everyday life, especially in a data center. A single server stays up for 3 years (~1000 days). Statistically\n\nWith 10 servers: 1 failure every 100 days (~3 months)\nWith 100 servers: 1 failure every 10 days\nWith 1000 servers: 1 failure/day\n\nThe main sources of failures\n\nHardware/Software\nElectrical, Cooling, …\nUnavailability of a resource due to overload\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\nLALN data [DSN 2006]\n\nData for 5000 machines, for 9 years\nHardware failures: 60%, Software: 20%, Network 5%\n\nDRAM error analysis [Sigmetrics 2009]\n\nData for 2.5 years\n8% of DIMMs affected by errors\n\nDisk drive failure analysis [FAST 2007]\n\nUtilization and temperature major causes of failures\n\n\n\n\nFailure types\n\nPermanent (e.g., broken motherboard)\nTransient (e.g., unavailability of a resource due to overload)"
  },
  {
    "objectID": "03_intro_hadoop.html#network-bandwidth",
    "href": "03_intro_hadoop.html#network-bandwidth",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "6.3 Network bandwidth",
    "text": "6.3 Network bandwidth\nNetwork becomes the bottleneck if big amounts of data need to be exchanged between nodes/servers. Assuming a network bandwidth (in a data centre) equal to 10 Gbps, it means that moving 10 TB from one server to another would take more than 2 hours. So, data should be moved across nodes only when it is indispensable.\nInstead of moving data to the data centre, the code (i.e., programs) should be moved between the nodes: this approach is called Data Locality, and in this way very few MBs of code are exchanged between the severs, instead of huge amount of data."
  },
  {
    "objectID": "03_intro_hadoop.html#single-node-architecture",
    "href": "03_intro_hadoop.html#single-node-architecture",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "7.1 Single node architecture",
    "text": "7.1 Single node architecture\n\n\nSingle node architecture\n\n\n\n\n\n\n\nSingle node architecture: Machine Learning and Statistics\n\n\n\nSmall data: data can be completely loaded in main memory.\n\n\n\n\n\nSingle node architecture: “Classical” data mining”\n\n\n\nLarge data: data can not be completely loaded in main memory.\n\nLoad in main memory one chunk of data at a time, process it and store some statistics\nCombine statistics to compute the final result"
  },
  {
    "objectID": "03_intro_hadoop.html#cluster-architecture",
    "href": "03_intro_hadoop.html#cluster-architecture",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "7.2 Cluster architecture",
    "text": "7.2 Cluster architecture\nTo overcome the previously explained issues, a new architecture based on clusters of servers (i.e., data centres) has been devised. In this way:\n\nComputation is distributed across servers\nData are stored/distributed across servers\n\nThe standard architecture in the Big data context (∼2012) is based on\n\nCluster of commodity Linux nodes/servers (32 GB of main memory per node)\nGigabit Ethernet interconnection\n\n\n7.2.1 Commodity cluster architecture\n\n\nCommodity cluster architecture\n\n\n\nThe servers in each rack are very similar to each other, so that the servers would take the same time to process the data and none of them will become a bottleneck for the overall processing.\nNotice that\n\nIn each rack, the servers are directly connected with each other in pairs\nRacks are directly connected with each other in pairs"
  },
  {
    "objectID": "03_intro_hadoop.html#scalability",
    "href": "03_intro_hadoop.html#scalability",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "7.3 Scalability",
    "text": "7.3 Scalability\nCurrent systems must scale to address\n\nThe increasing amount of data to analyze\nThe increasing number of users to serve\nThe increasing complexity of the problems\n\nTwo approaches are usually used to address scalability issues\n\nVertical scalability (scale up)\nHorizontal scalability (scale out)\n\n\n7.3.1 Scale up vs. Scale out\n\nVertical scalability (scale up): add more power/resources (i.e., main memory, CPUs) to a single node (high-performing server). The cost of super-computers is not linear with respect to their resources: the marginal cost increases as the power/resources increase.\nHorizontal scalability (scale out): add more nodes (commodity servers) to a system. The cost scales approximately linearly with respect to the number of added nodes. But data center efficiency is a difficult problem to solve.\n\nFor data-intensive workloads, a large number of commodity servers is preferred over a small number of high-performing servers, since, at the same cost, it is possible to deploy a system that processes data more efficiently and is more fault-tolerant.\nHorizontal scalability (scale out) is preferred for big data applications, but distributed computing is hard: new systems hiding the complexity of the distributed part of the problem to developers are needed."
  },
  {
    "objectID": "03_intro_hadoop.html#cluster-computing-challenges",
    "href": "03_intro_hadoop.html#cluster-computing-challenges",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "7.4 Cluster computing challenges",
    "text": "7.4 Cluster computing challenges\n\nDistributed programming is hard\n\nProblem decomposition and parallelization\nTask synchronization\n\nTask scheduling of distributed applications is critical: assign tasks to nodes by trying to\n\nSpeed up the execution of the application\nExploit (almost) all the available resources\nReduce the impact of node failures\n\nDistributed data storage\n\nHow to store data persistently on disk and keep it available if nodes can fail? Redundancy is the solution, but it increases the complexity of the system.\n\nNetwork bottleneck\n\nReduce the amount of data send through the network by moving computation and code to data.\n\n\n\n\n\n\nDistributed computing history\n\n\n\n\n\nDistributed computing is not a new topic\n\nHPC (High-performance computing) ~1960\nGrid computing ~1990\nDistributed databases ~1990\n\nHence, many solutions to the mentioned challenges are already available, but we are now facing big data-driven problems: the former solutions are not adequate to address big data volumes."
  },
  {
    "objectID": "03_intro_hadoop.html#typical-big-data-problem",
    "href": "03_intro_hadoop.html#typical-big-data-problem",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "7.5 Typical Big data problem",
    "text": "7.5 Typical Big data problem\nThe typical way to address a Big Data problem (given a collection of historical data)\n\nIterate over a large number of records/objects\nExtract something of interest from each record/object\nAggregate intermediate results\nGenerate final output\n\nNotice that, if in the second step it is needed to have some kind of knowledge of what’s in the other records, this Big data framework is not the best solution: the computations on isolated records is not possible anymore, and so this whole architecture is not suitable.\nThe challenges:\n\nParallelization\nDistributed storage of large data sets (Terabytes, Petabytes)\nNode Failure management\nNetwork bottleneck\nDiverse input format (data diversity & heterogeneity)"
  },
  {
    "objectID": "03_intro_hadoop.html#hadoop-vs.-hpc",
    "href": "03_intro_hadoop.html#hadoop-vs.-hpc",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "8.1 Hadoop vs. HPC",
    "text": "8.1 Hadoop vs. HPC\nHadoop\n\nDesigned for Data intensive workloads\nUsually, no CPU demanding/intensive tasks\n\nHPC (High-performance computing)\n\nA supercomputer with a high-level computational capacity (performance of a supercomputer is measured in floating-point operations per second (FLOPS))\nDesigned for CPU intensive tasks\nUsually it is used to process “small” data sets"
  },
  {
    "objectID": "03_intro_hadoop.html#main-components",
    "href": "03_intro_hadoop.html#main-components",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "8.2 Main components",
    "text": "8.2 Main components\nCore components of Hadoop:\n\nDistributed Big Data Processing Infrastructure based on the MapReduce programming paradigm\n\nProvides a high-level abstraction view: programmers do not need to care about task scheduling and synchronization\nFault-tolerant: node and task failures are automatically managed by the Hadoop system\n\nHDFS (Hadoop Distributed File System)\n\nHigh availability distributed storage\nFault-tolerant\n\n\nHadoop virtualizes the file system, so that the interaction resembles a local file system, even if this case it spans on multiple disks on multiple servers.\nSo Hadoop is in charge of:\n\nsplitting the input files\nstore the data in different servers\nmanaging the reputation of the blocks\n\n\n\nHadoop main components\n\n\n\nNotice that, in this example, the number of replicas (i.e., the number of copies) of each block (e.g., \\(C_0\\), \\(C_1\\), \\(C_6\\), etc.) is equal to two. Multiple copies are needed to correctly manage server failures: two copies are never stored in the same server.\nNotice that, with 2 copies of the same file, the user is always sure that 1 failure can be managed with no interruptions in the data processing and without the risk of losing data. In general, the number of failures that and HDFS can sustain with no repercussions is equal to \\((\\textbf{number of copies})-1\\).\nWhen a failure occures, Hadoop immediately starts to create new copies of the data, to reach again the set number of replicas."
  },
  {
    "objectID": "03_intro_hadoop.html#distributed-big-data-processing-infrastructure",
    "href": "03_intro_hadoop.html#distributed-big-data-processing-infrastructure",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "8.3 Distributed Big data processing infrastructure",
    "text": "8.3 Distributed Big data processing infrastructure\nHadoop allows to separate the what from the how because Hadoop programs are based on the MapReduce programming paradigm:\n\nMapReduce abstracts away the “distributed” part of the problem (scheduling, synchronization, etc), so that programmers can focus on the what;\nthe distributed part (scheduling, synchronization, etc) of the problem is handled by the framework: the Hadoop infrastructure focuses on the how.\n\nBut an in-depth knowledge of the Hadoop framework is important to develop efficient applications: the design of the application must exploit data locality and limit network usage/data sharing."
  },
  {
    "objectID": "03_intro_hadoop.html#hdfs",
    "href": "03_intro_hadoop.html#hdfs",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "8.4 HDFS",
    "text": "8.4 HDFS\nHDFS is the standard Apache Hadoop distributed file system. It provides global file namespace, and stores data redundantly on multiple nodes to provide persistence and availability (fault-tolerant file system).\nThe typical usage pattern for Hadoop:\n\nhuge files (GB to TB);\ndata is rarely updated (create new files or append to existing ones);\nreads and appends are common, and random read/write operations are not performed.\n\nEach file is split in chunks (also called blocks) that are spread across the servers.\n\nEach chunck is replicated on different servers (usually there are 3 replicas per chunk), ensuring persistence and availability. To further increase persistence and availability, replicas are stored in different racks, if it possible.\nEach chunk contains a part of the content of one single file. It is not possible to have the content of two files in the same chunk/block\nTypically each chunk is 64-128 MB, and the chunk size is defined when configuring Hadoop.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\n2 files in 4 chunks\n\n\n\nEach square represents a chunk in the HDFS. Each chunk contains 64 MB of data, so file 1 (65 MB) sticks out by 1 MB from a single chunk, while file 2 (127 MB) does not completely fill two chunks. The empty chunk portions are not filled by any other file.\nSo, even if the total space occupied from the files would be 192 MB (3 chunks), the actual space they occupy is 256 (4 chunks): Hadoop does not allow two files to occupy the same chunk, so that two different processes would not try to access a block at the same time.\n\n\n\nThe Master node, (a.k.a., Name Nodes in HDFS) is a special node/server that\n\nStores HDFS metadata (e.g., the mapping between the name of a file and the location of its chunks)\nMight be replicated (to prevent stoppings due to the failure of the Master node)\n\nClient applications can access the file through HDFS APIs: they talk to the master node to find data/chuck servers associated with the file of interest, and then connect to the selected chunk servers to access data.\n\n\n\n\n\n\nHadoop ecosystem\n\n\n\nThe HDFS and the YARN scheduler are the two main components of Hadoop, however there are modules, and each project/system addresses one specific class of problems.\n\nHive: a distributed relational database, based on MapReduce, for querying data stored in HDFS by means of a query language based on SQL;\nHBase: a distributed column-oriented database that uses HDFS for storing data;\nPig: a data flow language and execution environment, based on MapReduce, for exploring very large datasets;\nSqoop: a tool for efficiently moving data from traditional relational databases and external flat file sources to HDFS;\nZooKeeper: a distributed coordination service, that provides primitives such as distributed locks.\n…\n\nThe integration of these components with Hadoop is not as good as the integration of the Spark components with Spark."
  },
  {
    "objectID": "03_intro_hadoop.html#word-count",
    "href": "03_intro_hadoop.html#word-count",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "9.1 Word count",
    "text": "9.1 Word count\n\n\n\n\n\n\n\n\nInput\nProblem\nOutput\n\n\n\n\na large textual file of words\ncount the number of times each distinct word appears in the file\na list of pairs word, number, counting the number of occurrences of each specific word in the input file\n\n\n\n\n9.1.1 Case 1: Entire file fits in main memory\nA traditional single node approach is probably the most efficient solution in this case. The complexity and overheads of a distributed system affects the performance when files are “small” (“small” depends on the available resources).\n\n\n9.1.2 Case 2: File too large to fit in main memory\nHow to split this problem in a set of (almost) independent sub-tasks, and execute them in parallel on a cluster of servers?\nAssuming that\n\nThe cluster has 3 servers\nThe content of the input file is: “Toy example file for Hadoop. Hadoop running example”\nThe input file is split into 2 chunks\nThe number of replicas is 1\n\n\n\nWord count solution\n\n\n\nThe problem can be easily parallelized:\n\nEach server processes its chunk of data and counts the number of times each word appears in its own chunk\n\nEach server can execute its sub-task independently from the other servers of the cluster: asynchronization is not needed in this phase\nThe output generated from each chunk by each server represents a partial result\n\nEach server sends its local (partial) list of pairs \\(<\\textbf{word}, \\textbf{number of occurrences in its chunk}>\\) to a server that is in charge of aggregating all local results and computing the global result. The server in charge of computing the global result needs to receive all the local (partial) results to compute and emit the final list: a synchronization operation is needed in this phase.\n\nAssume a more realistic situation\n\nThe file size is 100 GB and the number of distinct words occurring in it is at most 1000\nThe cluster has 101 servers\nThe file is spread across 100 servers (1 server is the Master node) and each of these servers contains one (different) chunk of the input file (i.e., the file is optimally spread across 100 servers, and so each server contains 1/100 of the file in its local hard drives)\n\n\n9.1.2.1 Complexity\n\nEach server reads 1 GB of data from its local hard drive (it reads one chunk from HDFS): the time needed to process the data is equal to a few seconds;\nEach local list consists of at most 1,000 pairs (because the number of distinct words is 1,000): each list consists of a few MBs;\nThe maximum amount of data sent on the network is 100 times the size of a local list (number of servers x local list size): the MBs that are moved through the network consists of some MBs.\n\nSo, the critical step is the first one: the result of this phase should be as small as possible, to reduce the data moving between nodes during the following phase.\nIs also the aggregating step parallelizable? Yes, in the sense that the key-value pairs associated with the same key are sent to the same server in order to apply the aggregating function. So, different servers work in parallel, computing the aggregations on different keys.\n\n\n9.1.2.2 Scalability\nScalability can be defined along two dimensions\n\nIn terms of data: given twice the amount of data, the word count algorithm takes approximately no more than twice as long to run. Each server has to process twice the data, and so execution time to compute local list is doubled.\nIn terms of resources: given twice the number of servers, the word count algorithm takes approximately no more than half as long to run. Each server processes half of the data, and execution time to compute local list is halved.\n\nWe are assuming that the time needed to send local results to the node in charge of computing the final result and the computation of the final result are considered negligible in this running example. However, notice that frequently this assumption is not true, indeed it depends on the complexity of the problem and on the ability of the developer to limit the amount of data sent on the network."
  },
  {
    "objectID": "03_intro_hadoop.html#mapreduce-approach-key-ideas",
    "href": "03_intro_hadoop.html#mapreduce-approach-key-ideas",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "9.2 MapReduce approach key ideas",
    "text": "9.2 MapReduce approach key ideas\n\nScale “out”, not “up”: increase the number of servers, avoiding to upgrade the resources (CPU, memory) of the current ones\nMove processing to data: the network has a limited bandwidth\nProcess data sequentially, avoid random access: seek operations are expensive. Big data applications usually read and analyze all input records/objects: random access is useless\n\n\n9.2.1 Data locality\nTraditional distributed systems (e.g., HPC) move data to computing nodes (servers). This approach cannot be used to process TBs of data, since the network bandwidth is limited So, Hadoop moves code to data: code (few KB) is copied and executed on the servers where the chunks of data are stored. This approach is based on “data locality”."
  },
  {
    "objectID": "03_intro_hadoop.html#hadoop-and-mapreduce-usage-scope",
    "href": "03_intro_hadoop.html#hadoop-and-mapreduce-usage-scope",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "9.3 Hadoop and MapReduce usage scope",
    "text": "9.3 Hadoop and MapReduce usage scope\nHadoop/MapReduce is designed for\n\nBatch processing involving (mostly) full scans of the input data\nData-intensive applications\n\nRead and process the whole Web (e.g., PageRank computation)\nRead and process the whole Social Graph (e.g., LinkPrediction, a.k.a. “friend suggestion”)\nLog analysis (e.g., Network traces, Smart-meter data)\n\n\nIn general, MapReduce can be used when the same function is applied on multiple records one at a time, and its result then has to be aggregated.\n\n\n\n\n\n\nWarning\n\n\n\nNotice that Hadoop/MapReduce is not the panacea for all Big Data problems. In particular, does not feet well\n\nIterative problems\nRecursive problems\nStream data processing\nReal-time processing"
  },
  {
    "objectID": "03_intro_hadoop.html#what-can-mapreduce-do",
    "href": "03_intro_hadoop.html#what-can-mapreduce-do",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "10.1 What can MapReduce do",
    "text": "10.1 What can MapReduce do\nSolving complex problems is difficult, however there are several important problems that can be adapted to MapReduce\n\nLog analysis\nPageRank computation\nSocial graph analysis\nSensor data analysis\nSmart-city data analysis\nNetwork capture analysis"
  },
  {
    "objectID": "03_intro_hadoop.html#building-blocks-map-and-reduce",
    "href": "03_intro_hadoop.html#building-blocks-map-and-reduce",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "10.2 Building blocks: Map and Reduce",
    "text": "10.2 Building blocks: Map and Reduce\nMapReduce is based on two main “building blocks”, which are the Map and Reduce functions.\n\nMap function: it is applied over each element of an input data set and emits a set of (key, value) pairs\nReduce function: it is applied over each set of (key, value) pairs (emitted by the Map function) with the same key and emits a set of (key, value) pairs. This is the final result."
  },
  {
    "objectID": "03_intro_hadoop.html#solving-the-word-count-problem",
    "href": "03_intro_hadoop.html#solving-the-word-count-problem",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "10.3 Solving the word count problem",
    "text": "10.3 Solving the word count problem\n\n\n\n\n\n\n\n\nInput\nProblem\nOutput\n\n\n\n\na large textual file of words\ncount the number of times each distinct word appears in the file\na list of pairs word, number, counting the number of occurrences of each specific word in the input file\n\n\n\nThe input textual file is considered as a list \\(L\\) of words\n\\[\nL = [\\text{toy}, \\text{example}, \\text{toy}, \\text{example}, \\text{hadoop}]\n\\]\n\n\nWord count running example\n\n\n\n\nMap phase: apply a function on each element of a list of key-value pairs (notice that the example above is not 100% correct: the elements of the list should also be key-value pairs);\nShuffle and sort phase: group by key; in this phase the key-value pairs having the same key are collected together in the same node, but no computation is performed;\nReduce phase: apply an aggregating function on each group; this step can be parallelized: one node may consider some keys, while another one considers others.\n\nA key-value pair \\((w, 1)\\) is emitted for each word \\(w\\) in \\(L\\).\nIn other words, the Map function \\(m\\) is \\[\nm(w) = (w, 1)\n\\]\nA new list of (key, value) pairs \\(L_m\\) is generated. Notice that, in this case the key-value pairs generated for each word is just one, but in other cases more than one key-value pair is generated from each element of the starting list.\nThen, the key-value pairs in \\(L_m\\) are aggregated by key (i.e., by word \\(w\\) in the example).\n\n10.3.1 Map\nIn the Map step, one group \\(G_w\\) is generated for each word \\(w\\). Each group \\(G_w\\) is a key-list pair \\[\n(w, [\\textbf{list of values}])\n\\] where \\([\\textbf{list of values}]\\) contains all the values of the pairs associated with the word \\(w\\).\nConsidering the example, \\(\\textbf{[list of values]}\\) is a list of \\([1, 1, 1, ...]\\), and, given a group \\(G_w\\), the number of ones in \\([1, 1, 1, ...]\\) is equal to the occurrences of word \\(w\\) in the input file.\nNotice that also the input of Map should be a list of key-value pairs. If a simple list of elements is passed to Map, Hadoop transforms the elements in key-value pairs, such that the value is equal to the element (e.g., the word) and the key is equal to the offset of the element in the input file.\n\n\n10.3.2 Reduce\nFor each group \\(G_w\\) a key-value pair is emitted as follows \\[\n(w,\\sum_{G_w}{[\\textbf{list of values}]})\n\\] So, the result of the Reduce function is \\(r(G_w) = (w,\\sum_{Gw}{[\\textbf{list of values}]})\\).\nThe resulting list of emitted pairs is the solution of the word count problem: in the list there is one pair (word \\(w\\), number of occurrences) for each word in our running example."
  },
  {
    "objectID": "03_intro_hadoop.html#mapreduce-phases",
    "href": "03_intro_hadoop.html#mapreduce-phases",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "10.4 MapReduce Phases",
    "text": "10.4 MapReduce Phases\n\n10.4.1 Map\nThe Map phase can be viewed as a transformation over each element of a data set. This transformation is a function \\(m\\) defined by developers, and it is invoked one time for each input element. Each invocation of \\(m\\) happens in isolation, allowing the parallelization of the application of \\(m\\) to each element of a data set in a straightforward manner.\nThe formal definition of Map is \\[\n(k_1, v_1) \\rightarrow [(k_2, v_2)]\n\\] Notice that\n\nSince the input data set is a list of key-value pairs, the argument of the Map function is a key-value pair; so, the Map function \\(N\\) times, where \\(N\\) is the number of input key-value pairs;\nThe Map function emits a list of key-value pairs for each input record, and the list can also be empty;\nNo data is moved between nodes during this phase.\n\n\n\n10.4.2 Reduce\nThe Reduce phase can be viewed as an aggregate operation. The aggregate function is a function \\(r\\) defined by developers, and it is invoked one time for each distinct key, aggregating all the values associated with it. Also the reduce phase can be performed in parallel and in isolation, since each group of key-value pairs with the same key can be processed in isolation.\nThe formal definition of Reduce is \\[\n(k_2, [v_2]) \\rightarrow [(k_3, v_3)]\n\\] Notice that\n\nThe Reduce function receives a list of values \\([v_2]\\) associated with a specific key \\(k_2\\); so the Reduce function is invoked \\(M\\) times, where \\(M\\) is the number of different keys in the input list;\nThe Reduce function emits a list of key-value pairs.\n\n\n\n10.4.3 Shuffle and sort\nThe shuffle and sort phase is always the same: it works by grouping the output of the Map phase by key. It does not need to be defined by developers, and it is already provided by the Hadoop system."
  },
  {
    "objectID": "03_intro_hadoop.html#data-structures",
    "href": "03_intro_hadoop.html#data-structures",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "10.5 Data structures",
    "text": "10.5 Data structures\nKey-value pair is the basic data structure in MapReduce. Keys and values can be integers, float, strings, …, in general they can also be (almost) arbitrary data structures defined by the designer. Notice that both input and output of a MapReduce program are lists of key-value pairs.\nAll in all, the design of MapReduce involves imposing the key-value structure on the input and output data sets. For example, in a collection of Web pages, input keys may be URLs and values may be their HTML content.\nIn many applications, the key part of the input data set is ignored. In other words, the Map function usually does not consider the key of its key-value pair argument (e.g., word count problem). Some specific applications exploit also the keys of the input data (e.g., keys can be used to uniquely identify records/objects)."
  },
  {
    "objectID": "03_intro_hadoop.html#pseudocode-of-word-count-solution-using-mapreduce",
    "href": "03_intro_hadoop.html#pseudocode-of-word-count-solution-using-mapreduce",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "10.6 Pseudocode of word count solution using MapReduce",
    "text": "10.6 Pseudocode of word count solution using MapReduce\nMap\ndef map(key, value):\n    '''\n    :key: offset of the word in the file\n    :value: a word of the input document\n    '''\n    return (value, 1)\nReduce\ndef reduce(key, values):\n    '''\n    :key: a word \n    :values: a list of integers\n    '''\n    occurrences = 0\n    for c in values:\n        occurrences = occurrences + c\n    return (key, occurrences)"
  },
  {
    "objectID": "04_hadoop_implementation.html",
    "href": "04_hadoop_implementation.html",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "",
    "text": "7 The components: summary\nThe programming language to use to give instructions to Hadoop is Java. A Hadoop MapReduce program consists of three main parts:\nEach part is “implemented” by means of a specific class.\nThe main characteristics Hadoop implementation of the MapReduce are the following\nAlways start from these templates. The parts of the code that should be changed to customize the Hadoop application are highlighted using notes.\nIn standard MapReduce applications, the (key,value) pairs emitted by the Mappers are sent to the Reducers through the network. However, some pre-aggregations could be performed to limit the amount of network data by using Combiners (also called “minireducers”).\nConsider the standard word count problem, and suppose that the input file is split in two input splits, hence, two Mappers are instantiated (one for each split).\nA combiner can be locally called on the output (key, value) pairs of each mapper (it works on data stored in the main-memory or on the local hard disks) to pre-aggregate data, reducing the data moving through the network.\nSo, in MapReduce applications that include Combiners after the Mappers, the (key,value) pairs emitted by the Mappers are analyzed in main-memory (or on the local disk) and aggregated by the Combiners. Each Combiner pre-aggregates the values associated with the pairs emitted by the Mappers of a cluster node, limiting the amount of network data generated by each cluster node.\nPersonalized Data Types are useful when the value of a key-value pair is a complex data type. Personalized Data Types are defined by implementing the org.apache.hadoop.io.Writable interface. The following methods must be implemented\nTo properly format the output of the job usually also the following method is “redefined”\nSuppose to be interested in complex values composed of two parts, such as a counter (int) and a sum (float). In this case, an ad-hoc Data Type can be used to implement this complex data type in Hadoop.\nThe configuration object is used to share the (basic) configuration of the Hadoop environment across the driver, the mappers and the reducers of the application/job. It stores a list of (property-name, property-value) pairs.\nAlso, personalized (property-name, property-value) pairs can be specified in the driver, and they can be used to share some parameters of the application with mappers and reducers. The personalized (property-name, property-value) pairs are useful to define shared small (constant) properties that are available only during the execution of the program. The driver sets these parameters, and Mappers and Reducers can access them, however they cannot modify them.\nHadoop provides a set of basic, built-in, counters to store some statistics about jobs, mappers, reducers, for example\nAlso other ad-hoc, user-defined, counters can be defined to compute global “statistics” associated with the goal of the application.\nIn some applications, all the work can be performed by the mapper(s) (e.g., record filtering applications): Hadoop allows executing Map-only jobs, avoiding the reduce phase, and also the shuffle and sort phase.\nThe output of the map job is directly stored in HDFS, since the set of pairs emitted by the map phase is already the final output.\nMapper classes are also characterized by a setup and a cleanup method, which are empty if they are not overridden."
  },
  {
    "objectID": "04_hadoop_implementation.html#driver-instance",
    "href": "04_hadoop_implementation.html#driver-instance",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "7.1 Driver (instance)",
    "text": "7.1 Driver (instance)\nThe Driver is characterized by the main() method, which accepts arguments from the command line (i.e., it is the entry point of the application). Also, it has a run() method\n\nIt configures the job\nIt submits the job to the Hadoop Cluster\nIt “coordinates” the work flow of the application\nIt runs on the client machine (i.e., it does not run on the cluster)"
  },
  {
    "objectID": "04_hadoop_implementation.html#mapper-instance",
    "href": "04_hadoop_implementation.html#mapper-instance",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "7.2 Mapper (instance)",
    "text": "7.2 Mapper (instance)\nThe Mapper is an instance of the Mapper class.\n\nIt “implements” the map phase;\nIt is characterized by the map() method, which processes the (key, value) pairs of the input file and emits (key, value) pairs and is invoked one time for each input (key, value) pair;\nIt runs on the cluster.\n\n\n\n\n\n\n\nTip\n\n\n\nThe Driver will try to create one Mapper instance for each input block, pushing to the maximum parallelization possible."
  },
  {
    "objectID": "04_hadoop_implementation.html#reducer-instance",
    "href": "04_hadoop_implementation.html#reducer-instance",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "7.3 Reducer (instance)",
    "text": "7.3 Reducer (instance)\nThe Reducer is an instance of the Reduce class.\n\nIt “implements” the reduce phase;\nIt is characterized by the reduce() method, which processes (key, [list of values]) pairs and emits (key, value) pairs and is invoked one time for each distinct key;\nIt runs on the cluster."
  },
  {
    "objectID": "04_hadoop_implementation.html#driver-class",
    "href": "04_hadoop_implementation.html#driver-class",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "8.1 Driver class",
    "text": "8.1 Driver class\nThe Driver class extends the org.apache.hadoop.conf.Configured class and implements the org.apache.hadoop.util.Tool interface 1.\nIt is possible to write a Driver class that does not extend Configured and does not implement Tool, however some low level details related to some command line parameters must be managed in that case.\nThe designer/developer implements the main() and run() methods.\nThe run() method configures the job, defining\n\nThe name of the Job\nThe job Input format\nThe job Output format\nThe Mapper class\n\nName of the class\nType of its input (key, value) pairs\nType of its output (key, value) pairs\n\nThe Reducer class\n\nName of the class\nType of its input (key, value) pairs\nType of its output (key, value) pairs\n\nThe Number of Reducers2"
  },
  {
    "objectID": "04_hadoop_implementation.html#mapper-class",
    "href": "04_hadoop_implementation.html#mapper-class",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "8.2 Mapper class",
    "text": "8.2 Mapper class\nThe Mapper class extends the\norg.apache.hadoop.mapreduce.Mapper\nclass which is a generic type/generic class with four type parameters:\n\ninput key type\ninput value type\noutput key type\noutput value type\n\nThe designer/developer implements the map() method, that is automatically called by the framework for each (key, value) pair of the input file.\nThe map() method\n\nProcesses its input (key, value) pairs by using standard Java code\nEmits (key, value) pairs by using the context.write(key, value) method"
  },
  {
    "objectID": "04_hadoop_implementation.html#reducer-class",
    "href": "04_hadoop_implementation.html#reducer-class",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "8.3 Reducer class",
    "text": "8.3 Reducer class\nThe Reducer class extends the\norg.apache.hadoop.mapreduce.Reducer\nclass, which is a generic type/generic class with four type parameters:\n\ninput key type\ninput value type\noutput key type\noutput value type\n\nThe designer/developer implements the reduce() method, that is automatically called by the framework for each (key, [list of values]) pair obtained by aggregating the output of the mapper(s).\nThe reduce() method\n\nProcesses its input (key, [list of values]) pairs by using standard Java code\nEmits (key, value) pairs by using the context.write(key, value) method"
  },
  {
    "objectID": "04_hadoop_implementation.html#data-types",
    "href": "04_hadoop_implementation.html#data-types",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "8.4 Data Types",
    "text": "8.4 Data Types\nHadoop has its own basic data types optimized for network serialization\n\norg.apache.hadoop.io.Text: like Java String\norg.apache.hadoop.io.IntWritable: like Java Integer\norg.apache.hadoop.io.LongWritable: like Java Long\norg.apache.hadoop.io.FloatWritable: like Java Float\n…\n\nThe basic Hadoop data types implement the org.apache.hadoop.io.Writable and org.apache.hadoop.io.WritableComparable interfaces\n\nAll classes (data types) used to represent keys are instances of WritableComparable: keys must be “comparable” for supporting the sort and shuffle phase\nAll classes (data types) used to represent values are instances of Writable: usually, they are also instances of WritableComparable even if it is not indispensable\n\nDevelopers can define new data types by implementing the org.apache.hadoop.io.Writable and/or org.apache.hadoop.io.WritableComparable interfaces, allowing to manage complex data types."
  },
  {
    "objectID": "04_hadoop_implementation.html#input-inputformat",
    "href": "04_hadoop_implementation.html#input-inputformat",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "8.5 Input: InputFormat",
    "text": "8.5 Input: InputFormat\nThe input of the MapReduce program is an HDFS file (or an HDFS folder), but the input of the Mapper is a set of (key, value) pairs.\nThe classes extending the org.apache.hadoop.mapreduce.InputFormat abstract class are used to read the input data and “logically transform” the input HDFS file in a set of (key, value) pairs.\nInputFormat describes the input-format specification for a MapReduce application and processes the input file(s). The InputFormat class is used to\n\nRead input data and validate the compliance of the input file with the expected input-format\nSplit the input file(s) into logical Input Splits, each of which is then assigned to an individual Mapper\nProvide the RecordReader implementation to be used to divide the logical input split in a set of (key,value) pairs (also called records) for the mapper\n\n\n\nGetting data to the Mapper\n\n\n\nInputFormat identifies partitions of the data that form an input split\n\nEach input split is a (reference to a) part of the input file processed by a single mapper\nEach split is divided into records, and the mapper processes one record (i.e., a (key,value) pair) at a time\n\nA set of predefined classes extending the InputFormat abstract class are available for standard input file formats\n\nTextInputFormat: InputFormat for plain text files\nKeyValueTextInputFormat: another InputFormat for plain text files\nSequenceFileInputFormat: an InputFormat for sequential/binary files\n…\n\n\n8.5.1 TextInputFormat\nTextInputFormat is an InputFormat for plain text files. Files are broken into lines, where either linefeed or carriage-return are used to signal end of line. One pair (key, value) is emitted for each line of the file:\n\nKey is the position (offset) of the line in the file\nValue is the content of the line\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nGetting data to the Mapper\n\n\n\n\n\n\n\n\n8.5.2 KeyValueTextInputFormat\nKeyValueTextInputFormat is an InputFormat for plain text files, where each line must have the format\nkey<separator>value\nand the default separator is tab (\\t).\nFiles are broken into lines, and either linefeed or carriage-return are used to signal end of line, and each line is split into key and value parts by considering the separator symbol/character.\nOne pair (key, value) is emitted for each line of the file\n\nKey is the text preceding the separator\nValue is the text following the separator\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nGetting data to the Mapper"
  },
  {
    "objectID": "04_hadoop_implementation.html#output-outputformat",
    "href": "04_hadoop_implementation.html#output-outputformat",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "8.6 Output: OutputFormat",
    "text": "8.6 Output: OutputFormat\nThe classes extending the org.apache.hadoop.mapreduce.OutputFormat abstract class are used to write the output of the MapReduce program in HDFS.\nA set of predefined classes extending the OutputFormat abstract class are available for standard output file formats\n\nTextOutputFormat: an OutputFormatfor plain text files\nSequenceFileOutputFormat: an OutputFormatfor sequential/binary files\n…\n\n\n8.6.1 TextOutputFormat\nTextOutputFormat is an OutputFormat for plain text files: for each output (key, value) pair, TextOutputFormat writes one line in the output file. In particular, the format of each output line is\n\"key\\tvalue\\n\""
  },
  {
    "objectID": "04_hadoop_implementation.html#driver",
    "href": "04_hadoop_implementation.html#driver",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "9.1 Driver",
    "text": "9.1 Driver\n/* Set package */ // # <1> \npackage it.polito.bigdata.hadoop.mypackage; \n\n/* Import libraries */\nimport java.io.IOException;\n\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\n/* Driver class */\npublic class MapReduceAppDriver extends Configured implements Tool { // # <2> \n    @Override\n    public int run(String[] args) throws Exception {\n        /* variables */\n        int exitCode; \n        //...\n\n        // Parse parameters\n        numberOfReducers = Integer.parseInt(args[0]); // Number of instances of the Reducer class\n        inputPath = new Path(args[1]); // Can be the path to a folder or to a file. If this is a folder path, the application will read all the files in it\n        outputDir = new Path(args[2]); // This is always the path to a folder\n\n        // Define and configure a new job \n        Configuration conf = this.getConf(); // Create a configuration object to design in it the application configuration\n        Job job = Job.getInstance(conf); // Creation of the job, that is the application instance\n\n        // Assign a name to the job\n        job.setJobName(\"My First MapReduce program\"); // # <3>\n\n        // Set path of the input file/folder (if it is a folder, the job reads all the files in the specified folder) for this job\n        FileInputFormat.addInputPath(job, inputPath);\n\n        // Set path of the output folder for this job\n        FileOutputFormat.setOutputPath(job, outputDir);\n\n        // Set input format\n        // TextInputFormat = textual files; the input types are (keys: LongWritable, values: text)\n        // KeyValueTextInputFormat = textual files; the input types are (keys: text, values: text)\n        job.setInputFormatClass(TextInputFormat.class); // This class also includes the information about the type of input data // # <4>\n\n        // Set job output format\n        job.setOutputFormatClass(TextOutputFormat.class); // # <5>\n\n        // Specify the class of the Driver for this job\n        job.setJarByClass(MapReduceAppDriver.class); // # <6>\n\n        // Set mapper class\n        job.setMapperClass(MyMapperClass.class); // # <7>\n        \n        // Set map output key and value classes; these are also the key - value types of the reduces class\n        job.setMapOutputKeyClass(output key type.class); // where type changes depending on the type (e.g., text, IntWritable) // # <8>\n        job.setMapOutputValueClass(output value type.class); // # <9>\n\n        // Set reduce class\n        job.setReducerClass(MyReducerClass.class); // # <10>\n\n        // Set reduce output key and value classes\n        job.setOutputKeyClass(output key type.class); // # <11>\n        job.setOutputValueClass(output value type.class); // # <12>\n\n        // Set number of reducers\n        job.setNumReduceTasks(numberOfReducers);\n\n        // Execute the job and wait for completion\n        if (job.waitForCompletion(true)==true) // with this method the application is run\n            exitCode=0;\n        else\n            exitCode=1;\n        return exitCode;\n    } // End of the run method\n\n    /* main method of the driver class */\n    public static void main(String args[]) throws Exception { // This part of the code is always the same\n        /* Exploit the ToolRunner class to \"configure\" and run the Hadoop application */\n        int res = ToolRunner.run(\n            new Configuration(), \n            new MapReduceAppDriver(), // # <13>\n            args\n        );\n        System.exit(res);\n    } // End of the main method\n} // End of public class MapReduceAppDriver\n\nmypackage\nMapReduceAppDriver\n\"My First MapReduce program\"\nTextInputFormat\nTextInputFormat\nMapReduceAppDriver\nMyMapperClass\noutput value type\noutput value type\nMyReducerClass\noutput value type\noutput value type\nMapReduceAppDriver"
  },
  {
    "objectID": "04_hadoop_implementation.html#mapper",
    "href": "04_hadoop_implementation.html#mapper",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "9.2 Mapper",
    "text": "9.2 Mapper\n/* Set package */\npackage it.polito.bigdata.hadoop.mypackage; // # <1>\n\n/* Import libraries */\nimport java.io.IOException;\n\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.io.*;\n\n/* Mapper Class */\nclass myMapperClass extends Mapper< // Mapper is a template // # <2>\n    MapperInputKeyType, // Input key type (must be consistent with the InputFormat class specified in the Driver) // # <3>\n    MapperInputValueType, // Input value type (must be consistent with the InputFormat class specified in the Driver) // # <4>\n    MapperOutputKeyType, // Output key type // # <5>\n    MapperOutputValueType // Output value type // # <6>\n>{\n/* Implementation of the map method */\n    protected void map(\n        MapperInputKeyType key, // Input key // # <7>\n        MapperInputValueType value, // Input value // # <8>\n        Context context // This is an object containing the write method, that has to be invoked to return the (key, value) pairs\n    ) throws IOException, InterruptedException {\n\n        /* \n        Process the input (key, value) pair and emit a set of (key,value) pairs. \n        context.write(...) is used to emit (key, value) pairs context.write(new outputkey, new outputvalue); \n        */ \n\n        context.write(new outputkey, new outputvalue); // # <9>\n        // Notice context.write(...) has to be invoked a number of times equal to the number of (key, value) pairs have to be returned. Even 0 times is accepted\n\n        // In the mapper instance also setup and cleanup methods can be implemented, but are not mandatory. Instead, the map method is mandatory\n\n    } // End of the map method\n} // End of class myMapperClass\n\nmypackage\nmyMapperClass\nMapperInputKeyType\nMapperInputValueType\nMapperOutputKeyType\nMapperOutputValueType\nMapperInputKeyType\nMapperInputValueType\noutputkey and outputvalue"
  },
  {
    "objectID": "04_hadoop_implementation.html#reducer",
    "href": "04_hadoop_implementation.html#reducer",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "9.3 Reducer",
    "text": "9.3 Reducer\n/* Set package */\npackage it.polito.bigdata.hadoop.mypackage; // # <1>\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.io.*;\n\n/* Reducer Class */\nclass myReducerClass extends Reducer< // Reducer is a template // # <2>\n    ReducerInputKeyType, // Input key type (must be consistent with the OutputKeyType of the Mapper) // # <3>\n    ReducerInputValueType, // Input value type (must be consistent with the OutputValueType of the Mapper) // # <4>\n    ReducerOutputKeyType, // Output key type (must be consistent with the OutputFormat class specified in the Driver) // # <5>\n    ReducerOutputValueType // Output value type (must be consistent with the OutputFormat class specified in the Driver) // # <6>\n>{\n    /* Implementation of the reduce method */\n    protected void reduce(\n        ReducerInputKeyType key, // Input key // # <7>\n        Iterable<ReducerInputValueType> values, // Input values (list of values). Notice that since this is an Iterable it is not possible to iterate over it more than once (it works like a Python generator). This is done because the iterable can read directly the list from the file system without moving and storing the data on the local server // # <8>\n        Context context\n    ) throws IOException, InterruptedException {\n\n        /* \n        Process the input (key, [list of values]) pair and emit a set of (key,value) pairs. \n        context.write(...) is used to emit (key, value) pairs context.write(new outputkey, new outputvalue); \n        */\n\n        context.write(new outputkey, new outputvalue); // # <9>\n        // Notice context.write(...) has to be invoked a number of times equal to the number of (key, value) pairs have to be returned. Even 0 times is accepted\n        // \"new\" has to be always specified\n\n    } // End of the reduce method\n} // End of class myReducerClass\n\nmypackage\nmyReducerClass\nReducerInputKeyType\nReducerInputValueType\nReducerOutputKeyType\nReducerOutputValueType\nReducerInputKeyType\nReducerInputValueType\noutputkey and outputvalue"
  },
  {
    "objectID": "04_hadoop_implementation.html#sec-wordcountexample",
    "href": "04_hadoop_implementation.html#sec-wordcountexample",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "9.4 Example of a MapReduce program in Hadoop: Word Count",
    "text": "9.4 Example of a MapReduce program in Hadoop: Word Count\nThe Word count problem consists of\n\nInput: (unstructured) textual file, where each line of the input file can contains a set of words\nOutput: number of occurrences of each word appearing in the input file\nParameters/arguments of the application:\n\nargs[0]: number of instances of the reducer\nargs[1]: path of the input file\nargs[2]: path of the output folder\n\n\n\n\n\n\n\n\nWord Count input and output examples\n\n\n\n\n\nInput file\nToy example file for Hadoop. Hadoop running example.\nOutput file\n(toy,1)\n(example,2)\n(file,1)\n(for,1)\n(hadoop,2)\n(running,1)\n\n\n\n\n9.4.1 Driver\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\n/* Driver class */\npublic class WordCount extends Configured implements Tool {\n    @Override\n    public intrun(String[] args) throws Exception {\n        Path inputPath;\n        Path outputDir;\n        int numberOfReducers;\n        int exitCode; \n        \n        // Parse input parameters\n        numberOfReducers = Integer.parseInt(args[0]);\n        inputPath = new Path(args[1]);\n        outputDir = new Path(args[2]);\n\n        // Define and configure a new job\n        Configuration conf = this.getConf();\n        Job job = Job.getInstance(conf); \n        \n        // Assign a name to the job\n        job.setJobName(\"WordCounter\");\n\n        // Set path of the input file/folder (if it is a folder, the job reads all the files in the specified folder) for this job\n        FileInputFormat.addInputPath(job, inputPath);\n\n        // Set path of the output folder for this job\n        FileOutputFormat.setOutputPath(job, outputDir);\n\n        // Set input format\n        // TextInputFormat = textual files \n        job.setInputFormatClass(TextInputFormat.class);\n\n        // Set job output format\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        // Specify the class of the Driver for this job\n        job.setJarByClass(WordCount.class);\n\n        // Set mapper class\n        job.setMapperClass(WordCountMapper.class);\n        \n        // Set map output key and value classes\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        // Set reduce class\n        job.setReducerClass(WordCountReducer.class);\n        \n        // Set reduce output key and value classes\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // Set number of reducers\n        job.setNumReduceTasks(numberOfReducers);\n\n        // Execute the job and wait for completion\n        if (job.waitForCompletion(true)==true)\n            exitCode=0;\n        else\n            exitCode=1;\n        return exitCode;\n    } // End of the run method\n\n    /* main method of the driver class */\n    public static void main(String args[]) throws Exception {\n\n        /* Exploit the ToolRunner class to \"configure\" and run the \n        Hadoop application */\n    \n        intres = ToolRunner.run(\n            new Configuration(), \n            new WordCount(), \n            args\n        );\n        System.exit(res);\n    } // End of the main method\n} // End of public class WordCount\n\n\n9.4.2 Mapper\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\n/* MapperClass */\nclass WordCountMapper extends Mapper<\n    LongWritable, // Input key type\n    Text, // Input value type\n    Text, // Output key type\n    IntWritable // Output value type\n>{\n    /* Implementation of the map method */\n    protected void map(\n        LongWritable key, // Input key type\n        Text value, // Input value type\n        Context context\n    ) throws IOException, InterruptedException {\n        // Split each sentence in words. Use whitespace(s) as delimiter\n        // The split method returns an array of strings\n        String[] words = value.toString().split(\"\\\\s+\");\n\n        // Iterate over the set of words\n        for(String word : words) {\n            // Transform word case\n            String cleanedWord = word.toLowerCase();\n\n            // emit one pair (word, 1) for each input word\n            context.write(new Text(cleanedWord), new IntWritable(1));\n        }\n    } // End map method\n} // End of class WordCountMapper\n\n\n9.4.3 Reducer\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\n/* Reducer Class */\nclass WordCountReducer extends Reducer<\nText, // Input key type\nIntWritable, // Input value type\nText, // Output key type\nIntWritable // Output value type\n>{\n    /* Implementation of the reduce method */\n    protected void reduce(\n        Text key, // Input key type\n        Iterable<IntWritable> values, // Input value type\n        Context context\n    ) throws IOException, InterruptedException{\n        int occurrances= 0;\n        \n        // Iterate over the set of values and sum them \n        for (IntWritable value : values) {\n            occurrances = occurrances+ value.get();\n        }\n\n        // Emit the total number of occurrences of the current word\n        context.write(key, new IntWritable(occurrances));\n    } // End reduce method\n} // End of class WordCountReducer"
  },
  {
    "objectID": "04_hadoop_implementation.html#combiner-instance",
    "href": "04_hadoop_implementation.html#combiner-instance",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "10.1 Combiner (instance)",
    "text": "10.1 Combiner (instance)\nThe Combiner is an instance of the org.apache.hadoop.mapreduce.Reducer class. Notice that there is not a specific combiner-template class.\n\nIt “implements” a pre-reduce phase that aggregates the pairs emitted in each node by Mappers\nIt is characterized by the reduce() method\nIt processes (key, [list of values]) pairs and emits (key, value) pairs\nIt runs on the cluster"
  },
  {
    "objectID": "04_hadoop_implementation.html#combiner-class",
    "href": "04_hadoop_implementation.html#combiner-class",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "10.2 Combiner class",
    "text": "10.2 Combiner class\nThe Combiner class extends the org.apache.hadoop.mapreduce.Reducer class, that is a generic type/generic class with four type parameters:\n\ninput key type\ninput value type\noutput key type\noutput value type\n\nCombiners and Reducers extend the same class, and the designer/developer implements the reduce() method also for the Combiner instances. The Combiner is automatically called by Hadoop for each (key, [list of values]) pair obtained by aggregating the local output of a Mapper.\nThe Combiner class is specified by using the job.setCombinerClass() method in the run method of the Driver (i.e., in the job configuration part of the code)."
  },
  {
    "objectID": "04_hadoop_implementation.html#example-adding-the-combiner-to-the-word-count-problem",
    "href": "04_hadoop_implementation.html#example-adding-the-combiner-to-the-word-count-problem",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "10.3 Example: adding the Combiner to the Word Count problem",
    "text": "10.3 Example: adding the Combiner to the Word Count problem\nConsider the word count problem (see Section 9.4 for details), to add the combiner to solution seen before:\n\nSpecify the combiner class in the Driver\nDefine the Combiner class. The reduce method of the combiner aggregates local pairs emitted by the mappers of a single cluster node, and emits partial results (local number of occurrences for each word) from each cluster node that is used to run our application.\n\n\n10.3.1 Specify combiner class in the Driver\nAdd the call to the combiner class in the Driver, before the return around line 68\n        // Set combiner class\n        job.setCombinerClass(WordCountCombiner.class);\n\n\n10.3.2 Define the Combiner class\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\n/* Combiner Class */\nclass WordCountCombiner extends Reducer<\n    Text, // Input key type\n    IntWritable, // Input value type\n    Text, // Output key type\n    IntWritable // Output value type\n>{\n/* Implementation of the reduce method */\n    protected void reduce(\n        Text key, // Input key type\n        Iterable<IntWritable> values, // Input value type\n        Context context\n    ) throws IOException, InterruptedException{\n        int occurrances= 0;\n        // Iterate over the set of values and sum them \n        for (IntWritable value : values) {\n            occurrances = occurrances+ value.get();\n        }\n        // Emit the total number of occurrences of the current word\n        context.write(key, new IntWritable(occurrances));\n    } // End reduce method\n} // End of class WordCountCombiner"
  },
  {
    "objectID": "04_hadoop_implementation.html#final-thoughts",
    "href": "04_hadoop_implementation.html#final-thoughts",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "10.4 Final thoughts",
    "text": "10.4 Final thoughts\nThe reducer and the combiner classes perform the same computation (the reduce method of the two classes is the same). Indeed, the developer/designer does not really need two different classes: he can simply specify that WordCountReducer is also the combiner class, for example by adding in the driver job.setCombinerClass(WordCountReducer.class). In 99% of the Hadoop applications the same class can be used to implement both combiner and reducer."
  },
  {
    "objectID": "04_hadoop_implementation.html#example-2",
    "href": "04_hadoop_implementation.html#example-2",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "11.1 Example",
    "text": "11.1 Example\n/* Set package */\n\npackage it.polito.bigdata.hadoop.combinerexample;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\npublic class SumAndCountWritable implements \norg.apache.hadoop.io.Writable {\n    /* Private variables */\n    private float sum = 0;\n    private int count = 0;\n\n    /* Methods to get and set private variables of the class */\n    public float getSum() {\n        return sum;\n    }\n\n    public void setSum(float sumValue) {\n        sum=sumValue;\n    }\n\n    public int getCount() {\n        return count;\n    }\n\n    public void setCount(int countValue) {\n        count=countValue;\n    }\n\n    /* Methods to serialize and deserialize the contents of the \n    instances of this class */\n    @Override /* Serialize the fields of this object to out */\n    public void write(DataOutput out) throws IOException {\n        out.writeFloat(sum);\n        out.writeInt(count);\n    }\n\n    @Override /* Deserialize the fields of this object from in */\n    public void readFields(DataInput in) throws IOException {\n        sum=in.readFloat();\n        count=in.readInt();\n    }\n\n    /* Specify how to convert the contents of the instances of this \n    class to a String\n    * Useful to specify how to store/write the content of this class\n    * in a textual file */\n    public String toString()\n    {\n        String formattedString=\n        new String(\"sum=\"+sum+\",count=\"+count);\n        return formattedString;\n    }\n}"
  },
  {
    "objectID": "04_hadoop_implementation.html#complex-keys",
    "href": "04_hadoop_implementation.html#complex-keys",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "11.2 Complex keys",
    "text": "11.2 Complex keys\nPersonalized Data Types can be used also to manage complex keys. In that case the Personalized Data Type must implement the org.apache.hadoop.io.WritableComparable interface, since keys must be\n\ncompared/sorted: it is possible by implementing the compareTo() method\nsplit in groups: it is possible by implementing the hashCode() method"
  },
  {
    "objectID": "04_hadoop_implementation.html#how-to-use-these-parameters",
    "href": "04_hadoop_implementation.html#how-to-use-these-parameters",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "12.1 How to use these parameters",
    "text": "12.1 How to use these parameters\nIn the driver\n\nRetrieve the configuration object\n\nConfiguration conf = this.getConf();\n\nSet personalized properties\n\nconf.set(\"property-name\", \"value\");\nIn the Mapper and/or Reducer\ncontext.getConfiguration().get(\"property-name\")\nThis method returns a String containing the value of the specified property."
  },
  {
    "objectID": "04_hadoop_implementation.html#user-defined-counters",
    "href": "04_hadoop_implementation.html#user-defined-counters",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "13.1 User-defined counters",
    "text": "13.1 User-defined counters\nUser-defined counters are defined by means of Java enum, and each application can define an arbitrary number of enums. The name of the enum is the group name,and each enum has a number of “fields”, which are the counter names.\nCounters are incremented in the Mappers and Reducers by using the increment() method\ncontext.getCounter(countername).increment(value);\nThe global/final value of each counter, which is available at the end of the job, is then stored/printed by the Driver (at the end of the execution of the job). Driver can retrieve the final values of the counters using the getCounters() and findCounter() methods.\nUser-defined counters can be also defined on the fly by using the method incrCounter(\"group name\", \"counter name\", value). Dynamic counters are useful when the set of counters is unknown at design time."
  },
  {
    "objectID": "04_hadoop_implementation.html#example-use-the-counters",
    "href": "04_hadoop_implementation.html#example-use-the-counters",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "13.2 Example: use the counters",
    "text": "13.2 Example: use the counters\nIn the driver, add\npublic static enum COUNTERS {\n    ERROR_COUNT,\n    MISSING_FIELDS_RECORD_COUNT\n}\nThis enum defines two counters\n\nCOUNTERS.ERROR_COUNT\nCOUNTERS.MISSING_FIELDS_RECORD_COUNT\n\nTo increment the COUNTERS.ERROR_COUNT counter in the mapper or the reducer, use\ncontext.getCounter(COUNTERS.ERROR_COUNT).increment(1);\nTo retrieve the final value of the COUNTERS.ERROR_COUNT counter in the driver, use\nCounter errorCounter = job.getCounters().findCounter(COUNTERS.ERROR_COUNT);"
  },
  {
    "objectID": "04_hadoop_implementation.html#implementation-of-a-map-only-job",
    "href": "04_hadoop_implementation.html#implementation-of-a-map-only-job",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "14.1 Implementation of a Map-only job",
    "text": "14.1 Implementation of a Map-only job\nTo implement a Map-only job\n\nImplement the map method\nSet the number of reducers to 0 during the configuration of the job (in the driver), writing\n\njob.setNumReduceTasks(0);"
  },
  {
    "objectID": "04_hadoop_implementation.html#setup-method",
    "href": "04_hadoop_implementation.html#setup-method",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "15.1 Setup method",
    "text": "15.1 Setup method\nThe setup method is called once for each mapper prior to the many calls of the map method. It can be used to set the values of in-mapper variables, which are used to maintain in-mapper statistics and preserve the state (locally for each mapper) within and across calls to the map method."
  },
  {
    "objectID": "04_hadoop_implementation.html#cleanup-method",
    "href": "04_hadoop_implementation.html#cleanup-method",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "15.2 Cleanup method",
    "text": "15.2 Cleanup method\nThe map method, invoked many times, updates the value of the in-mapper variables. Each mapper (each instance of the mapper class) has its own copy of the in-mapper variables.\nThe cleanup method is called once for each mapper after the many calls to the map method, and it can be used to emit (key,value) pairs based on the values of the in-mapper variables/statistics.\nAlso the reducer classes are characterized by a setup and a cleanup method.\n\nThe setup method is called once for each reducer prior to the many calls of the reduce method.\nThe cleanup method is called once for each reducer after the many calls of the reduce method.\n\nIn-MapperCombiners are a possible improvement over “standard” Combiners\n\nInitialize a set of in-mapper variables during the instance of the Mapper, in the setup method of the mapper;\nUpdate the in-mapper variables/statistics in the map method. Usually, no (key,value) pairs are emitted in the map method of an in-mapper combiner.\n\nAfter all the input records (input (key, value) pairs) of a mapper have been analyzed by the map method, emit the output (key, value) pairs of the mapper: (key, value) pairs are emitted in the cleanup method of the mapper based on the values of the in-mapper variables\nThe in-mapper variables are used to perform the work of the combiner in the mapper, allowing to improve the overall performance of the application. However, pay attention to the amount of used main memory: each mapper may use a limited amount of main-memory, hence in-mapper variables should be “small” (at least smaller than the maximum amount of memory assigned to each mapper)."
  },
  {
    "objectID": "04_hadoop_implementation.html#in-mapper-combiner-word-count-pseudocode",
    "href": "04_hadoop_implementation.html#in-mapper-combiner-word-count-pseudocode",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "15.3 In-Mapper combiner: Word count pseudocode",
    "text": "15.3 In-Mapper combiner: Word count pseudocode\nclass MAPPER\n    method setup\n        A = new AssociativeArray\n    method map(offset key, line l)\n        for all word w ∈ line l do\n            A{w} = A{w} + 1\n    method cleanup\n        for all word w in A do\n            EMIT(term w , count A{w})"
  },
  {
    "objectID": "04_hadoop_implementation.html#structure",
    "href": "04_hadoop_implementation.html#structure",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "16.1 Structure",
    "text": "16.1 Structure\n\nsrc folder: contains the source code. May contain subfolders, but the important point is that it must contain the java files\n\nDriverBigData.java\nMapperBigData.java\nReducerBigData.java\n\ntarget folder:\n\n.jar file: useful to run the application on the cluster. It’s the java archive that collects the three classes of the Hadoop application\n\npom.xml file: used to configure the Hadoop application"
  },
  {
    "objectID": "04_hadoop_implementation.html#how-to-run-the-project",
    "href": "04_hadoop_implementation.html#how-to-run-the-project",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "16.2 How to run the project",
    "text": "16.2 How to run the project\nUsing Eclipse\n\nselect the Driver .java file\nRight click\nClick “Run As”\nIf the arguments have already been set:\n\nClick “Java Application”\n\nOtherwise\n\nClick “Run Configurations”, to set the arguments\nGo to “Arguments” section, and write the arguments. The arguments are\n\nthe number of reducers: 2\nthe (relative) path of the input folder example_data\nthe (relative) path of the output folder example_data_output\n\n\n\n2 example_data example_data_output\nThe output files are\n\nan empty file “_SUCCESS”, if the application run successfully\none file for each reducer instance: the intersection between the sets of words in each file is empty, which means that all the same words were processed by the same Reducer. For this reason the output is always a folder and not a single file."
  },
  {
    "objectID": "04_hadoop_implementation.html#how-to-create-a-.jar-file-from-the-project",
    "href": "04_hadoop_implementation.html#how-to-create-a-.jar-file-from-the-project",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "16.3 How to create a .jar file from the project",
    "text": "16.3 How to create a .jar file from the project\nUsing Eclipse, to create a .jar file from the project to run the project on the cluster\n\nRight click on the project name (e.g., “MapReduceProject”)\nClick “Runs As”\nClick “Maven build…”\nIn “Goals” write “package”\nClick “Run”"
  },
  {
    "objectID": "04_hadoop_implementation.html#how-to-run-the-.jar-in-the-bigdatapolito-cluster",
    "href": "04_hadoop_implementation.html#how-to-run-the-.jar-in-the-bigdatapolito-cluster",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "16.4 How to run the .jar in the BigData@Polito cluster",
    "text": "16.4 How to run the .jar in the BigData@Polito cluster\n\nGo to https://jupyter.polito.it/ (i.e., the server gateway) and connect using the credentials\nCopy the .jar file on server\nUpload the input data in the HDFS\nUse the terminal to run the .jar, using the hadoop command\n\nhadoop jar Exercise1-1.0.0.jar \\\nit.polito.bigdata.hadoop.exercise1.DriverBigData \\\n2 example_data example_data_output\nIn this configuration there are 3 file systems\n\nThe local file system on the personal PC\nThe local file system on the gateway server\nThe distributed file system on the Hadoop cluster (the interface to manage it is https://bigdatalab.polito.it/hue)"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html",
    "href": "05_mapreduce_patterns_1.html",
    "title": "7  MapReduce patterns - 1",
    "section": "",
    "text": "8 Summarization Patterns\nSummarization Patterns are used to implement applications that produce top-level/summarized view of the data, such as\nAre used to select the subset of input records of interest"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html#numerical-summarizations",
    "href": "05_mapreduce_patterns_1.html#numerical-summarizations",
    "title": "7  MapReduce patterns - 1",
    "section": "8.1 Numerical summarizations",
    "text": "8.1 Numerical summarizations\nThe goal is to group records/objects by a key field(s) and calculate a numerical aggregate (e.g., average, max, min, standard deviation) per group, to provide a top-level view of large input data sets so that a few high-level statistics can be analyzed by domain experts to identify trends, anomalies, etc.\n\n8.1.1 Structure\n\nMappers output (key, value) pairs where\n\nkey is associated with the fields used to define groups;\nvalue is associated with the fields used to compute the aggregate statistics.\n\nReducers receive a set of numerical values for each “group-by” key and compute the final statistics for each “group”. Combiners can be used to speed up performances, if the computed statistic has specific properties (e.g., it is commutative and associative).\n\n\n\nNumerical summarization structure\n\n\n\nUse cases are\n\nWord count\nRecord count (per group)\nMin/Max/Count (per group)\nAverage/Median/Standard deviation (per group)"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html#inverted-index-summarization",
    "href": "05_mapreduce_patterns_1.html#inverted-index-summarization",
    "title": "7  MapReduce patterns - 1",
    "section": "8.2 Inverted index summarization",
    "text": "8.2 Inverted index summarization\nThe goal is to build an index from the input data to support faster searches or data enrichment: it maps terms to a list of identifiers to improve search efficiency.\n\n8.2.1 Structure\n\nMappers output (key, value) pairs where\n\nkey is the set of fields to index (a keyword)\nvalue is a unique identifier of the objects to associate with each “keyword”\n\nReducers receive a set of identifiers for each keyword and simply concatenate them\nCombiners are usually not useful when using this pattern, since there are no values to aggregate\n\n\n\nNumerical summarization structure\n\n\n\nA use case is a web search engine (word – List of URLs, i.e. Inverted Index)."
  },
  {
    "objectID": "05_mapreduce_patterns_1.html#counting-with-counters",
    "href": "05_mapreduce_patterns_1.html#counting-with-counters",
    "title": "7  MapReduce patterns - 1",
    "section": "8.3 Counting with counters",
    "text": "8.3 Counting with counters\nThe goal is to compute count summarizations of data sets to provide a top-level view of large data sets, so that few high-level statistics can be analyzed by domain experts to identify trends, anomalies, …\n\n8.3.1 Structure\n\nMappers process each input record and increment a set of counters\nThis is a map-only job: no reducers and no combiners have to be implemented\nThe results are stored/printed by the Driver of the application\n\n\n\nNumerical summarization structure\n\n\n\nUse cases\n\nCount number of records\nCount a small number of unique instances\nSummarizations"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html#filtering",
    "href": "05_mapreduce_patterns_1.html#filtering",
    "title": "7  MapReduce patterns - 1",
    "section": "9.1 Filtering",
    "text": "9.1 Filtering\nThe goal is to filter out input records that are not of interest/keep only the ones that are of interest, to focus the analysis of the records of interest. Indeed, depending on the goals of your application, frequently only a small subset of the input data is of interest for further analyses.\n\n9.1.1 Structure\nThe input of the mapper is a set of records\n\nKey = primary key\nValue = record\n\nMappers output one (key, value) pair for each record that satisfies the enforced filtering rule\n\nKey is associated with the primary key of the record\nValue is associated with the selected record\n\nReducers are useless in this pattern, since a map-only job is executed (number of reduce set to 0).\n\n\nNumerical summarization structure\n\n\n\nUse cases\n\nRecord filtering\nTracking events\nDistributed grep\nData cleaning"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html#top-k",
    "href": "05_mapreduce_patterns_1.html#top-k",
    "title": "7  MapReduce patterns - 1",
    "section": "9.2 Top K",
    "text": "9.2 Top K\nThe goal is to select a small set of top K records according to a ranking function to focus on the most important records of the input data set: frequently the interesting records are those ranking first according to a ranking function (i.e., most profitable items, outliers).\n\n9.2.1 Structure\n\n9.2.1.1 Mappers\nEach mapper initializes an in-mapper (local) top k list. k is usually small (e.g., 10), and the current (local) top k-records of each mapper(i.e., instance of the mapper class) can be stored in main memory\n\nThe initialization is performed in the setup method of the mapper\nThe map function updates the current in-mapper top k list\n\nThe cleanup method emits the k (key, value) pairs associated with the in-mapper local top k records\n\nKey is the “null key”\nValue is a in-mapper top k record\n\n\n\n9.2.1.2 Reducer\nA single reducer must be instantiated (i.e., one single instance of the reducer class). One single global view over the intermediate results emitted by the mappers to compute the final top k records. It computes the final top k list by merging the local lists emitted by the mappers. All input (key, value) pairs have the same key, hence the reduce method is called only once\n\n\nNumerical summarization structure\n\n\n\nUse cases\n\nOutlier analysis (based on a ranking function)\nSelect interesting data (based on a ranking function)"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html#distinct",
    "href": "05_mapreduce_patterns_1.html#distinct",
    "title": "7  MapReduce patterns - 1",
    "section": "9.3 Distinct",
    "text": "9.3 Distinct\nThe goal is to find a unique set of values/records, since in some applications duplicate records are useless (actually duplicated records are frequently useless).\n\nMappers emit one (key, value) pair for each input record\n\nKey = input record\nValue = null value\n\nReducers emit one (key, value) pair for each input (key, list of values) pair\n\nKey = input key, (i.e., input record)\nValue = null value\n\n\n\n\nNumerical summarization structure\n\n\n\nUse cases\n\nDuplicate data removal\nDistinct value selection"
  },
  {
    "objectID": "06_mapreduce_advanced_topics.html",
    "href": "06_mapreduce_advanced_topics.html",
    "title": "8  MapReduce and Hadoop Advanced Topics",
    "section": "",
    "text": "9 Multiple inputs\nIn some applications data are read from two or more datasets, also having different formats.\nHadoop allows reading data from multiple inputs (multiple datasets) with different formats by specifying one mapper for each input dataset. However, the key-value pairs emitted by the mappers must be consistent in terms of data types.\nIn the driver use the addInputPath method of the MultipleInputs class multiple times to\nIn some applications it could be useful to store the output key-value pairs of a MapReduce application in different files. Each file contains a specific subset of the emitted key-value pairs, based on some rules (usually this approach is useful for splitting and filtering operations), and each file name has a prefix that is used to specify the “content” of the file.\nAll the files are stored in one single output directory: there aren’t multiple output directories, but only multiple output files with different prefixes.\nHadoop allows specifying the prefix of the output files: the standard prefix is “part-” (see the content of the output directory of some of the previous applications).\nThe MultipleOutputs class is used to specify the prefixes of the output files\nSome applications need to share and cache (small) read-only files to perform efficiently their task. These files should be accessible by all nodes of the cluster in an efficient way, hence a copy of the shared/cached (HDFS) files should be available locally in all nodes used to run the application.\nDistributedCache is a facility provided by the Hadoop-based MapReduce framework to cache files (e.g., text, archives, jars needed by applications).\nIn image Figure 11.1, in HDFS disks there are the HDFS file(s) to be shared by means of the distributed cache, while on the disks there are local copies of the file(s) shared by means of the distributed cache. A local copy of the file(s) shared by means of the distributed cache is created only in the servers running the application that uses the shared file(s).\nIn the Driver of the application, the set of shared/cached files are specifiedby using the job.addCacheFile(path) method. During the initialization of the job, Hadoop creates a “local copy” of the shared/cached files in all nodes that are used to execute some tasks (mappers or reducers) of the job (i.e., of the running application). The shared/cache file is read by the mapper (or the reducer), usually in its setup method, since the shared/cached file is available locally in the used nodes/servers, its content can be read efficiently.\nThe efficiency of the distributed cache depends on the number of multiple mappers (or reducers) running on the same node/server: for each node a local copy of the file is copied during the initialization of the job, and the local copy of the file is used by all mappers (reducers) running on the same node/server.\nWithout the distributed cache, each mapper (reducer) should read, in the setup method, the shared HDFS file, hence, more time is needed because reading data from HDFS is more inefficient than reading data from the local file system of the node running the mappers (reducers)."
  },
  {
    "objectID": "06_mapreduce_advanced_topics.html#driver",
    "href": "06_mapreduce_advanced_topics.html#driver",
    "title": "8  MapReduce and Hadoop Advanced Topics",
    "section": "10.1 Driver",
    "text": "10.1 Driver\nUse the method MultipleOutputs.addNamedOutput multiple times in the Driver to specify the prefixes of the output files. This method has 4 parameter\n\nThe job object\nThe “name/prefix” of MultipleOutputs\nThe OutputFormat class\nThe key output data type class\nThe value output data type class\n\nCall this method one time for each “output file type”\n\n\n\n\n\n\nMultiple outputs example\n\n\n\n\n\nMultipleOutputs.addNamedOutput(\n    job, \n    \"hightemp\", \n    TextOutputFormat.class, \n    Text.class, \n    NullWritable.class\n);\n\nMultipleOutputs.addNamedOutput(\n    job, \n    \"normaltemp\", \n    TextOutputFormat.class, \n    Text.class, \n    NullWritable.class\n);\nThis example defines two types of output files\n\nThe first type of output files while have the prefix \"hightemp\"\nThe second type of output files while have the prefix \"normaltemp\""
  },
  {
    "objectID": "06_mapreduce_advanced_topics.html#map-only",
    "href": "06_mapreduce_advanced_topics.html#map-only",
    "title": "8  MapReduce and Hadoop Advanced Topics",
    "section": "10.2 Map-only",
    "text": "10.2 Map-only\nDefine a private MultipleOutputs variable in the mapper if the job is a map-only job (in the reducer otherwise)\nprivate MultipleOutputs<Text, NullWritable> mos = null;\nCreate an instance of the MultipleOutputs class in the setup method of the mapper (or in the reducer)\nmos = new MultipleOutputs<Text, NullWritable>(context);\nUse the write method of the MultipleOutputs object in the map method (or in the reduce method) to write the key-value pairs in the file of interest\n\n\n\n\n\n\nExample\n\n\n\n\n\nmos.write(\"hightemp\", key, value);\nThis example writes the current key-value pair in a file with the prefix \"hightemp-\"\nmos.write(\"normaltemp\", key, value);\nThis example writes the current key-value pair in a file with the prefix \"normaltemp-\"\n\n\n\nClose the MultipleOutputs object in the cleanup method of the mapper (or of the reducer)\n\n\n\n\n\n\nExample\n\n\n\n\n\nmos.close();"
  },
  {
    "objectID": "06_mapreduce_advanced_topics.html#example-2",
    "href": "06_mapreduce_advanced_topics.html#example-2",
    "title": "8  MapReduce and Hadoop Advanced Topics",
    "section": "11.1 Example",
    "text": "11.1 Example\n\n11.1.1 Driver\npublic int run(String[] args) throws Exception {\n    //...\n\n    // Add the shared/cached HDFS file in the \n    // distributed cache\n    job.addCacheFile(new Path(\"hdfs path/filename\").toUri());\n\n    //...\n}\n\n\n11.1.2 Mapper/Reducer\nprotected void setup(Context context) throws IOException, InterruptedException{\n\n    String line;\n    // Retrieve the (original) paths of the distributed files\n    URI[] urisCachedFiles = context.getCacheFiles();\n\n    // Read the content of the cached file and process it.\n    // In this example the content of the first shared file is opened.\n    BufferedReaderfile = new BufferedReader(\n        new FileReader(\n            new File(\n                new Path(urisCachedFiles[0].getPath()).getName()\n            )\n        )\n    );\n\n    // Iterate over the lines of the file\n    while ((line = file.readLine()) != null) {\n        // process the current line\n        //...\n    }\n    file.close();\n}\nNotice that .getName() retrieves the name of the file. The shared file is stored in the root of a local temporary folder (one for each server that is used to run the application) associated with the distributed cache. The path of the original folder is different from the one used to store the local copy of the shared file."
  },
  {
    "objectID": "07_mapreduce_patterns_2.html",
    "href": "07_mapreduce_patterns_2.html",
    "title": "9  MapReduce patterns - 2",
    "section": "",
    "text": "10 Data organization patterns\nData organization patterns are used to reorganize/split in subsets the input data\nThe output of an application based on an organization pattern is usually the input of another application(s)\nMetapatterns are used to organize the workflow of a complex application executing many jobs\nAre use to implement the join operators of the relational algebra (i.e., the join operators of traditional relational databases)\nThe explanation will focus on the natural join however, the pattern is analogous for the other types of joins (theta-, semi-, outer-join)."
  },
  {
    "objectID": "07_mapreduce_patterns_2.html#binning",
    "href": "07_mapreduce_patterns_2.html#binning",
    "title": "9  MapReduce patterns - 2",
    "section": "10.1 Binning",
    "text": "10.1 Binning\nThe goal is to organize/move the input records into categories, to partition a big data set into distinct, smaller data sets (“bins”) containing similar records. Each partition is usually the input of a following analysis.\nThis is done because the input data set contains heterogonous data, but each data analysis usually is focused only on a specific subsets of the data.\n\n10.1.1 Structure\nBinning is based on a Map-only job\n\nDriver sets the list of “bins/output files” by means of MultipleOutputs\nMappers select, for each input (key, value) pair, the output bin/file associated with it and emit a (key,value) in that file\n\nkey of the emitted pair is key of the input pair\nvalue of the emitted pair is value of the input pair\n\nNo Combiner or Reducer is used in this pattern\n\n\n\nBinning structure"
  },
  {
    "objectID": "07_mapreduce_patterns_2.html#shuffling",
    "href": "07_mapreduce_patterns_2.html#shuffling",
    "title": "9  MapReduce patterns - 2",
    "section": "10.2 Shuffling",
    "text": "10.2 Shuffling\nThe goal is to randomize the order of the data (records), for anonymization reasons or for selecting a subset of random data (records).\n\n10.2.1 Structure\n\nMappers emit one (key, value) for each input record\n\nkey is a random key (i.e., a random number)\nvalue is the input record\n\nReducers emit one (key, value) pair for each value in [list-of-values] of the input (key, [list-of-values]) pair\n\n\n\nShuffling structure"
  },
  {
    "objectID": "07_mapreduce_patterns_2.html#job-chaining",
    "href": "07_mapreduce_patterns_2.html#job-chaining",
    "title": "9  MapReduce patterns - 2",
    "section": "11.1 Job Chaining",
    "text": "11.1 Job Chaining\nThe goal is to execute a sequence of jobs (synchronizing them). Job chaining allows to manage the workflow of complex applications based on many phases (iterations). Each phase is associated with a different MapReduce Job (i.e., one sub-application), and the output of a phase is the input of the next one. This is done because real application are usually based on many phases.\n\n11.1.1 Structure\n\nThe (single) Driver contains the workflow of the application and executes the jobs in the proper order\nMappers, reducers, and combiners: each phase of the complex application is implement by a MapReduce Job (i.e., it is associated with a mapper, a reducer, and a combiner, if it is useful)\n\n\n\nJob chaining structure\n\n\n\nMore complex workflows, which execute jobs in parallel, can also be implemented, however, the synchronization of the jobs become more complex."
  },
  {
    "objectID": "07_mapreduce_patterns_2.html#reduce-side-natural-join",
    "href": "07_mapreduce_patterns_2.html#reduce-side-natural-join",
    "title": "9  MapReduce patterns - 2",
    "section": "12.1 Reduce side natural join",
    "text": "12.1 Reduce side natural join\nThe goal is to join the content of two relations (i.e., relational tables) when both tables are large.\n\n12.1.1 Structure\nThere are two mapper classes, that is one mapper class for each table. Mappers emit one (key, value) pair for each input record\n\nKey is the value of the common attribute(s)\nValue is the concatenation of the name of the table of the current record and the content of the current record\n\n\n\n\n\n\n\nExample\n\n\n\nSuppose join the following tables have to be joined\n\nUsers with schema userid, name, surname\nLikes with schema userid, movieGenre\n\nThe values userid=u1, name=Paolo, surname=Garza of the Users table will generate the pair\n(userid=u1, \"Users:name=Paolo, surname=Garza\")\nThe values userid=u1, movieGenre=horror of the Likes table will generate the pair\n(userid=u1, \"Likes:movieGenre=horror\")\n\n\nThe reducers iterate over the values associated with each key (value of the common attributes) and compute the “local natural join” for the current key. So, they generate a copy for each pair of values such that one record is a record of the first table and the other is the record of the other table.\n\n\n\n\n\n\nExample\n\n\n\nThe (key, [list of values]) pair\n(userid=u1, [\"User:name=Paolo, surname=Garza\", \"Likes:movieGenre=horror\", \"Likes:movieGenre=adventure\"]) \nwill generate the following output (key,value) pairs\n(userid=u1, \"name=Paolo, surname=Garza, genre=horror\")\n\n(userid=u1, \"name=Paolo, surname=Garza, genre=adventure\")\n\n\n\n\nReduce side natural join structure"
  },
  {
    "objectID": "07_mapreduce_patterns_2.html#map-side-natural-join",
    "href": "07_mapreduce_patterns_2.html#map-side-natural-join",
    "title": "9  MapReduce patterns - 2",
    "section": "12.2 Map side natural join",
    "text": "12.2 Map side natural join\nThe goal is to join the content of two relations (i.e., relational tables) when one table is large, while the other is small enough to be completely loaded in main memory (frequently one of the two tables is small).\n\n12.2.1 Structure\nThis is a Map-only job\n\nMapper class processes the content of the large table: it receives one input (key,value) pair for each record of the large table, and joins it with the “small” table.\n\nThe distributed cache approach is used to “provide” a copy of the small table to all mappers: each mapper performs the “local natural join” between the current record (of the large table) it is processing and the records of the small table (that is in the distributed cache).\nNotice that the content of the small table (file) is loaded in the main memory of each mapper during the execution of its setup method.\n\n\nMap side natural join structure"
  },
  {
    "objectID": "07_mapreduce_patterns_2.html#other-join-patterns",
    "href": "07_mapreduce_patterns_2.html#other-join-patterns",
    "title": "9  MapReduce patterns - 2",
    "section": "12.3 Other join patterns",
    "text": "12.3 Other join patterns\nThe SQL language is characterized by many types of joins\n\nTheta-join\nSemi-join\nOuter-join\n\nThe same patterns used for implementing the natural join can be used also for the other SQL joins.\nThe “local join” in the reducer of the reduce side natural join (in the mapper of the map side natural join) is replaced with the type of join of interest (theta-, semi-, or outer-join)."
  },
  {
    "objectID": "08_sql_operators_mapreduce.html",
    "href": "08_sql_operators_mapreduce.html",
    "title": "10  Relational Algebra Operations and MapReduce",
    "section": "",
    "text": "The relational algebra and the SQL language have many useful operators\n\nSelection\nProjection\nUnion, intersection, and difference\nJoin (see Join design patterns)\nAggregations and Group by (see the Summarization design patterns)\n\nThe MapReduce paradigm can be used to implement relational operators, however the MapReduce implementation is efficient only when a full scan of the input table(s) is needed (i.e., when queries are not selective and process all data). Selective queries, which return few tuples/records of the input tables, are usually not efficient when implemented by using a MapReduce approach.\nMost preprocessing activities involve relational operators (e.g., ETL processes in the data warehousing application context).\nRelations/Tables (also the big ones) can be stored in the HDFS distributed file system, broken in blocks and spread across the servers of the Hadoop cluster.\nNotice that in relational algebra, relations/tables do not contain duplicate records by definition, and this constraint must be satisfied by both the input and the output relations/tables.\n\n11 Selection\n\\[\n\\sigma_C(R)\n\\]\nSelection applies predicate (condition) \\(C\\) to each record of table \\(R\\), and produces a relation containing only the records that satisfy predicate \\(C\\).\nThe selection operator can be implemented by using the filtering pattern.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the table Courses\n\n\n\nCCode\nCName\nSemester\nProfID\n\n\n\n\nM2170\nComputer science\n1\nD102\n\n\nM4880\nDigital systems\n2\nD104\n\n\nF1401\nElectronics\n1\nD104\n\n\nF0410\nDatabases\n2\nD102\n\n\n\nFind the courses held in the second semester\n\\[\n\\sigma_{\\textbf{Semester}=2}(\\textbf{Courses})\n\\]\nThe resulting table is\n\n\n\nCCode\nCName\nSemester\nProfID\n\n\n\n\nM4880\nDigital systems\n2\nD104\n\n\nF0410\nDatabases\n2\nD102\n\n\n\n\n\n\nSelection is a map-only job, where each mapper analyzes one record at a time of its split and, if the record satisfies \\(C\\) then it emits a (key,value) pair with key=record and value=null, otherwise, it discards the record.\n\n\n12 Projection\n\\[\n\\pi_S(R)\n\\]\nProjection, for each record of table \\(R\\), keeps only the attributes in \\(S\\). It produces a relation with a schema equal to \\(S\\) (i.e., a relation containing only the attributes in \\(S\\)), an it removes duplicates, if any.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the table Professors\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nSmith\nElectronics\n\n\n\nFind the surnames of all professors.\n\\[\n\\pi_{\\textbf{PSurname}}(\\textbf{Professors})\n\\]\nThe resulting table is\n\n\n\nPSurname\n\n\n\n\nSmith\n\n\nJones\n\n\n\nNotice that duplicated values are removed.\n\n\n\nIn a projection\n\nEach mapper analyzes one record at a time of its split, and, for each record \\(r\\) in \\(R\\), it selects the values of the attributes in \\(S\\) and constructs a new record \\(r'\\), and emits a (key,value) pair with key=r' and value=null.\nEach reducer emits one (key, value) pair for each input (key, [list of values]) pair with key=r' and value=null.\n\n\n\n13 Union\n\\[\nR \\cup S\n\\]\nGiven that \\(R\\) and \\(S\\) have the same schema, an union produces a relation with the same schema of \\(R\\) and \\(S\\). There is a record \\(t\\) in the output of the union operator for each record \\(t\\) appearing in \\(R\\) or \\(S\\). Duplicated records are removed.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the tables DegreeCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\nand MasterCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD101\nRed\nElectronics\n\n\n\nFind information relative to the professors of degree courses or master’s degrees.\n\\[\n\\textbf{DegreeCourseProf} \\cup \\textbf{MasterCourseProf}\n\\]\nThe resulting table is\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\nD101\nRed\nElectronics\n\n\n\n\n\n\nIn a union\n\nMappers, for each input record \\(t\\) in \\(R\\), emit one (key, value) pair with key=t and value=null, and for each input record \\(t\\) in \\(S\\), emit one (key, value) pair with key=t and value=null.\nReducers emit one (key, value) pair for each input (key, [list of values]) pair with key=t and value=null (i.e., one single copy of each input record is emitted).\n\n\n\n14 Intersection\n\\[\nR \\cap S\n\\]\nGiven that \\(R\\) and \\(S\\) have the same schema, an intersection produces a relation with the same schema of \\(R\\) and \\(S\\). There is a record \\(t\\) in the output of the intersection operator if and only if \\(t\\) appears in both relations (\\(R\\) and \\(S\\)).\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the tables DegreeCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\nand MasterCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD101\nRed\nElectronics\n\n\n\nFind information relative to professors teaching both degree courses and master’s courses.\n\\[\n\\textbf{DegreeCourseProf} \\cap \\textbf{MasterCourseProf}\n\\]\nThe resulting table is\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\n\n\n\n\nIn an intersection\n\nMappers, for each input record \\(t\\) in \\(R\\), emit one (key, value) pair with key=t and value=\"R\", and For each input record \\(t\\) in \\(S\\), emit one (key, value) pair with key=t and value=\"S\".\nReducers emit one (key, value) pair with key=t and value=null for each input (key, [list of values]) pair with [list of values] containing two values. Notice that it happens if and only if both \\(R\\) and \\(S\\) contain \\(t\\).\n\n\n\n15 Difference\n\\[\nR-S\n\\]\nGiven that \\(R\\) and \\(S\\) have the same schema, a difference produces a relation with the same schema of \\(R\\) and \\(S\\). There is a record \\(t\\) in the output of the difference operator if and only if \\(t\\) appears in \\(R\\) but not in \\(S\\).\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the tables DegreeCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\nand MasterCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD101\nRed\nElectronics\n\n\n\nFind the professors teaching degree courses but not master’s courses.\n\\[\n\\textbf{DegreeCourseProf} - \\textbf{MasterCourseProf}\n\\]\nThe resulting table is\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\n\n\n\nIn a difference\n\nMappers, for each input record \\(t\\) in \\(R\\), emit one (key, value) pair with key=t and value=name of the relation (i.e., \\(R\\)). For each input record \\(t\\) in \\(S\\), emit one (key, value) pair with key=t and value=name of the relation (i.e., \\(S\\)). Notice that two mapper classes are needed: one for each relation.\nReducers emit one (key, value) pair with key=t and value=null for each input (key, [list of values]) pair with [list of values] containing only the value \\(R\\). Notice that it happens if and only if \\(t\\) appears in \\(R\\) but not in \\(S\\).\n\n\n\n16 Join\nThe join operators can be implemented by using the Join pattern, using the reduce side or the map side pattern depending on the size of the input relations/tables.\n\n\n17 Aggregations and Group by\nAggregations and Group by are implemented by using the Summarization pattern."
  },
  {
    "objectID": "10b_spark_submit_execute.html",
    "href": "10b_spark_submit_execute.html",
    "title": "11  How to submit/execute a Spark application",
    "section": "",
    "text": "12 Spark submit\nSpark programs are executed (submitted) by using the spark-submit command. It is a command line program, characterized by a set of parameters (e.g., the name of the jar file containing all the classes of the Spark application we want to execute, the name of the Driver class, the parameters of the Spark application).\nspark-submit has also two parameters that are used to specify where the application is executed."
  },
  {
    "objectID": "10b_spark_submit_execute.html#options-of-spark-submit---master",
    "href": "10b_spark_submit_execute.html#options-of-spark-submit---master",
    "title": "11  How to submit/execute a Spark application",
    "section": "12.1 Options of spark-submit: --master",
    "text": "12.1 Options of spark-submit: --master\n--master\nIt specifies which environment/scheduler is used to execute the application\n\n\n\n\n\n\n\nspark://host:port\nThe spark scheduler is used\n\n\nmesos://host:port\nThe mesos scheduler is used\n\n\nyarn\nThe YARN scheduler (i.e., the one of Hadoop)\n\n\nlocal\nThe application is executed exclusively on the local PC"
  },
  {
    "objectID": "10b_spark_submit_execute.html#options-of-spark-submit---deploy-mode",
    "href": "10b_spark_submit_execute.html#options-of-spark-submit---deploy-mode",
    "title": "11  How to submit/execute a Spark application",
    "section": "12.2 Options of spark-submit: --deploy-mode",
    "text": "12.2 Options of spark-submit: --deploy-mode\n--deploy-mode\nIt specifies where the Driver is launched/executed\n\n\n\n\n\n\n\nclient\nThe driver is launched locally (in the “local” PC executing spark-submit)\n\n\ncluster\nThe driver is launched on one node of the cluster\n\n\n\n\n\n\n\n\n\nDeployment mode: cluster and client\n\n\n\nIn cluster mode\n\nThe Spark driver runs in the ApplicationMaster on a cluster node.\nThe cluster nodes are used also to store RDDs and execute transformations and actions on the RDDs\nA single process in a YARN container is responsible for both driving the application and requesting resources from YARN.\nThe resources (memory and CPU) of the client that launches the application are not used.\n\n\n\nCluster deployment mode\n\n\n\nIn client mode\n\nThe Spark driver runs on the host where the job is submitted (i.e., the resources of the client are used to execute the Driver)\nThe cluster nodes are used to store RDDs and execute transformations and actions on the RDDs\nThe ApplicationMaster is responsible only for requesting executor containers from YARN.\n\n\n\nClient deployment mode"
  },
  {
    "objectID": "10b_spark_submit_execute.html#setting-the-executors",
    "href": "10b_spark_submit_execute.html#setting-the-executors",
    "title": "11  How to submit/execute a Spark application",
    "section": "12.3 Setting the executors",
    "text": "12.3 Setting the executors\nspark-submit allows specifying the characteristics of the executors\n\n\n\noption\nmeaning\ndefault value\n\n\n\n\n--num-executors\nThe number of executors\n2 executors\n\n\n--executor-cores\nThe number of cores per executor\n1 core\n\n\n--executor-memory\nMain memory per executor\n1 GB\n\n\n\nNotice that the maximum values of these parameters are limited by the configuration of the cluster."
  },
  {
    "objectID": "10b_spark_submit_execute.html#setting-the-drivers",
    "href": "10b_spark_submit_execute.html#setting-the-drivers",
    "title": "11  How to submit/execute a Spark application",
    "section": "12.4 Setting the drivers",
    "text": "12.4 Setting the drivers\nspark-submit allows specifying the characteristics of the driver\n\n\n\noption\nmeaning\ndefault value\n\n\n\n\n–driver-cores\nThe number of cores for the driver\n1 core\n\n\n–driver-memory\nMain memory for the driver\n1 GB\n\n\n\nAlso the maximum values of these parameters are limited by the configuration of the cluster when the --deploy-mode is set to cluster."
  },
  {
    "objectID": "10b_spark_submit_execute.html#execution-examples",
    "href": "10b_spark_submit_execute.html#execution-examples",
    "title": "11  How to submit/execute a Spark application",
    "section": "12.5 Execution examples",
    "text": "12.5 Execution examples\nThe following command submits a Spark application on a Hadoop cluster\nspark-submit \\ \n--deploy-mode cluster \\\n--master yarn MyApplication.py arguments\nIt executes/submits the application contained in MyApplication.py, and the application is executed on a Hadoop cluster based on the YARN scheduler. Notice that the Driver is executed in a node of cluster.\nThe following command submits a Spark application on a local PC\nspark-submit \\\n--deploy-mode client \\\n--master local MyApplication.py arguments\nIt executes/submits the application contained in MyApplication.py. Notice that the application is completely executed on the local PC:\n\nBoth Driver and Executors\nHadoop is not needed in this case\nOnly the Spark software is needed"
  },
  {
    "objectID": "10_intro_spark.html",
    "href": "10_intro_spark.html",
    "title": "12  Introduction to Spark",
    "section": "",
    "text": "13 Motivations\nSpark is based on a basic component (the Spark Core component) that is exploited by all the high-level data analytics components: this solution provides a more uniform and efficient solution with respect to Hadoop where many non-integrated tools are available. In this way, when the efficiency of the core component is increased also the efficiency of the other high-level components increases."
  },
  {
    "objectID": "10_intro_spark.html#mapreduce-and-spark-iterative-jobs-and-data-io",
    "href": "10_intro_spark.html#mapreduce-and-spark-iterative-jobs-and-data-io",
    "title": "12  Introduction to Spark",
    "section": "13.1 MapReduce and Spark iterative jobs and data I/O",
    "text": "13.1 MapReduce and Spark iterative jobs and data I/O\nIterative jobs, with MapReduce, involve a lot of disk I/O for each iteration and stage, and disk I/O is very slow (even if it is local I/O)\n\n\nIterative jobs\n\n\n\n\nMotivation: using MapReduce for complex iterative jobs or multiple jobs on the same data involves lots of disk I/O\nOpportunity: the cost of main memory decreased, hence, large main memories are available in each server\nSolution: keep more data in main memory, and that’s the basic idea of Spark\n\nSo an iterative job in MapReduce makes wide use of disk reading/writing\n\n\nIterative jobs in MapReduce\n\n\n\nInstead, an iterative job in Spark uses the main memory\n\n\nIterative jobs in Spark\n\n\n\nData (or at least part of it) are shared between the iterations by using the main memory , which is 10 to 100 times faster than disk.\nMoreover, to run multiple queries on the same data, in MapReduce the data must be read multiple times (once for each query)\n\n\nAnalysing the same data in MapReduce\n\n\n\nInstead, in Spark the data have to be loaded only once in the main memory\n\n\nAnalysing the same data in Spark\n\n\n\nIn other words, data are read only once from HDFS and stored in main memory, splitting of the data across the main memory of each server."
  },
  {
    "objectID": "10_intro_spark.html#resilient-distributed-data-sets-rdds",
    "href": "10_intro_spark.html#resilient-distributed-data-sets-rdds",
    "title": "12  Introduction to Spark",
    "section": "13.2 Resilient distributed data sets (RDDs)",
    "text": "13.2 Resilient distributed data sets (RDDs)\nIn Spark, data are represented as Resilient Distributed Datasets (RDDs), which are Partitioned/Distributed collections of objects spread across the nodes of a cluster, and are stored in main memory (when it is possible) or on local disk.\nSpark programs are written in terms of operations on resilient distributed data sets.\nRDDs are built and manipulated through a set of parallel transformations (e.g., map, filter, join) and actions (e.g., count, collect, save), and RDDs are automatically rebuilt on machine failure.\nThe Spark computing framework provides a programming abstraction (based on RDDs) and transparent mechanisms to execute code in parallel on RDDs\n\nIt hides complexities of fault-tolerance and slow machines\nIt manages scheduling and synchronization of the jobs"
  },
  {
    "objectID": "10_intro_spark.html#mapreduce-vs-spark",
    "href": "10_intro_spark.html#mapreduce-vs-spark",
    "title": "12  Introduction to Spark",
    "section": "13.3 MapReduce vs Spark",
    "text": "13.3 MapReduce vs Spark\n\n\n\n\nHadoop MapReduce\nSpark\n\n\n\n\nStorage\nDisk only\nIn-memory or on disk\n\n\nOperations\nMap and Reduce\nMap, Reduce, Join, Sample, …\n\n\nExecution model\nBatch\nBatch, interactive, streaming\n\n\nProgramming environments\nJava\nScala, Java, Python, R\n\n\n\nWith respect to MapReduce, Spark has a lower overhead for starting jobs and has less expensive shuffles.\nIn-memory RDDs can make a big difference in performance\n\n\nPerfomance comparison"
  },
  {
    "objectID": "10_intro_spark.html#resilient-distributed-data-sets-rdds-1",
    "href": "10_intro_spark.html#resilient-distributed-data-sets-rdds-1",
    "title": "12  Introduction to Spark",
    "section": "15.1 Resilient Distributed Data sets (RDDs)",
    "text": "15.1 Resilient Distributed Data sets (RDDs)\nRDDs are the primary abstraction in Spark: they are distributed collections of objects spread across the nodes of a clusters, which means that they are split in partitions, and each node of the cluster that is running an application contains at least one partition of the RDD(s) that is (are) defined in the application.\nRDDs are stored in the main memory of the executors running in the nodes of the cluster (when it is possible) or in the local disk of the nodes if there is not enough main memory. This allows to execute in parallel the code invoked on eah node: each executor of a worker node runs the specified code on its partition of the RDD.\n\n\n\n\n\n\nExample of an RDD split in 3 partitions\n\n\n\n\n\n\n\nExample of RDD splits\n\n\n\nMore partitions mean more parallelism.\n\n\n\nRDDs are immutable once constructed (i.e., the content of an RDD cannot be modified). Spark tracks lineage information to efficiently recompute lost data in case of failures of some executors: for each RDD, Spark knows how it has been constructed and can rebuilt it if a failure occurs. This information is represented by means of a DAG (Direct Acyclic Graph) connecting input data and RDDs.\nRDDs can be created\n\nby parallelizing existing collections of the hosting programming language (e.g., collections and lists of Scala, Java, Pyhton, or R): in this case the number of partition is specified by the user\nfrom (large) files stored in HDFS: in this case there is one partition per HDFS block\nfrom files stored in many traditional file systems or databases\nby transforming an existing RDDs: in this case the number of partitions depends on the type of transformation\n\nSpark programs are written in terms of operations on resilient distributed data sets\n\nTransformations: map, filter, join, …\nActions: count, collect, save, …\n\nTo summarize, in the Spark framework\n\nSpark manages scheduling and synchronization of the jobs\nSpark manages the split of RDDs in partitions and allocates RDDs partitions in the nodes of the cluster\nSpark hides complexities of fault-tolerance and slow machines (RDDs are automatically rebuilt in case of machine failures)"
  },
  {
    "objectID": "10_intro_spark.html#supported-languages",
    "href": "10_intro_spark.html#supported-languages",
    "title": "12  Introduction to Spark",
    "section": "16.1 Supported languages",
    "text": "16.1 Supported languages\nSpark supports many programming languages\n\nScala: this is the language used to develop the Spark framework and all its components (Spark Core, Spark SQL, Spark Streaming, MLlib, GraphX)\nJava\nPython\nR"
  },
  {
    "objectID": "10_intro_spark.html#structure-of-spark-programs",
    "href": "10_intro_spark.html#structure-of-spark-programs",
    "title": "12  Introduction to Spark",
    "section": "16.2 Structure of Spark programs",
    "text": "16.2 Structure of Spark programs\n\n\n\n\n\n\nSpark official terminology\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nApplication\nUser program built on Spark, consisting of a driver program and executors on the cluster.\n\n\nDriver program\nThe process running the main() function of the application and creating the SparkContext.\n\n\nCluster manager\nAn external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN).\n\n\nDeploy mode\nIt distinguishes where the driver process runs: in “cluster” mode (in this case the framework launches the driver inside of the cluster) or in “client” mode (in this case the submitter launches the driver outside of the cluster).\n\n\nWorker node\nAny node of the cluster that can run application code in the cluster.\n\n\nExecutor\nA process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them; each application has its own executors.\n\n\nTask\nA unit of work that will be sent to one executor.\n\n\nJob\nA parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect).\n\n\nStage\nEach job gets divided into smaller sets of tasks called stages, such that the output of one stage is the input of the next stage(s), except for the stages that compute (part of) the final result (i.e., the stages without output edges in the graph representing the workflow of the application). Indeed, the outputs of those stages is stored in HDFS or a database.\n\n\n\nThe shuffle operation is always executed between two stages\n\nData must be grouped/repartitioned based on a grouping criteria that is different with respect to the one used in the previous stage\nSimilar to the shuffle operation between the map and the reduce phases in MapReduce\nShuffle is a heavy operation\n\nSee the official documentation for more.\n\n\nThe Driver program contains the main method. It defines the workflow of the application, and accesses Spark through the SparkContext object, which represents a connection to the cluster.\nThe Driver program defines Resilient Distributed Datasets (RDDs) that are allocated in the nodes of the cluster, and invokes parallel operations on RDDs.\nThe Driver program defines\n\nLocal variables: these are standard variables of the Python programs\nRDDs: these are distributed variables stored in the nodes of the cluster\nThe SparkContext object, which allows to\n\ncreate RDDs\nsubmit executors (processes) that execute in parallel specific operations on RDDs\nperform Transformations and Actions\n\n\nThe worker nodes of the cluster are used to run your application by means of executors. Each executor runs on its partition of the RDD(s) the operations that are specified in the driver.\n\n\nDistributed execution of Spark\n\n\n\nRDDs are distributed across executors (each RDD is split in partitions that are spread across the available executors)."
  },
  {
    "objectID": "10_intro_spark.html#local-execution-of-spark",
    "href": "10_intro_spark.html#local-execution-of-spark",
    "title": "12  Introduction to Spark",
    "section": "16.3 Local execution of Spark",
    "text": "16.3 Local execution of Spark\nSpark programs can also be executed locally: local threads are used to parallelize the execution of the application on RDDs on a single PC. Local threads can be seen are “pseudo-worker” nodes, and a local scheduler is launched to run Spark programs locally. It is useful to develop and test the applications before deploying them on the cluster.\n\n\nDistributed execution of Spark"
  },
  {
    "objectID": "10_intro_spark.html#count-line-program",
    "href": "10_intro_spark.html#count-line-program",
    "title": "12  Introduction to Spark",
    "section": "17.1 Count line program",
    "text": "17.1 Count line program\nThe steps of this program are\n\ncount the number of lines of the input file, whose name is set to “myfile.txt”\nprint the results on the standard output\n\nfrom pyspark import SparkConf, SparkContext\n\nif __name__ == \"__main__\":\n\n    # Create a configuration object and\n    # set the name of the application\n    conf = SparkConf().setAppName(\"Spark Line Count\") # <1>\n\n    # Create a Spark Context object\n    sc = SparkContext(conf=conf) # <1>\n\n    # Store the path of the input file in inputfile\n    inputFile= \"myfile.txt\" # <1>\n\n    # Build an RDD of Strings from the input textual file\n    # Each element of the RDD is a line of the input file\n    linesRDD = sc.textFile(inputFile) # <2>\n\n    # Count the number of lines in the input file\n    # Store the returned value in the local variable numLines\n    numLines = linesRDD.count() # <1>\n\n    # Print the output in the standard output\n    print(\"NumLines:\", numLines)\n\n    # Close the Spark Context object\n    sc.stop()\n\nLocal Python variable: it is allocated in the main memory of the same process instancing the Driver.\nIt is allocated/stored in the main memory or in the local disk of the executors of the worker nodes.\n\n\nLocal variables can be used to store only “small” objects/data (i.e., the maximum size is equal to the main memory of the process associated with the Driver)\nRDDs are used to store “big/large” collections of objects/data in the nodes of the cluster\n\nIn the main memory of the worker nodes, when it is possible\nIn the local disks of the worker nodes, when it is necessary"
  },
  {
    "objectID": "10_intro_spark.html#word-count-program",
    "href": "10_intro_spark.html#word-count-program",
    "title": "12  Introduction to Spark",
    "section": "17.2 Word Count program",
    "text": "17.2 Word Count program\nIn the Word Count implemented by means of Spark\n\nThe name of the input file is specified by using a command line parameter (i.e., argv[1])\nThe output of the application (i.e., the pairs (word, number of occurrences) are stored in an output folder (i.e., argv[2]))\n\nNotice that there is no need to worry about the details.\nfrom pyspark import SparkConf, SparkContext\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Word count example\n    \"\"\"\n    inputFile= sys.argv[1]\n    outputPath = sys.argv[2]\n\n    # Create a configuration object and\n    # set the name of the application\n    conf = SparkConf().setAppName(\"Spark Word Count\")\n    \n    # Create a Spark Context object\n    sc = SparkContext(conf=conf)\n\n    # Build an RDD of Strings from the input textual file\n    # Each element of the RDD is a line of the input file\n    lines = sc.textFile(inputFile)\n\n    # Split/transform the content of lines in a\n    # list of words and store them in the words RDD\n    words = lines.flatMap(lambda line: line.split(sep=' '))\n    \n    # Map/transform each word in the words RDD\n    # to a pair/tuple (word,1) and store the result \n    # in the words_one RDD\n    words_one = words.map(lambda word: (word, 1))\n\n    # Count the num. of occurrences of each word.\n    # Reduce by key the pairs of the words_one RDD and store\n    # the result (the list of pairs (word, num. of occurrences)\n    # in the counts RDD\n    counts = words_one.reduceByKey(lambda c1, c2: c1 + c2)\n\n    # Store the result in the output folder\n    counts.saveAsTextFile(outputPath)\n\n    # Close/Stop the Spark Context object\n    sc.stop()"
  },
  {
    "objectID": "11_rdd_based_programming.html",
    "href": "11_rdd_based_programming.html",
    "title": "13  RDD based programming",
    "section": "",
    "text": "14 Spark Context\nThe “connection” of the driver to the cluster is based on the Spark Context object\nThe Spark Context object can be obtained also by using the SparkContext.getOrCreate(conf) method, whose only parameter is a configuration object. Notice that, if the SparkContext object already exists for this application, the current SparkContext object is returned, otherwise, a new SparkContext object is returned: there is always one single SparkContext object for each application.\nA Spark RDD is an immutable distributed collection of objects. Each RDD is split in partitions, allowing to parallelize the code based on RDDs (i.e., code is executed on each partition in isolation).\nRDDs can contain any type of Scala, Java, and Python objects, including user-defined classes.\nRDDs can be created\nRDD support two types of operations\nMany transformations (and some actions) are based on user provided functions that specify which transformation function must be applied on the elements of the input RDD. For example, the filter() transformation selects the elements of an RDD satisfying a user specified constraint, which is a Boolean function applied on each element of the input RDD.\nEach language has its own solution to pass functions to Spark’s transformations and actions. In Python, it is possible to use"
  },
  {
    "objectID": "11_rdd_based_programming.html#create-rdds-from-files",
    "href": "11_rdd_based_programming.html#create-rdds-from-files",
    "title": "13  RDD based programming",
    "section": "16.1 Create RDDs from files",
    "text": "16.1 Create RDDs from files\nTo built an RDD from an input textual file, use the textFile(name) method of the SparkContext class.\n\nThe returned RDD is an RDD of Strings associated with the content of the name textual file;\nEach line of the input file is associated with an object (a string) of the instantiated RDD;\nBy default, if the input file is an HDFS file the number of partitions of the created RDD is equal to the number of HDFS blocks used to store the file, in order to support data locality.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Build an RDD of strings from the input textual file\n# myfile.txt\n# Each element of the RDD is a line of the input file\ninputFile = \"myfile.txt\"\nlines = sc.textFile(inputFile)\nNotice that no computation occurs when sc.textFile() is invoked: Spark only records how to create the RDD, and the data is lazily read from the input file only when the data is needed (i.e., when an action is applied on lines, or on one of its “descendant” RDDs).\n\n\n\nTo build an RDD from a folder containing textual files, use the textFile(name) method of the SparkContext class.\n\nIf name is the path of a folder all files inside that folder are considered;\nThe returned RDD contains one string for each line of the files contained on the name folder.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Build an RDD of strings from all the files stored in\n# myfolder\n# Each element of the RDD is a line of the input files\ninputFolder = \"myfolder/\"\nlines = sc.textFile(inputFolder)\nNotice that all files inside myfolder are considered, also those without suffix or with a suffix different from “.txt”.\n\n\n\nTo set the (minimum) number of partitions, use the textFile(name, minPartitions) method of the SparkContext class.\n\nThis option can be used to increase the parallelization of the submitted application;\nFor the HDFS files, the number of partitions minPartitions must be greater than the number of blocks/chunks.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Build an RDD of strings from the input textual file\n# myfile.txt\n# The number of partitions is manually set to 4\n# Each element of the RDD is a line of the input file\ninputFile = \"myfile.txt“\nlines = sc.textFile(inputFile, 4)"
  },
  {
    "objectID": "11_rdd_based_programming.html#create-rdds-from-a-local-python-collection",
    "href": "11_rdd_based_programming.html#create-rdds-from-a-local-python-collection",
    "title": "13  RDD based programming",
    "section": "16.2 Create RDDs from a local Python collection",
    "text": "16.2 Create RDDs from a local Python collection\nAn RDD can be built from a local Python collection/list of local python objects using the parallelize(c) method of the SparkContext class\n\nThe created RDD is an RDD of objects of the same type of objects of the input python collection c\nIn the created RDD, there is one object for each element of the input collection\nSpark tries to set the number of partitions automatically based on your cluster’s characteristics\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Create a local python list\ninputList = ['First element', 'Second element', 'Third element']\n\n# Build an RDD of Strings from the local list.\n# The number of partitions is set automatically by Spark\n# There is one element of the RDD for each element\n# of the local list\ndistRDDList = sc.parallelize(inputList)\nNotice that no computation occurs when sc.parallelize(c) is invoked: Spark only records how to create the RDD, and the data is lazily read from the input file only when the data is needed (i.e., when an action is applied on distRDDlist, or on one of its “descendant” RDDs).\n\n\n\nWhen the parallelize(c) is invoked, Spark tries to set the number of partitions automatically based on the cluster’s characteristics, but the developer can set the number of partition by using the method parallelize(c, numSlices) of the SparkContext class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Create a local python list\ninputList = ['First element', 'Second element', 'Third element']\n\n# Build an RDD of Strings from the local list.\n# The number of partitions is set to 3\n# There is one element of the RDD for each element\n# of the local list\ndistRDDList = sc.parallelize(inputList, 3)"
  },
  {
    "objectID": "11_rdd_based_programming.html#save-rdds",
    "href": "11_rdd_based_programming.html#save-rdds",
    "title": "13  RDD based programming",
    "section": "16.3 Save RDDs",
    "text": "16.3 Save RDDs\nAn RDD can be easily stored in textual (HDFS) files using the saveAsTextFile(path) method of the RDD class\n\npath is the path of a folder\nThe method is invoked on the RDD to store in the output folder\nEach object of the RDD on which the saveAsTextFile method is invoked is stored in one line of the output files stored in the output folder, and there is one output file for each partition of the input RDD.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Store the content of linesRDD in the output folder\n# Each element of the RDD is stored in one line\n# of the textual files of the output folder\noutputPath=\"risFolder/\"\nlinesRDD.saveAsTextFile(outputPath)\nNotice that saveAsTextFile() is an action, hence Spark computes the content associated with linesRDD when saveAsTextFile() is invoked. Spark computes the content of an RDD only when that content is needed.\nMoreover, notice that the output folder contains one textual file for each partition of linesRDD, such that each output file contains the elements of one partition."
  },
  {
    "objectID": "11_rdd_based_programming.html#retrieve-the-content-of-rdds-and-store-it-local-python-variables",
    "href": "11_rdd_based_programming.html#retrieve-the-content-of-rdds-and-store-it-local-python-variables",
    "title": "13  RDD based programming",
    "section": "16.4 Retrieve the content of RDDs and store it local Python variables",
    "text": "16.4 Retrieve the content of RDDs and store it local Python variables\nThe content of an RDD can be retrieved from the nodes of the cluster and stored in a local python variable of the Driver using the collect() method of the RDD class.\nThe collect() method of the RDD class is invoked on the RDD to retrieve. It returns a local python list of objects containing the same objects of the considered RDD.\n\n\n\n\n\n\nWarning\n\n\n\nPay attention to the size of the RDD: large RDDs cannot be stored in a local variable of the Driver.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n# Retrieve the content of the linesRDD and store it\n# in a local python list\n# The local python list contains a copy of each\n# element of linesRDD\ncontentOfLines=linesRDD.collect()\n\n\n\n\n\n\n\ncontentOfLines\nLocal python variable: it is allocated in the main memory of the Driver process/task\n\n\nlinesRDD\nRDD of strings: it is distributed across the nodes of the cluster"
  },
  {
    "objectID": "11_rdd_based_programming.html#transformations",
    "href": "11_rdd_based_programming.html#transformations",
    "title": "13  RDD based programming",
    "section": "17.1 Transformations",
    "text": "17.1 Transformations\nTransformations are operations on RDDs that return a new RDD. This type of operation apply a transformation on the elements of the input RDD(s) and the result of the transformation is stored in/associated with a new RDD.\nRemember that RDDs are immutable, hence the content of an already existing RDD cannot be changed, and it only possible to applied a transformation on the content of an RDD and then store/assign the result in/to a new RDD.\nTransformations are computed lazily, which means that transformations are computed (executed) only when an action is applied on the RDDs generated by the transformation operations. When a transformation is invoked, Spark keeps only track of the dependency between the input RDD and the new RDD returned by the transformation, and the content of the new RDD is not computed.\nThe graph of dependencies between RDDs represents the information about which RDDs are used to create a new RDD. This is called lineage graph, and it is represented as a DAG (Directed Acyclic Graph): it is needed to compute the content of an RDD the first time an action is invoked on it, or to compute again the content of an RDD (or some of its partitions) when failures occur.\nThe lineage graph is also useful for optimization purposes: when the content of an RDD is needed, Spark can consider the chain of transformations that are applied to compute the content of the needed RDD and potentially decide how to execute the chain of transformations. In this way, Spark can potentially change the order of some transformations or merge some of them based on its optimization engine."
  },
  {
    "objectID": "11_rdd_based_programming.html#actions",
    "href": "11_rdd_based_programming.html#actions",
    "title": "13  RDD based programming",
    "section": "17.2 Actions",
    "text": "17.2 Actions\nActions are operations that\n\nreturn results to the Driver program (i.e., return local python variables). Pay attention to the size of the returned results because they must be stored in the main memory of the Driver program.\nwrite the result in the storage (output file/folder). The size of the result can be large in this case since it is directly stored in the (distributed) file system.\n\n\n17.2.1 Example of lineage graph (DAG)\nConsider the following code\nfrom pyspark import SparkConf, SparkContext\nimport sys\n\nif __name__ == \"__main__\":\nconf = SparkConf().setAppName(\"Spark Application\")\nsc = SparkContext(conf=conf)\n\n# Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n# Select the rows containing the word \"error\"\nerrorsRDD = inputRDD.filter(lambda line: line.find('error')>=0)\n\n# Select the rows containing the word \"warning\"\nwarningRDD = inputRDD.filter(lambda line: line.find('warning')>=0)\n\n# Union of errorsRDD and warningRDD\n# The result is associated with a new RDD: badLinesRDD\nbadLinesRDD = errorsRDD.union(warningRDD)\n\n# Remove duplicates lines (i.e., those lines containing\n# both \"error\" and \"warning\")\nuniqueBadLinesRDD = badLinesRDD.distinct()\n\n# Count the number of bad lines by applying\n# the count() action\nnumBadLines = uniqueBadLinesRDD.count()\n\n# Print the result on the standard output of the driver\nprint(\"Lines with problems:\", numBadLines)\n\n\nVisual representation of the DAG\n\n\n\nNotice that:\n\nThe application reads the input log file only when the count() action is invoked: this is the first action of the program;\nfilter(), union(), and distinct() are transformations, so they are computed lazily;\nAlso textFile() is computed lazily, however it is not a transformation because it is not applied on an RDD.\n\nSpark, similarly to an SQL optimizer, can potentially optimize the execution of some transformations; for instance, in this case the two filters + union + distinct can be potentially optimized and transformed in one single filter applying the constraint (i.e. The element contains the string “error” or “warning”). This optimization improves the efficiency of the application, but Spark can performs this kind of optimizations only on particular types of RDDs: Datasets and DataFrames."
  },
  {
    "objectID": "11_rdd_based_programming.html#example-based-on-the-filter-transformation",
    "href": "11_rdd_based_programming.html#example-based-on-the-filter-transformation",
    "title": "13  RDD based programming",
    "section": "18.1 Example based on the filter transformation",
    "text": "18.1 Example based on the filter transformation\n\nCreate an RDD from a log file;\nCreate a new RDD containing only the lines of the log file containing the word “error”. The filter() transformation applies the filter constraint on each element of the input RDD; the filter constraint is specified by means of a Boolean function that returns true for the elements satisfying the constraint and false for the others.\n\n\n18.1.1 Solution based on lambda expressions (lambda)\n# Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n# Select the rows containing the word \"error\"\nerrorsRDD = inputRDD.filter(lambda l: l.find('error')>=0)\n\n\n\n\n\n\n\nlambda l: l.find('error')>=0\nThis part of the code, which is based on a lambda expression, defines on the fly the function to apply. This part of the code is applied on each object of inputRDD: if it returns true then the current object is “stored” in the new errorsRDD RDD, otherwise the input object is discarded.\n\n\n\n\n\n18.1.2 Solution based on function (def)\n# Define the content of the Boolean function that is applied\n# to select the elements of interest\ndef myFunction(l):\n    if l.find('error')>=0: return True\n    else: return False\n\n# Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n# Select the rows containing the word “error”\nerrorsRDD = inputRDD.filter(myFunction)\n\n\n\n\n\n\n\ndef myFunction(l):\nWhen it is invoked, this function analyses the value of the parameter line and returns True if the string line contains the substring “error”. Otherwise, it returns False.\n\n\n.filter(myFunction)\nFor each object o in inputRDD, the myFunction function is automatically invoked. If myFunction returns True, then o is stored in the new RDD errorsRDD. Otherwise, o is discarded.\n\n\n\n\n\n18.1.3 Solution based on function (def)\n# Define the content of the Boolean function that is applied\n# to select the elements of interest\ndef myFunction(l):\n    return l.find('error')>=0\n\n# Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n# Select the rows containing the word “error”\nerrorsRDD = inputRDD.filter(myFunction)\n\n\n\n\n\n\n\nreturn l.find('error')>=0\nThis part of the code is the same used in the lambda-based solution.\n\n\n.filter(myFunction)\nFor each object o in inputRDD, the myFunction function is automatically invoked. If myFunction returns True, then o is stored in the new RDD errorsRDD. Otherwise, o is discarded.\n\n\n\n\n\n18.1.4 Solution comparison\nThe two solutions are more or less equivalent in terms of efficiency\n\n\n\n\n\n\n\nLambda function-based code (lambda)\nLocal user defined functions (local def)\n\n\n\n\nMore concise\nLess concise\n\n\nMore readable\nLess readable\n\n\nMulti-statement functions or statements that do not return a value are not supported\nMulti-statement functions or statements that do not return a value are supported\n\n\nCode cannot be reused\nCode can be reused (some functions are used in several applications)"
  },
  {
    "objectID": "11_rdd_based_programming.html#single-input-rdd-transformations",
    "href": "11_rdd_based_programming.html#single-input-rdd-transformations",
    "title": "13  RDD based programming",
    "section": "19.1 Single input RDD transformations",
    "text": "19.1 Single input RDD transformations\n\n19.1.1 Filter transformation\nThe filter transformation is applied on one single RDD and returns a new RDD containing only the elements of the input RDD that satisfy a user specified condition.\nThe filter transformation is based on the filter(f) method of the RDD class: a function f returning a Boolean value is passed to the filter method, where f contains the code associated with the condition that we want to apply on each element e of the input RDD. If the condition is satisfied then the call method returns true and the input element e is selected, otherwise, it returns false and the e element is discarded.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a log file\nCreate a new RDD containing only the lines of the log file containing the word “error”.\n\n# Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n# Select the rows containing the word “error”\nerrorsRDD = inputRDD.filter(lambda e: e.find('error')>=0)\nNotice that, in this case, the input RDD contains strings, hence, the implemented lambda function is applied on one string at a time and returns a Boolean value.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing only the values greater than 2.\n\nUsing lambda\n# Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList);\n\n# Select the values greater than 2\ngreaterRDD = inputRDD.filter(lambda num : num>2)\nNotice that the input RDD contains integers, hence, the implemented lambda function is applied on one integer at a time and returns a Boolean value.\nUsing def\n# Define the function to be applied in the filter transformation\ndef greaterThan2(num):\n    return num>2\n\n# Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList);\n\n# Select the values greater than 2\ngreaterRDD = inputRDD.filter(greaterThan2)\nIn this case, the function to apply is defined using def and then is passed to the filter transformation.\n\n\n\n\n\n19.1.2 Map transformation\nThe map transformation is used to create a new RDD by applying a function f on each element of the input RDD: the new RDD contains exactly one element y for each element x of the input RDD, in particular the value of y is obtained by applying a user defined function f on x (e.g., y= f(x)). The data type of y can be different from the data type of x.\nThe map transformation is based on the RDD map(f) method of the RDD class: a function f implementing the transformation is passed to the map method, where f contains the code that is applied over each element of the input RDD to create the elements of the returned RDD. For each input element of the input RDD exactly one single new element is returned by f.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing the surnames of a list of users (each line of the file contains one surname);\nCreate a new RDD containing the length of each surname.\n\n# Read the content of the input textual file\ninputRDD = sc.textFile(\"usernames.txt\")\n\n# Compute the lengths of the input surnames\nlenghtsRDD = inputRDD.map(lambda line: len(line))\nNotice that the input RDD is an RDD of strings, hence, also the input of the lambda function is a String. Instead, the new RDD is an RDD of Integers, since the lambda function returns a new Integer for each input element.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing the square of each input element.\n\n# Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n# Compute the square of each input element\nsquaresRDD = inputRDD.map(lambda element: element*element)\n\n\n\n\n\n19.1.3 FlatMap transformation\nThe flatMap transformation is used to create a new RDD by applying a function f on each element of the input RDD. The new RDD contains a list of elements obtained by applying f on each element x of the input RDD; in other words, the function f applied on an element x of the input RDD returns a list of values [y] (e.g., [y]= f(x)). [y] can be the empty list.\nThe final result is the concatenation of the list of values obtained by applying f over all the elements of the input RDD (i.e., the final RDD contains the concatenation of the lists obtained by applying f over all the elements of the input RDD).\nNotice that\n\nduplicates are not removed\nthe data type of y can be different from the data type of x\n\nThe flatMap transformation is based on the flatMap(f) method of the RDD class A function f implementing the transformation is passed to the flatMap method, where f contains the code that is applied on each element of the input RDD and returns a list of elements which will be included in the new returned RDD: for each element of the input RDD a list of new elements is returned by f. The returned list can be empty.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing a generic text (each line of the input file can contain many words).\nCreate a new RDD containing the list of words, with repetitions, occurring in the input textual document. In other words, each element of the returned RDD is one of the words occurring in the input textual file, and the words occurring multiple times in the input file appear multiple times, as distinct elements, also in the returned RDD.\n\n# Read the content of the input textual file\ninputRDD = sc.textFile(\"document.txt\")\n\n# Compute/identify the list of words occurring in document.txt\nlistOfWordsRDD = inputRDD.flatMap(lambda l: l.split(' '))\nIn this case the lambda function returns a “list” of values for each input element. However, notice that the new RDD (i.e., listOfWordsRDD) contains the “concatenation” of the lists obtained by applying the lambda function over all the elements of inputRDD: the new RDD is an RDD of strings and not an RDD of lists of strings.\n\n\n\n\n\n19.1.4 Distinct information\nThe distinct transformation is applied on one single RDD and returns a new RDD containing the list of distinct elements (values) of the input RDD.\nThe distinct transformation is based on the distinct() method of the RDD class, and no functions are needed in this case.\nA shuffle operation is executed for computing the result of the distinct transformation, so that data from different input partitions gets be compared to remove duplicates. The shuffle operation is used to repartition the input data: all the repetitions of the same input element are associated with the same output partition (in which one single copy of the element is stored), and a hash function assigns each input element to one of the new partitions.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing the names of a list of users (each line of the input file contains one name);\nCreate a new RDD containing the list of distinct names occurring in the input file. The type of the new RDD is the same of the input RDD.\n\n# Read the content of a textual input file\ninputRDD = sc.textFile(\"names.txt\")\n\n# Select the distinct names occurring in inputRDD\ndistinctNamesRDD = inputRDD.distinct()\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing only the distinct values appearing in the input RDD.\n\n# Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n# Compute the set of distinct words occurring in inputRDD\ndistinctIntRDD = inputRDD.distinct()\n\n\n\n\n\n19.1.5 SortBy transformation\nThe sortBy transformation is applied on one RDD and returns a new RDD containing the same content of the input RDD sorted in ascending order.\nThe sortBy transformation is based on the sortBy(keyfunc) method of the RDD class: each element of the input RDD is initially mapped to a new value by applying the specified function keyfunc, and then the input elements are sorted by considering the values returned by the invocation of keyfunc on the input values.\nThe sortBy(keyfunc, ascending) method of the RDD class allows specifying if the values in the returned RDD are sorted in ascending or descending order by using the Boolean parameter ascending\n\nascending set to True means ascending order\nascending set to False means descending order\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing the names of a list of users (each line of the input file contains one name);\nCreate a new RDD containing the list of users sorted by name (based on the alphabetic order).\n\n# Read the content of a textual input file\ninputRDD = sc.textFile(\"names.txt\")\n\n# Sort the content of the input RDD by name.\n# Store the sorted result in a new RDD\nsortedNamesRDD = inputRDD.sortBy(lambda name: name)\nNotice that each input element of the lambda expression is a string. The goal is sorting the input names (strings) in alphabetic order, which is the standard sort order for strings. For this reason the lambda function returns the input strings without modifying them.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD from a textual file containing the names of a list of users (each line of the input file contains one name);\nCreate a new RDD containing the list of users sorted by the length of their name (i.e., the sort order is based on len(name)).\n\n# Read the content of a textual input file\ninputRDD = sc.textFile(\"names.txt\")\n\n# Sort the content of the input RDD by name.\n# Store the sorted result in a new RDD\nsortedNamesLenRDD = inputRDD.sortBy(lambda name: len(name))\nIn this case, each input element is a string but we are interested in sorting the input names (strings) by length (integer), which is not the standard sort order for strings. For this reason the lambda function returns the length of each input string, and the sort operation is performed on the returned integer values (the lengths of the input names).\n\n\n\n\n\n19.1.6 Sample transformation\nThe sample transformation is applied on one single RDD and returns a new RDD containing a random sample of the elements (values) of the input RDD.\nThe sample transformation is based on the sample(withReplacement, fraction) method of RDD class:\n\nwithReplacement specifies if the random sample is with replacement (True) or not (False);\nfraction specifies the expected size of the sample as a fraction of the input RDD’s size (values in the range \\([0, 1]\\)).\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing a set of sentences (each line of the file contains one sentence);\nCreate a new RDD containing a random sample of sentences, using the “without replacement” strategy and setting fraction to \\(0.2\\) (i.e., \\(20%\\)).\n\n# Read the content of a textual input file\ninputRDD = sc.textFile(\"sentences.txt\")\n\n# Create a random sample of sentences\nrandomSentencesRDD = inputRDD.sample(False,0.2)\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing a random sample of the input values, using the “replacement” strategy and setting fraction to \\(0.2\\) (i.e., \\(20%\\)).\n\n# Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n# Create a sample of the inputRDD\nrandomSentencesRDD = inputRDD.sample(True,0.2)"
  },
  {
    "objectID": "11_rdd_based_programming.html#set-transformations",
    "href": "11_rdd_based_programming.html#set-transformations",
    "title": "13  RDD based programming",
    "section": "19.2 Set transformations",
    "text": "19.2 Set transformations\nSpark provides also a set of transformations that operate on two input RDDs and return a new RDD. Some of them implement standard set transformations:\n\nUnion\nIntersection\nSubtract\nCartesian\n\nAll these transformations have\n\nTwo input RDDs: one is the RDD on which the method is invoked, while the other RDD is passed as parameter to the method\nOne output RDD\n\nAll the involved RDDs have the same data type when union, intersection, or subtract are used, instead mixed data types can be used with the cartesian transformation.\n\n19.2.1 Union transformation\nThe union transformation is based on the union(other) method of the RDD class: other is the second RDD to use, and the method returns a new RDD containing the union (with duplicates) of the elements of the two input RDDs.\n\n\n\n\n\n\nWarning\n\n\n\nDuplicates elements are not removed. This choice is related to optimization reasons: removing duplicates means having a global view of the whole content of the two input RDDs, but, since each RDD is split in partitions that are stored in different nodes of the cluster, the contents of all partitions should be shared to remove duplicates, and that’s a computationally costly operation.\nThe shuffle operation is not needed in this case.\n\n\nIf removing duplicates is needed after performing the union transformation, apply the distinct() transformation on the output of the union() transformation, but pay attention that distinct() is a computational costly operation (it is associated with a shuffle operation). Use distinct() if and only if duplicate removal is indispensable for the application.\n\n\n19.2.2 Intersection transformation\nThe intersection transformation is based on the intersection(other) method of the RDD class: other is the second RDD to use, and the method returns a new RDD containing the elements (without duplicates) occurring in both input RDDs.\nDuplicates are removed: a shuffle operation is executed for computing the result of intersection, since elements from different input partitions must be compared to find common elements.\n\n\n19.2.3 Subtract transformation\nThe subtract transformation is based on the subtract(other) method of the RDD class: other is the second RDD to use, and the result contains the elements appearing only in the RDD on which the subtract method is invoked. Notice that in this transformation the two input RDDs play different roles.\nDuplicates are not removed, but a shuffle operation is executed for computing the result of subtract, since elements from different input partitions must be compared.\n\n\n19.2.4 Cartesian transformation\nThe cartesian transformation is based on the cartesian(other) method of the RDD class: other is the second RDD to use, the data types of the objects of the two input RDDs can be different, and the returned RDD is an RDD of pairs (tuples) containing all the combinations composed of one element of the first input RDD and one element of the second input RDD (see later what an RDD of pairs is).\nIn this transformation a large amount of data is sent on the network: elements from different input partitions must be combined to compute the returned pairs, but the elements of the two input RDDs are stored in different partitions, which could be even in different servers.\n\n\n19.2.5 Examples of set transformations\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate two RDDs of integers\n\ninputRDD1 contains the values [1, 2, 2, 3, 3]\ninputRDD2 contains the values [3, 4, 5]\n\nCreate four new RDDs\n\noutputUnionRDD contains the union of inputRDD1 and inputRDD2\noutputIntersectionRDD contains the intersection of inputRDD1 and inputRDD2\noutputSubtractRDD contains the result of inputRDD1  inputRDD2\noutputCartesianRDD contains the cartesian product of inputRDD1 and inputRDD2\n\n\n# Create two RDD of integers\ninputList1 = [1, 2, 2, 3, 3]\ninputRDD1 = sc.parallelize(inputList1)\n\ninputList2 = [3, 4, 5]\ninputRDD2 = sc.parallelize(inputList2)\n\n# Create four new RDDs by using union, intersection, subtract, and cartesian\noutputUnionRDD = inputRDD1.union(inputRDD2)\n\noutputIntersectionRDD = inputRDD1.intersection(inputRDD2)\n\noutputSubtractRDD = inputRDD1.subtract(inputRDD2)\n\noutputCartesianRDD = inputRDD1.cartesian(inputRDD2)\n\n\n\n\n\n\n\noutputCartesianRDD\nEach element of the returned RDD is a pair (tuple) of integer elements.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate two RDDs\n\ninputRDD1 contains the Integer values [1, 2, 3]\ninputRDD2 contains the String values [\"A\", \"B\"]\n\nCreate a new RDD containing the cartesian product of inputRDD1 and inputRDD2\n\n# Create an RDD of Integers and an RDD of Strings\ninputList1 = [1, 2, 3]\ninputRDD1 = sc.parallelize(inputList1)\n\ninputList2 = [\"A\", \"B\"]\ninputRDD2 = sc.parallelize(inputList2)\n\n# Compute the cartesian product\noutputCartesianRDD = inputRDD1.cartesian(inputRDD2)\n\n\n\n\n\n\n\noutputCartesianRDD\nEach element of the returned RDD is a pair (tuple) of integer elements."
  },
  {
    "objectID": "11_rdd_based_programming.html#summary",
    "href": "11_rdd_based_programming.html#summary",
    "title": "13  RDD based programming",
    "section": "19.3 Summary",
    "text": "19.3 Summary\n\n19.3.1 Single input RDD transformations\nAll the examples reported in the following tables are applied on an RDD of integers containing the following elements (i.e., values): [1,2,3,3].\n\n\n\n\n\n\n\n\n\nTransformation\nPurpose\nExample function\nExample result\n\n\n\n\nfilter(f)\nReturn an RDD consisting only of the elements of the input RDD that pass the condition passed to filter(). The input RDD and the new RDD have the same data type.\nfilter(lambda x: x != 1)\n[2,3,3]\n\n\nmap(f)\nApply a function to each element in the RDD and return an RDD of the result. The applied function return one element for each element of the input RDD. The input RDD and the new RDD can have a different data type.\nmap(lambda x: x+1)\nFor each input element x, the element with value x+1 is included in the new RDD\n[2,3,4,4]\n\n\nflatMap(f)\nApply a function to each element in the RDD and return an RDD of the result. The applied function return a set of elements (from 0 to many) for each element of the input RDD. The input RDD and the new RDD can have a different data type.\nflatMap(lambda x: list(range(x,4))\nFor each input element x, the set of elements with values from x to 3 are returned\n[1,2,3,2,3,3,3]\n\n\ndistinct()\nRemove duplicates.\ndistinct()\n[1, 2, 3]\n\n\nsortBy(keyfunc)\nReturn a new RDD containing the same values of the input RDD sorted in ascending order.\nsortBy(lambda v: v)\nSort the input integer values in ascending order by using the standard integer sort order\n[1, 2, 3, 3]\n\n\nsample(withReplacement, fraction)\nSample the content of the input RDD, with or without replacement and return the selected sample. The input RDD and the new RDD have the same data type.\nsample(True, 0.2)\nNon deterministic\n\n\n\n\n\n\n\n\n\n\n\n\nTransformation\nPurpose\nExample function\nExample result\n\n\n\n\nfilter(f)\nReturn an RDD consisting only of the elements of the “input”” RDD that pass the condition passed to filter(). The “input” RDD and the new RDD have the same data type.\nfilter(lambda x: x != 1)\n[2,3,3]\n\n\nmap(f)\nApply a function to each element in the RDD and return an RDD of the result. The applied function return one element for each element of the input RDD. The input RDD and the new RDD can have a different data type.\nmap(lambda x: x+1) For each input element x, the element with value x+1 is included in the new RDD\n[2,3,4,4]\n\n\nflatMap(f)\nApply a function to each element in the RDD and return an RDD of the result. The applied function return a set of elements (from 0 to many) for each element of the input RDD. The input RDD and the new RDD can have a different data type.\nflatMap(lambda x: list(range(x,4)) For each input element x, the set of elements with values from x to 3 are returned\n[1,2,3,2,3,3,3]\n\n\ndistinct()\nRemove duplicates.\ndistinct()\n[1, 2, 3]\n\n\nsortBy(keyfunc)\nReturn a new RDD containing the same values of the input RDD sorted in ascending order\nsortBy(lambda v: v) Sort the input integer values in ascending order by using the standard integer sort order\n[1, 2, 3, 3]\n\n\nsample(withReplacement, fraction)\nSample the content of the input RDD, with or without replacement and return the selected sample. The input RDD and the new RDD have the same data type.\nsample(True, 0.2)\nNon deterministic"
  }
]