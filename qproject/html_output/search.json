[
  {
    "objectID": "index.html#index",
    "href": "index.html#index",
    "title": "Distributed architectures for big data processing and analytics",
    "section": "Index",
    "text": "Index\nThis Quarto Book contains notes collected during the lectures of “Distributed architectures for big data processing and analytics” (DABDPA) course, hold at the Master Degree in Data Science and Engineering (2022-2023) of Politecnico di Torino."
  },
  {
    "objectID": "00_01_intro.html",
    "href": "00_01_intro.html",
    "title": "2  Introduction to Big data",
    "section": "",
    "text": "Who generates Big Data\n\nUser generated content: social networks (web and mobile)\nHealth and scientific computing\nLog files: web server log files, machine system log files\nInternet of Things (IoT): sensor networks, RFIDs, smart meters\n\n\nExample of Big Data at work\n\n\nBigdata example\n\n\n\n\n\n\nThe five Vs of Big Data\n\nVolume: scale of data\nVariety: different forms of data\nVelocity: analysis of streaming data\nVeracity: uncertainty of data\nValue: exploit information provided by data\n\n\n\nThe bottleneck and the solution\n\nBottleneck\n\nProcessors process data\nHard drives store data\nWe need to transfer data from the disk to the processor\n\n\n\nSolution\n\nTransfer the processing power to the data\nMultiple distributed disks: each one holding a portion of a large dataset\nProcess in parallel different file portions from different disks"
  },
  {
    "objectID": "02_architectures.html",
    "href": "02_architectures.html",
    "title": "3  Big data architectures",
    "section": "",
    "text": "Definition of Big data architecture\n\n\n\n\n\n\nFrom Data Architecture Guide in Microsoft Learn\n\n\n\nA big data architecture is designed to handle the ingestion, processing, and analysis of data that is too large or complex for traditional database systems…\nBig data solutions typically involve one or more of the following types of workload:\n\nBatch processing of big data sources at rest\nReal-time processing of big data in motion\nInteractive exploration of big data\nPredictive analytics and machine learning\n\nConsider big data architectures when you need to:\n\nStore and process data in volumes too large for a traditional database\nTransform unstructured data for analysis and reporting\nCapture, process, and analyze unbounded streams of data in real time, or with low latency\n\n\n\n\n\nLambda architecture\nThe most frequently used big data architecture is the Lambda Architecture. The lambda architecture was proposed by Nathan Marz in 2011.\n\nDefinitions\n\n\n\n\n\n\nFrom Nathan Marz\n\n\n\nThe past decade has seen a huge amount of innovation in scalable data systems. These include large-scale computation systems like Hadoop and databases such as Cassandra and Riak. These systems can handle very large amounts of data, but with serious trade-offs.Hadoop, for example, can parallelize large-scale batch computations on very large amounts of data, but the computations have high latency. You don’t use Hadoop for anything where you need low-latency results.\nNoSQL databases like Cassandra achieve their scalability by offering you a much more limited data model than you’re used to with something like SQL. Squeezing your application into these limited data models can be very complex. And because the databases are mutable, they’re not human-fault tolerant.\nThese tools on their own are not a panacea. But when intelligently used in conjunction with one another, you can produce scalable systems for arbitrary data problems with human-fault tolerance and a minimum of complexity. This is the Lambda Architecture you’ll learn throughout the book.\n\n\n\n\n\n\n\n\nFrom What is Lambda Architecture? article in Databricks website\n\n\n\nLambda architecture is a way of processing massive quantities of data (i.e. “Big Data”) that provides access to batch-processing and stream-processing methods with a hybrid approach.\nLambda architecture is used to solve the problem of computing arbitrary functions.\n\n\n\n\n\n\n\n\nFrom Lambda architecture article in Wikipedia\n\n\n\nLambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.\nThis approach to architecture attempts to balance latency, throughput, and fault tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation.\nLambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.\n\n\n\n\nRequirements\nFault-tolerant against both hardware failures and human errors Support variety of use cases that include low latency querying as well as updates Linear scale-out capabilities Extensible, so that the system is manageable and can accommodate newer features easily\n\n\nQueries\n\\[\n\\textbf{query} = \\textbf{function}(\\textbf{all data})\n\\]\nSome query properties\n\nLatency: the time it takes to run a query\nTimeliness: how up to date the query results are (freshness and consistency)\nAccuracy: tradeoff between performance and scalability (approximations)\n\nIt is based on two data paths:\n\nCold path (batch layer)\n\nIt stores all of the incoming data in its raw form and performs batch processing on the data\nThe result of this processing is stored as batch views\n\nHot path (speed layer)\n\nIt analyzes data in real time\nThis path is designed for low latency, at the expense of accuracy\n\n\n\n\nBasic structure\n\n\nGeneral Lambda architecture\n\n\n\n\nAll data entering the system is dispatched to both the batch layer and the speed layer for processing\nThe batch layer has two functions:\n\nmanaging the master dataset(an immutable, append-only set of raw data), and\nto pre-compute the batch views\n\nThe serving layer indexes the batch views so that they can be queried in low-latency, ad-hoc way\nThe speed layer compensates for the high latency of updates to the serving layer and deals with recent data only\nAny incoming query can be answered by merging results from batch views and real-time views (e.g., the query looks at the serving layer for days until today, and looks at the speed layer for today’s data).\n\n\n\nDetailed view\n\n\nMore detailed of Lambda architecture\n\n\n\nStructure similar to the one described before\n\nData stream\nBatch layer\n\nimmutable data\nprecompute views\n\nReal-time layer\n\nprocess stream\nincrement views\n\nServing layer\n\n\nPossible instances\n\n\nMore detailed of Lambda architecture\n\n\n\nIn general, the technologies used are\n\nData stream: Kafka\nBatch layer:\n\nimmutable data: Hadoop HDFS\nprecompute views: Hadoop, Spark\nviews: Hive (it is a distributed relational database; SQL-like query language can be used)\n\nReal-time layer:\n\nprocess stream and increment views:\n\nSpark (it has a module available for managing stream data)\nApache Storm (pros: true real-time; cons: sometimes it approximates)\nFlink (used stream data analysis)\n\nviews: HBase, Cassandra, MongoDB\n\nServing layer\n\nIn general: choose the most suitable technology, but also be able to adapt on what’s available."
  },
  {
    "objectID": "03b_HDFS_clc.html",
    "href": "03b_HDFS_clc.html",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "",
    "text": "HDFS\nThe content of a HDFS file can be accessed by means of\n\nCommand line commands\nA basic web interface provided by Apache Hadoop. The HDFS content can only be browsed and its files downloaded from HDFS to the local file system, while uploading functionalities are not available.\nVendor-specific web interfaces providing a full set of functionalities (upload, download, rename, delete, …) (e.g., the HUE web application of Cloudera).\n\n\nUser folder\nEach user of the Hadoop cluster has a personal folder in the HDFS file system. The default folder of a user is /user/username\n\n\nCommand line\nThe hdfs command can be executed in a Linux shell to read/write/modify/delete the content of the distributed file system. The parameters/arguments of hdfs command are used to specify the operation to execute.\n\nContent of a folder\nTo list the content of a folder of the HDFS file system, use hdfs dfs -ls folder\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -ls /user/garza shows the content (list of files and folders) of the /user/garza folder.\nThe command hdfs dfs -ls . shows the content of the current folder (i.e., the content of /user/current_username).\nNotice that the mapping between the local linux user and the user of the cluster is based on\n\nA Kerberos ticket if Kerberos is active\nOtherwise the local linux user is considered\n\n\n\n\n\n\nContent of a file\nTo show the content of a file in the HDFS file system, use hdfs dfs -cat file_name\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -cat /user/garza/document.txt shows the content of the /user/garza/document.txt file stored in HDFS.\n\n\n\n\n\nCopy a file from local to HDFS\nTo copy a file from the local file system to the HDFS file system, use hdfs dfs -put local_file HDFS_path\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -put /data/document.txt /user/garza/ copies the local file /data/document.txt in the folder /user/garza in HDFS.\n\n\n\n\n\nCopy a file from HDFS to local\nTo copy a file from the HDFS file system to the local file system, use hdfs dfs -get HDFS_path local_file\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -get /user/garza/document.txt /data/ copies the HDFS file /user/garza/document.txt in the local file system folder /data/.\n\n\n\n\n\nDelete a file\nTo delete a file from the HDFS file system, use hdfs dfs -rm HDFS_path\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -rm /user/garza/document.txt deletes from HDFS the file /user/garza/document.txt\n\n\n\n\n\nOther commands\nThere are many other linux-like commands, for example\n\nrmdir\ndu\ntail\n\nSee the HDFS commands guide for a complete list.\n\n\n\n\nHadoop\nThe Hadoop programs are executed (submitted to the cluster) by using the hadoop command. It is a command line program, characterized by a set of parameters, such as\n\nthe name of the jar file containing all the classes of the MapReduce application we want to execute\nthe name of the Driver class\nthe parameters/arguments of the MapReduce application\n\n\nExample\nThe following command executes/submits a MapReduce application\nhadoop jar MyApplication.jar \\\nit.polito.bigdata.hadoop.DriverMyApplication \\\n1 inputdatafolder/ outputdatafolder/\n\nIt executes/submits the application contained in MyApplication.jar\nThe Driver Class is it.polito.bigdata.hadoop.DriverMyApplication\nThe application has three arguments\n\nNumber of reducers (1)\nInput data folder (inputdatafolder/)\nOutput data folder (outputdatafolder/)"
  },
  {
    "objectID": "03_intro_hadoop.html",
    "href": "03_intro_hadoop.html",
    "title": "5  Introduction to Hadoop and MapReduce",
    "section": "",
    "text": "Motivations of Hadoop and Big data frameworks\n\nData volumes\n\nThe amount of data increases every day\nSome numbers (∼2012):\n\nData processed by Google every day: 100+ PB\nData processed by Facebook every day: 10+ PB\n\nTo analyze them, systems that scale with respect to the data volume are needed\n\n\n\n\n\n\n\nExample: Google\n\n\n\n\n\nConsider this situation: you have to analyze 10 billion web pages, and the average size of a webpage is 20KB. So\n\nThe total size of the collection: 10 billion x 20KBs = 200TB\nAssuming the usage of HDD hard disk (read bandwidth: 150MB/sec), the time needed to read all web pages (without analyzing them) is equal to 2 million seconds (i.e., more than 15 days).\nAssuming the usage of SSD hard disk (read bandwidth: 550MB/sec), the time needed to read all web pages (without analyzing them) is equal to 2 million seconds (i.e., more than 4 days).\nA single node architecture is not adequate\n\n\n\n\n\n\nFailures\nFailures are part of everyday life, especially in a data center. A single server stays up for 3 years (~1000 days). Statistically\n\nWith 10 servers: 1 failure every 100 days (~3 months)\nWith 100 servers: 1 failure every 10 days\nWith 1000 servers: 1 failure/day\n\nThe main sources of failures\n\nHardware/Software\nElectrical, Cooling, …\nUnavailability of a resource due to overload\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\nLALN data [DSN 2006]\n\nData for 5000 machines, for 9 years\nHardware failures: 60%, Software: 20%, Network 5%\n\nDRAM error analysis [Sigmetrics 2009]\n\nData for 2.5 years\n8% of DIMMs affected by errors\n\nDisk drive failure analysis [FAST 2007]\n\nUtilization and temperature major causes of failures\n\n\n\n\nFailure types\n\nPermanent (e.g., broken motherboard)\nTransient (e.g., unavailability of a resource due to overload)\n\n\n\nNetwork bandwidth\nNetwork becomes the bottleneck if big amounts of data need to be exchanged between nodes/servers. Assuming a network bandwidth (in a data centre) equal to 10 Gbps, it means that moving 10 TB from one server to another would take more than 2 hours. So, data should be moved across nodes only when it is indispensable.\nInstead of moving data to the data centre, the code (i.e., programs) should be moved between the nodes: this approach is called Data Locality, and in this way very few MBs of code are exchanged between the severs, instead of huge amount of data.\n\n\n\nArchitectures\n\nSingle node architecture\n\n\nSingle node architecture\n\n\n\n\n\n\n\nSingle node architecture: Machine Learning and Statistics\n\n\n\nSmall data: data can be completely loaded in main memory.\n\n\n\n\n\nSingle node architecture: “Classical” data mining”\n\n\n\nLarge data: data can not be completely loaded in main memory.\n\nLoad in main memory one chunk of data at a time, process it and store some statistics\nCombine statistics to compute the final result\n\n\n\n\n\nCluster architecture\nTo overcome the previously explained issues, a new architecture based on clusters of servers (i.e., data centres) has been devised. In this way:\n\nComputation is distributed across servers\nData are stored/distributed across servers\n\nThe standard architecture in the Big data context (∼2012) is based on\n\nCluster of commodity Linux nodes/servers (32 GB of main memory per node)\nGigabit Ethernet interconnection\n\n\nCommodity cluster architecture\n\n\nCommodity cluster architecture\n\n\n\nThe servers in each rack are very similar to each other, so that the servers would take the same time to process the data and none of them will become a bottleneck for the overall processing.\nNotice that\n\nIn each rack, the servers are directly connected with each other in pairs\nRacks are directly connected with each other in pairs\n\n\n\n\nScalability\nCurrent systems must scale to address\n\nThe increasing amount of data to analyze\nThe increasing number of users to serve\nThe increasing complexity of the problems\n\nTwo approaches are usually used to address scalability issues\n\nVertical scalability (scale up)\nHorizontal scalability (scale out)\n\n\nScale up vs. Scale out\n\nVertical scalability (scale up): add more power/resources (i.e., main memory, CPUs) to a single node (high-performing server). The cost of super-computers is not linear with respect to their resources: the marginal cost increases as the power/resources increase.\nHorizontal scalability (scale out): add more nodes (commodity servers) to a system. The cost scales approximately linearly with respect to the number of added nodes. But data center efficiency is a difficult problem to solve.\n\nFor data-intensive workloads, a large number of commodity servers is preferred over a small number of high-performing servers, since, at the same cost, it is possible to deploy a system that processes data more efficiently and is more fault-tolerant.\nHorizontal scalability (scale out) is preferred for big data applications, but distributed computing is hard: new systems hiding the complexity of the distributed part of the problem to developers are needed.\n\n\n\nCluster computing challenges\n\nDistributed programming is hard\n\nProblem decomposition and parallelization\nTask synchronization\n\nTask scheduling of distributed applications is critical: assign tasks to nodes by trying to\n\nSpeed up the execution of the application\nExploit (almost) all the available resources\nReduce the impact of node failures\n\nDistributed data storage\n\nHow to store data persistently on disk and keep it available if nodes can fail? Redundancy is the solution, but it increases the complexity of the system.\n\nNetwork bottleneck\n\nReduce the amount of data send through the network by moving computation and code to data.\n\n\n\n\n\n\nDistributed computing history\n\n\n\n\n\nDistributed computing is not a new topic\n\nHPC (High-performance computing) ~1960\nGrid computing ~1990\nDistributed databases ~1990\n\nHence, many solutions to the mentioned challenges are already available, but we are now facing big data-driven problems: the former solutions are not adequate to address big data volumes.\n\n\n\n\n\nTypical Big data problem\nThe typical way to address a Big Data problem (given a collection of historical data)\n\nIterate over a large number of records/objects\nExtract something of interest from each record/object\nAggregate intermediate results\nGenerate final output\n\nNotice that, if in the second step it is needed to have some kind of knowledge of what’s in the other records, this Big data framework is not the best solution: the computations on isolated records is not possible anymore, and so this whole architecture is not suitable.\nThe challenges:\n\nParallelization\nDistributed storage of large data sets (Terabytes, Petabytes)\nNode Failure management\nNetwork bottleneck\nDiverse input format (data diversity & heterogeneity)\n\n\n\n\nApache Hadoop\nIt is scalable fault-tolerant distributed system for Big Data\n\nDistributed Data Storage\nDistributed Data Processing\n\nIt borrowed concepts/ideas from the systems designed at Google (Google File System for Google’s MapReduce). It is open source project under the Apache license, but there are also many commercial implementations (e.g., Cloudera, Hortonworks, MapR).\n\n\n\n\n\n\nHadoop history\n\n\n\n\n\n\nTimeline\n\n\n\n\n\n\nDate\nEvent\n\n\n\n\nDec 2004\nGoogle published a paper about GFS\n\n\nJuly 2005\nNutch uses MapReduce\n\n\nFeb 2006\nHadoop becomes a Lucene subproject\n\n\nApr 2007\nYahoo! runs it on a 1000-node cluster\n\n\nJan 2008\nHadoop becomes an Apache Top Level Project\n\n\nJul 2008\nHadoop is tested on a 4000 node cluster\n\n\nFeb 2009\nThe Yahoo! Search WebMap is a Hadoop application that runs on more than 10,000 core Linux cluster\n\n\nJun 2009\nYahoo! made available the source code of its production version of Hadoop\n\n\n2010\nFacebook claimed that they have the largest Hadoop cluster in the world with 21 PB of storage\n\n\nJul 27, 2011\nFacebook announced the data has grown to 30 PB\n\n\n\nWho uses/used Hadoop\n\nAmazon\nFacebook\nGoogle\nIBM\nJoost\nLast.fm\nNew York Times\nPowerSet\nVeoh\nYahoo!\n\n\n\n\n\nHadoop vs. HPC\nHadoop\n\nDesigned for Data intensive workloads\nUsually, no CPU demanding/intensive tasks\n\nHPC (High-performance computing)\n\nA supercomputer with a high-level computational capacity (performance of a supercomputer is measured in floating-point operations per second (FLOPS))\nDesigned for CPU intensive tasks\nUsually it is used to process “small” data sets\n\n\n\nMain components\nCore components of Hadoop:\n\nDistributed Big Data Processing Infrastructure based on the MapReduce programming paradigm\n\nProvides a high-level abstraction view: programmers do not need to care about task scheduling and synchronization\nFault-tolerant: node and task failures are automatically managed by the Hadoop system\n\nHDFS (Hadoop Distributed File System)\n\nHigh availability distributed storage\nFault-tolerant\n\n\nHadoop virtualizes the file system, so that the interaction resembles a local file system, even if this case it spans on multiple disks on multiple servers.\nSo Hadoop is in charge of:\n\nsplitting the input files\nstore the data in different servers\nmanaging the reputation of the blocks\n\n\n\nHadoop main components\n\n\n\nNotice that, in this example, the number of replicas (i.e., the number of copies) of each block (e.g., \\(C_0\\), \\(C_1\\), \\(C_6\\), etc.) is equal to two. Multiple copies are needed to correctly manage server failures: two copies are never stored in the same server.\nNotice that, with 2 copies of the same file, the user is always sure that 1 failure can be managed with no interruptions in the data processing and without the risk of losing data. In general, the number of failures that and HDFS can sustain with no repercussions is equal to \\((\\textbf{number of copies})-1\\).\nWhen a failure occures, Hadoop immediately starts to create new copies of the data, to reach again the set number of replicas.\n\n\nDistributed Big data processing infrastructure\nHadoop allows to separate the what from the how because Hadoop programs are based on the MapReduce programming paradigm:\n\nMapReduce abstracts away the “distributed” part of the problem (scheduling, synchronization, etc), so that programmers can focus on the what;\nthe distributed part (scheduling, synchronization, etc) of the problem is handled by the framework: the Hadoop infrastructure focuses on the how.\n\nBut an in-depth knowledge of the Hadoop framework is important to develop efficient applications: the design of the application must exploit data locality and limit network usage/data sharing.\n\n\nHDFS\nHDFS is the standard Apache Hadoop distributed file system. It provides global file namespace, and stores data redundantly on multiple nodes to provide persistence and availability (fault-tolerant file system).\nThe typical usage pattern for Hadoop:\n\nhuge files (GB to TB);\ndata is rarely updated (create new files or append to existing ones);\nreads and appends are common, and random read/write operations are not performed.\n\nEach file is split in chunks (also called blocks) that are spread across the servers.\n\nEach chunck is replicated on different servers (usually there are 3 replicas per chunk), ensuring persistence and availability. To further increase persistence and availability, replicas are stored in different racks, if it possible.\nEach chunk contains a part of the content of one single file. It is not possible to have the content of two files in the same chunk/block\nTypically each chunk is 64-128 MB, and the chunk size is defined when configuring Hadoop.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\n2 files in 4 chunks\n\n\n\nEach square represents a chunk in the HDFS. Each chunk contains 64 MB of data, so file 1 (65 MB) sticks out by 1 MB from a single chunk, while file 2 (127 MB) does not completely fill two chunks. The empty chunk portions are not filled by any other file.\nSo, even if the total space occupied from the files would be 192 MB (3 chunks), the actual space they occupy is 256 (4 chunks): Hadoop does not allow two files to occupy the same chunk, so that two different processes would not try to access a block at the same time.\n\n\n\nThe Master node, (a.k.a., Name Nodes in HDFS) is a special node/server that\n\nStores HDFS metadata (e.g., the mapping between the name of a file and the location of its chunks)\nMight be replicated (to prevent stoppings due to the failure of the Master node)\n\nClient applications can access the file through HDFS APIs: they talk to the master node to find data/chuck servers associated with the file of interest, and then connect to the selected chunk servers to access data.\n\n\n\n\n\n\nHadoop ecosystem\n\n\n\nThe HDFS and the YARN scheduler are the two main components of Hadoop, however there are modules, and each project/system addresses one specific class of problems.\n\nHive: a distributed relational database, based on MapReduce, for querying data stored in HDFS by means of a query language based on SQL;\nHBase: a distributed column-oriented database that uses HDFS for storing data;\nPig: a data flow language and execution environment, based on MapReduce, for exploring very large datasets;\nSqoop: a tool for efficiently moving data from traditional relational databases and external flat file sources to HDFS;\nZooKeeper: a distributed coordination service, that provides primitives such as distributed locks.\n…\n\nThe integration of these components with Hadoop is not as good as the integration of the Spark components with Spark.\n\n\n\n\n\nMapReduce: introduction\n\nWord count\n\n\n\n\n\n\n\n\nInput\nProblem\nOutput\n\n\n\n\na large textual file of words\ncount the number of times each distinct word appears in the file\na list of pairs word, number, counting the number of occurrences of each specific word in the input file\n\n\n\n\nCase 1: Entire file fits in main memory\nA traditional single node approach is probably the most efficient solution in this case. The complexity and overheads of a distributed system affects the performance when files are “small” (“small” depends on the available resources).\n\n\nCase 2: File too large to fit in main memory\nHow to split this problem in a set of (almost) independent sub-tasks, and execute them in parallel on a cluster of servers?\nAssuming that\n\nThe cluster has 3 servers\nThe content of the input file is: “Toy example file for Hadoop. Hadoop running example”\nThe input file is split into 2 chunks\nThe number of replicas is 1\n\n\n\nWord count solution\n\n\n\nThe problem can be easily parallelized:\n\nEach server processes its chunk of data and counts the number of times each word appears in its own chunk\n\nEach server can execute its sub-task independently from the other servers of the cluster: asynchronization is not needed in this phase\nThe output generated from each chunk by each server represents a partial result\n\nEach server sends its local (partial) list of pairs \\(<\\textbf{word}, \\textbf{number of occurrences in its chunk}>\\) to a server that is in charge of aggregating all local results and computing the global result. The server in charge of computing the global result needs to receive all the local (partial) results to compute and emit the final list: a synchronization operation is needed in this phase.\n\nAssume a more realistic situation\n\nThe file size is 100 GB and the number of distinct words occurring in it is at most 1000\nThe cluster has 101 servers\nThe file is spread across 100 servers (1 server is the Master node) and each of these servers contains one (different) chunk of the input file (i.e., the file is optimally spread across 100 servers, and so each server contains 1/100 of the file in its local hard drives)\n\n\nComplexity\n\nEach server reads 1 GB of data from its local hard drive (it reads one chunk from HDFS): the time needed to process the data is equal to a few seconds;\nEach local list consists of at most 1,000 pairs (because the number of distinct words is 1,000): each list consists of a few MBs;\nThe maximum amount of data sent on the network is 100 times the size of a local list (number of servers x local list size): the MBs that are moved through the network consists of some MBs.\n\nSo, the critical step is the first one: the result of this phase should be as small as possible, to reduce the data moving between nodes during the following phase.\nIs also the aggregating step parallelizable? Yes, in the sense that the key-value pairs associated with the same key are sent to the same server in order to apply the aggregating function. So, different servers work in parallel, computing the aggregations on different keys.\n\n\nScalability\nScalability can be defined along two dimensions\n\nIn terms of data: given twice the amount of data, the word count algorithm takes approximately no more than twice as long to run. Each server has to process twice the data, and so execution time to compute local list is doubled.\nIn terms of resources: given twice the number of servers, the word count algorithm takes approximately no more than half as long to run. Each server processes half of the data, and execution time to compute local list is halved.\n\nWe are assuming that the time needed to send local results to the node in charge of computing the final result and the computation of the final result are considered negligible in this running example. However, notice that frequently this assumption is not true, indeed it depends on the complexity of the problem and on the ability of the developer to limit the amount of data sent on the network.\n\n\n\n\nMapReduce approach key ideas\n\nScale “out”, not “up”: increase the number of servers, avoiding to upgrade the resources (CPU, memory) of the current ones\nMove processing to data: the network has a limited bandwidth\nProcess data sequentially, avoid random access: seek operations are expensive. Big data applications usually read and analyze all input records/objects: random access is useless\n\n\nData locality\nTraditional distributed systems (e.g., HPC) move data to computing nodes (servers). This approach cannot be used to process TBs of data, since the network bandwidth is limited So, Hadoop moves code to data: code (few KB) is copied and executed on the servers where the chunks of data are stored. This approach is based on “data locality”.\n\n\n\nHadoop and MapReduce usage scope\nHadoop/MapReduce is designed for\n\nBatch processing involving (mostly) full scans of the input data\nData-intensive applications\n\nRead and process the whole Web (e.g., PageRank computation)\nRead and process the whole Social Graph (e.g., LinkPrediction, a.k.a. “friend suggestion”)\nLog analysis (e.g., Network traces, Smart-meter data)\n\n\nIn general, MapReduce can be used when the same function is applied on multiple records one at a time, and its result then has to be aggregated.\n\n\n\n\n\n\nWarning\n\n\n\nNotice that Hadoop/MapReduce is not the panacea for all Big Data problems. In particular, does not feet well\n\nIterative problems\nRecursive problems\nStream data processing\nReal-time processing\n\n\n\n\n\n\nThe MapReduce programming paradigm\nThe MapReduce programming paradigm is based on the basic concepts of Functional programming. Actually, MapReduce “implements” a subset of functional programming, and, because of this, the programming model appears quite limited and strict: everything is based on two “functions” with predefined signatures, that are Map and Reduce.\n\nWhat can MapReduce do\nSolving complex problems is difficult, however there are several important problems that can be adapted to MapReduce\n\nLog analysis\nPageRank computation\nSocial graph analysis\nSensor data analysis\nSmart-city data analysis\nNetwork capture analysis\n\n\n\nBuilding blocks: Map and Reduce\nMapReduce is based on two main “building blocks”, which are the Map and Reduce functions.\n\nMap function: it is applied over each element of an input data set and emits a set of (key, value) pairs\nReduce function: it is applied over each set of (key, value) pairs (emitted by the Map function) with the same key and emits a set of (key, value) pairs. This is the final result.\n\n\n\nSolving the word count problem\n\n\n\n\n\n\n\n\nInput\nProblem\nOutput\n\n\n\n\na large textual file of words\ncount the number of times each distinct word appears in the file\na list of pairs word, number, counting the number of occurrences of each specific word in the input file\n\n\n\nThe input textual file is considered as a list \\(L\\) of words\n\\[\nL = [\\text{toy}, \\text{example}, \\text{toy}, \\text{example}, \\text{hadoop}]\n\\]\n\n\nWord count running example\n\n\n\n\nMap phase: apply a function on each element of a list of key-value pairs (notice that the example above is not 100% correct: the elements of the list should also be key-value pairs);\nShuffle and sort phase: group by key; in this phase the key-value pairs having the same key are collected together in the same node, but no computation is performed;\nReduce phase: apply an aggregating function on each group; this step can be parallelized: one node may consider some keys, while another one considers others.\n\nA key-value pair \\((w, 1)\\) is emitted for each word \\(w\\) in \\(L\\).\nIn other words, the Map function \\(m\\) is \\[\nm(w) = (w, 1)\n\\]\nA new list of (key, value) pairs \\(L_m\\) is generated. Notice that, in this case the key-value pairs generated for each word is just one, but in other cases more than one key-value pair is generated from each element of the starting list.\nThen, the key-value pairs in \\(L_m\\) are aggregated by key (i.e., by word \\(w\\) in the example).\n\nMap\nIn the Map step, one group \\(G_w\\) is generated for each word \\(w\\). Each group \\(G_w\\) is a key-list pair \\[\n(w, [\\textbf{list of values}])\n\\] where \\([\\textbf{list of values}]\\) contains all the values of the pairs associated with the word \\(w\\).\nConsidering the example, \\(\\textbf{[list of values]}\\) is a list of \\([1, 1, 1, ...]\\), and, given a group \\(G_w\\), the number of ones in \\([1, 1, 1, ...]\\) is equal to the occurrences of word \\(w\\) in the input file.\nNotice that also the input of Map should be a list of key-value pairs. If a simple list of elements is passed to Map, Hadoop transforms the elements in key-value pairs, such that the value is equal to the element (e.g., the word) and the key is equal to the offset of the element in the input file.\n\n\nReduce\nFor each group \\(G_w\\) a key-value pair is emitted as follows \\[\n(w,\\sum_{G_w}{[\\textbf{list of values}]})\n\\] So, the result of the Reduce function is \\(r(G_w) = (w,\\sum_{Gw}{[\\textbf{list of values}]})\\).\nThe resulting list of emitted pairs is the solution of the word count problem: in the list there is one pair (word \\(w\\), number of occurrences) for each word in our running example.\n\n\n\nMapReduce Phases\n\nMap\nThe Map phase can be viewed as a transformation over each element of a data set. This transformation is a function \\(m\\) defined by developers, and it is invoked one time for each input element. Each invocation of \\(m\\) happens in isolation, allowing the parallelization of the application of \\(m\\) to each element of a data set in a straightforward manner.\nThe formal definition of Map is \\[\n(k_1, v_1) \\rightarrow [(k_2, v_2)]\n\\] Notice that\n\nSince the input data set is a list of key-value pairs, the argument of the Map function is a key-value pair; so, the Map function \\(N\\) times, where \\(N\\) is the number of input key-value pairs;\nThe Map function emits a list of key-value pairs for each input record, and the list can also be empty;\nNo data is moved between nodes during this phase.\n\n\n\nReduce\nThe Reduce phase can be viewed as an aggregate operation. The aggregate function is a function \\(r\\) defined by developers, and it is invoked one time for each distinct key, aggregating all the values associated with it. Also the reduce phase can be performed in parallel and in isolation, since each group of key-value pairs with the same key can be processed in isolation.\nThe formal definition of Reduce is \\[\n(k_2, [v_2]) \\rightarrow [(k_3, v_3)]\n\\] Notice that\n\nThe Reduce function receives a list of values \\([v_2]\\) associated with a specific key \\(k_2\\); so the Reduce function is invoked \\(M\\) times, where \\(M\\) is the number of different keys in the input list;\nThe Reduce function emits a list of key-value pairs.\n\n\n\nShuffle and sort\nThe shuffle and sort phase is always the same: it works by grouping the output of the Map phase by key. It does not need to be defined by developers, and it is already provided by the Hadoop system.\n\n\n\nData structures\nKey-value pair is the basic data structure in MapReduce. Keys and values can be integers, float, strings, …, in general they can also be (almost) arbitrary data structures defined by the designer. Notice that both input and output of a MapReduce program are lists of key-value pairs.\nAll in all, the design of MapReduce involves imposing the key-value structure on the input and output data sets. For example, in a collection of Web pages, input keys may be URLs and values may be their HTML content.\nIn many applications, the key part of the input data set is ignored. In other words, the Map function usually does not consider the key of its key-value pair argument (e.g., word count problem). Some specific applications exploit also the keys of the input data (e.g., keys can be used to uniquely identify records/objects).\n\n\nPseudocode of word count solution using MapReduce\nMap\ndef map(key, value):\n    '''\n    :key: offset of the word in the file\n    :value: a word of the input document\n    '''\n    return (value, 1)\nReduce\ndef reduce(key, values):\n    '''\n    :key: a word \n    :values: a list of integers\n    '''\n    occurrences = 0\n    for c in values:\n        occurrences = occurrences + c\n    return (key, occurrences)"
  },
  {
    "objectID": "04_hadoop_implementation.html",
    "href": "04_hadoop_implementation.html",
    "title": "6  How to write MapReduce programs in Hadoop",
    "section": "",
    "text": "Designers and developers focus on the definition of the Map and Reduce functions (i.e., \\(m\\) and \\(r\\)), and they don’t need to manage the distributed execution of the map, shuffle and sort, and reduce phases. Indeed, the Hadoop framework coordinates the execution of the MapReduce program, managing:\n\nthe parallel execution of the map and reduce phases\nthe execution of the shuffle and sort phase\nthe scheduling of the subtasks\nthe synchronization\n\n\nThe components: summary\nThe programming language to use to give instructions to Hadoop is Java. A Hadoop MapReduce program consists of three main parts:\n\nDriver\nMapper\nReducer\n\nEach part is “implemented” by means of a specific class.\n\n\n\n\n\n\nTerminology\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nDriver class\nThe class containing the method/code that coordinates the configuration of the job and the “workflow” of the application\n\n\nMapper class\nA class “implementing” the map function\n\n\nReducer class\nA class “implementing” the reduce function\n\n\nDriver\nInstance of the Driver class (i.e., an object)\n\n\nMapper\nInstance of the Mapper class (i.e., an object)\n\n\nReducer\nInstance of the Reducer class (i.e., an object)\n\n\n(Hadoop) Job\nExecution/run of a MapReduce code over a data set\n\n\nTask\nExecution/run of a Mapper (Map task) or a Reducer (Reduce task) on a slice of data. Notice that there may be many tasks for each job\n\n\nInput split\nFixed-size piece of the input data. Usually each split has approximately the same size of a HDFS block/chunk\n\n\n\n\n\n\nDriver (instance)\nThe Driver is characterized by the main() method, which accepts arguments from the command line (i.e., it is the entry point of the application). Also, it has a run() method\n\nIt configures the job\nIt submits the job to the Hadoop Cluster\nIt “coordinates” the work flow of the application\nIt runs on the client machine (i.e., it does not run on the cluster)\n\n\n\nMapper (instance)\nThe Mapper is an instance of the Mapper class.\n\nIt “implements” the map phase;\nIt is characterized by the map() method, which processes the (key, value) pairs of the input file and emits (key, value) pairs and is invoked one time for each input (key, value) pair;\nIt runs on the cluster.\n\n\n\n\n\n\n\nTip\n\n\n\nThe Driver will try to create one Mapper instance for each input block, pushing to the maximum parallelization possible.\n\n\n\n\nReducer (instance)\nThe Reducer is an instance of the Reduce class.\n\nIt “implements” the reduce phase;\nIt is characterized by the reduce() method, which processes (key, [list of values]) pairs and emits (key, value) pairs and is invoked one time for each distinct key;\nIt runs on the cluster.\n\n\n\n\nHadoop implementation of the MapReduce phases\nThe main characteristics Hadoop implementation of the MapReduce are the following\n\nThe input key-value pairs are read from the HDFS file system.\nThe map method of the Mapper is invoked over each input key-value pair, and emits a set of intermediate key-value pairs that are stored in the local file system of the computing server (they are not stored in HDFS).\nThe intermediate results are aggregated by means of a shuffle and sort procedure, and a set of (key, [list of values]) pairs is generated. Notice that one (key, [list of values]) for each distinct key.\nThe reduce method of the Reducer is applied over each (key, [list of values]) pair, and emits a set of key-value pairs that are stored in HDFS (the final result of the MapReduce application).\nIntermediate key-value pairs are transient, which means that they are not stored on the distributed files system, while they are stored locally to the node producing or processing them.\nIn order to parallelize the work/the job, Hadoop executes a set of tasks in parallel\n\nIt instantiates one Mapper (Task) for each input split\nIt instantiates a user-specified number of Reducers: each reducer is associated with a set of keys, and it receives and processes all the key-value pairs associated with its set of keys\n\nMappers and Reducers are executed on the nodes/servers of the clusters\n\n\n\nMapReduce data flow with a single reducer\n\n\n\n\n\nMapReduce data flow with multiple reducers\n\n\n\n\nDriver class\nThe Driver class extends the org.apache.hadoop.conf.Configured class and implements the org.apache.hadoop.util.Tool interface 1.\nIt is possible to write a Driver class that does not extend Configured and does not implement Tool, however some low level details related to some command line parameters must be managed in that case.\nThe designer/developer implements the main() and run() methods.\nThe run() method configures the job, defining\n\nThe name of the Job\nThe job Input format\nThe job Output format\nThe Mapper class\n\nName of the class\nType of its input (key, value) pairs\nType of its output (key, value) pairs\n\nThe Reducer class\n\nName of the class\nType of its input (key, value) pairs\nType of its output (key, value) pairs\n\nThe Number of Reducers2\n\n\n\nMapper class\nThe Mapper class extends the\norg.apache.hadoop.mapreduce.Mapper\nclass which is a generic type/generic class with four type parameters:\n\ninput key type\ninput value type\noutput key type\noutput value type\n\nThe designer/developer implements the map() method, that is automatically called by the framework for each (key, value) pair of the input file.\nThe map() method\n\nProcesses its input (key, value) pairs by using standard Java code\nEmits (key, value) pairs by using the context.write(key, value) method\n\n\n\nReducer class\nThe Reducer class extends the\norg.apache.hadoop.mapreduce.Reducer\nclass, which is a generic type/generic class with four type parameters:\n\ninput key type\ninput value type\noutput key type\noutput value type\n\nThe designer/developer implements the reduce() method, that is automatically called by the framework for each (key, [list of values]) pair obtained by aggregating the output of the mapper(s).\nThe reduce() method\n\nProcesses its input (key, [list of values]) pairs by using standard Java code\nEmits (key, value) pairs by using the context.write(key, value) method\n\n\n\nData Types\nHadoop has its own basic data types optimized for network serialization\n\norg.apache.hadoop.io.Text: like Java String\norg.apache.hadoop.io.IntWritable: like Java Integer\norg.apache.hadoop.io.LongWritable: like Java Long\norg.apache.hadoop.io.FloatWritable: like Java Float\n…\n\nThe basic Hadoop data types implement the org.apache.hadoop.io.Writable and org.apache.hadoop.io.WritableComparable interfaces\n\nAll classes (data types) used to represent keys are instances of WritableComparable: keys must be “comparable” for supporting the sort and shuffle phase\nAll classes (data types) used to represent values are instances of Writable: usually, they are also instances of WritableComparable even if it is not indispensable\n\nDevelopers can define new data types by implementing the org.apache.hadoop.io.Writable and/or org.apache.hadoop.io.WritableComparable interfaces, allowing to manage complex data types.\n\n\nInput: InputFormat\nThe input of the MapReduce program is an HDFS file (or an HDFS folder), but the input of the Mapper is a set of (key, value) pairs.\nThe classes extending the org.apache.hadoop.mapreduce.InputFormat abstract class are used to read the input data and “logically transform” the input HDFS file in a set of (key, value) pairs.\nInputFormat describes the input-format specification for a MapReduce application and processes the input file(s). The InputFormat class is used to\n\nRead input data and validate the compliance of the input file with the expected input-format\nSplit the input file(s) into logical Input Splits, each of which is then assigned to an individual Mapper\nProvide the RecordReader implementation to be used to divide the logical input split in a set of (key,value) pairs (also called records) for the mapper\n\n\n\nGetting data to the Mapper\n\n\n\nInputFormat identifies partitions of the data that form an input split\n\nEach input split is a (reference to a) part of the input file processed by a single mapper\nEach split is divided into records, and the mapper processes one record (i.e., a (key,value) pair) at a time\n\nA set of predefined classes extending the InputFormat abstract class are available for standard input file formats\n\nTextInputFormat: InputFormat for plain text files\nKeyValueTextInputFormat: another InputFormat for plain text files\nSequenceFileInputFormat: an InputFormat for sequential/binary files\n…\n\n\nTextInputFormat\nTextInputFormat is an InputFormat for plain text files. Files are broken into lines, where either linefeed or carriage-return are used to signal end of line. One pair (key, value) is emitted for each line of the file:\n\nKey is the position (offset) of the line in the file\nValue is the content of the line\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nGetting data to the Mapper\n\n\n\n\n\n\n\n\nKeyValueTextInputFormat\nKeyValueTextInputFormat is an InputFormat for plain text files, where each line must have the format\nkey<separator>value\nand the default separator is tab (\\t).\nFiles are broken into lines, and either linefeed or carriage-return are used to signal end of line, and each line is split into key and value parts by considering the separator symbol/character.\nOne pair (key, value) is emitted for each line of the file\n\nKey is the text preceding the separator\nValue is the text following the separator\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nGetting data to the Mapper\n\n\n\n\n\n\n\n\n\nOutput: OutputFormat\nThe classes extending the org.apache.hadoop.mapreduce.OutputFormat abstract class are used to write the output of the MapReduce program in HDFS.\nA set of predefined classes extending the OutputFormat abstract class are available for standard output file formats\n\nTextOutputFormat: an OutputFormatfor plain text files\nSequenceFileOutputFormat: an OutputFormatfor sequential/binary files\n…\n\n\nTextOutputFormat\nTextOutputFormat is an OutputFormat for plain text files: for each output (key, value) pair, TextOutputFormat writes one line in the output file. In particular, the format of each output line is\n\"key\\tvalue\\n\"\n\n\n\n\nStructure of a MapReduce program in Hadoop\nAlways start from these templates. The parts of the code that should be changed to customize the Hadoop application are highlighted using notes.\n\nDriver\n#/* Set package */ // # <1> \npackage it.polito.bigdata.hadoop.mypackage; \n\n/* Import libraries */\nimport java.io.IOException;\n\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\n/* Driver class */\n#public class MapReduceAppDriver extends Configured implements Tool { // # <2> \n    @Override\n    public int run(String[] args) throws Exception {\n        /* variables */\n        int exitCode; \n        //...\n\n        // Parse parameters\n        numberOfReducers = Integer.parseInt(args[0]); // Number of instances of the Reducer class\n        inputPath = new Path(args[1]); // Can be the path to a folder or to a file. If this is a folder path, the application will read all the files in it\n        outputDir = new Path(args[2]); // This is always the path to a folder\n\n        // **********************\n        //     JOB\n        // **********************\n        // Define and configure a new job \n        Configuration conf = this.getConf(); // Create a configuration object to design in it the application configuration\n        Job job = Job.getInstance(conf); // Creation of the job, that is the application instance\n\n        // Assign a name to the job\n        #job.setJobName(\"My First MapReduce program\"); // # <3>\n\n        // Set path of the input file/folder (if it is a folder, the job reads all the files in the specified folder) for this job\n        FileInputFormat.addInputPath(job, inputPath);\n\n        // Set path of the output folder for this job\n        FileOutputFormat.setOutputPath(job, outputDir);\n\n        // Set input format\n        // TextInputFormat = textual files; the input types are (keys: LongWritable, values: text)\n        // KeyValueTextInputFormat = textual files; the input types are (keys: text, values: text)\n        #job.setInputFormatClass(TextInputFormat.class); // This class also includes the information about the type of input data // # <4>\n\n        // Set job output format\n        #job.setOutputFormatClass(TextOutputFormat.class); // # <5>\n\n        // **********************\n        //   DRIVER\n        // **********************\n        // Specify the class of the Driver for this job\n        #job.setJarByClass(MapReduceAppDriver.class); // # <6>\n\n        // **********************\n        //   MAPPER\n        // **********************\n        // Set mapper class\n        #job.setMapperClass(MyMapperClass.class); // # <7>\n        \n        // Set map output key and value classes; these are also the key - value types of the reduces class\n        #job.setMapOutputKeyClass(output key type.class); // where type changes depending on the type (e.g., text, IntWritable) // # <8>\n        #job.setMapOutputValueClass(output value type.class); // # <9>\n\n        // **********************\n        //   REDUCER\n        // **********************\n        // Set reduce class\n        #job.setReducerClass(MyReducerClass.class); // # <10>\n\n        // Set reduce output key and value classes\n        #job.setOutputKeyClass(output key type.class); // # <11>\n        #job.setOutputValueClass(output value type.class); // # <12>\n\n        // Set number of reducers\n        job.setNumReduceTasks(numberOfReducers);\n\n        // **********************\n        //   OTHER\n        // **********************\n        // Execute the job and wait for completion\n        if (job.waitForCompletion(true)==true) // with this method the application is run\n            exitCode=0;\n        else\n            exitCode=1;\n        return exitCode;\n    } // End of the run method\n\n    /* main method of the driver class */\n    public static void main(String args[]) throws Exception { // This part of the code is always the same\n        /* Exploit the ToolRunner class to \"configure\" and run the Hadoop application */\n        int res = ToolRunner.run(\n            new Configuration(), \n            #new MapReduceAppDriver(), // # <13>\n            args\n        );\n        System.exit(res);\n    } // End of the main method\n} // End of public class MapReduceAppDriver\n\nmypackage\nMapReduceAppDriver\n\"My First MapReduce program\"\nTextInputFormat\nTextInputFormat\nMapReduceAppDriver\nMyMapperClass\noutput value type\noutput value type\nMyReducerClass\noutput value type\noutput value type\nMapReduceAppDriver\n\n\n\nMapper\n/* Set package */\n#package it.polito.bigdata.hadoop.mypackage; // # <1>\n\n/* Import libraries */\nimport java.io.IOException;\n\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.io.*;\n\n/* Mapper Class */\n#class myMapperClass extends Mapper< // Mapper is a template // # <2>\n    #MapperInputKeyType, // Input key type (must be consistent with the InputFormat class specified in the Driver) // # <3>\n    #MapperInputValueType, // Input value type (must be consistent with the InputFormat class specified in the Driver) // # <4>\n    #MapperOutputKeyType, // Output key type // # <5>\n    #MapperOutputValueType // Output value type // # <6>\n>{\n/* Implementation of the map method */\n    protected void map(\n        #MapperInputKeyType key, // Input key // # <7>\n        #MapperInputValueType value, // Input value // # <8>\n        Context context // This is an object containing the write method, that has to be invoked to return the (key, value) pairs\n    ) throws IOException, InterruptedException {\n\n        /* \n        Process the input (key, value) pair and emit a set of (key,value) pairs. \n        context.write(...) is used to emit (key, value) pairs context.write(new outputkey, new outputvalue); \n        */ \n\n        #context.write(new outputkey, new outputvalue); // # <9>\n        // Notice context.write(...) has to be invoked a number of times equal to the number of (key, value) pairs have to be returned. Even 0 times is accepted\n\n        // In the mapper instance also setup and cleanup methods can be implemented, but are not mandatory. Instead, the map method is mandatory\n\n    } // End of the map method\n} // End of class myMapperClass\n\nmypackage\nmyMapperClass\nMapperInputKeyType\nMapperInputValueType\nMapperOutputKeyType\nMapperOutputValueType\nMapperInputKeyType\nMapperInputValueType\noutputkey and outputvalue\n\n\n\nReducer\n/* Set package */\n#package it.polito.bigdata.hadoop.mypackage; // # <1>\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.io.*;\n\n/* Reducer Class */\n#class myReducerClass extends Reducer< // Reducer is a template // # <2>\n    #ReducerInputKeyType, // Input key type (must be consistent with the OutputKeyType of the Mapper) // # <3>\n    #ReducerInputValueType, // Input value type (must be consistent with the OutputValueType of the Mapper) // # <4>\n    #ReducerOutputKeyType, // Output key type (must be consistent with the OutputFormat class specified in the Driver) // # <5>\n    #ReducerOutputValueType // Output value type (must be consistent with the OutputFormat class specified in the Driver) // # <6>\n>{\n    /* Implementation of the reduce method */\n    protected void reduce(\n        #ReducerInputKeyType key, // Input key // # <7>\n        Iterable<ReducerInputValueType> values, // Input values (list of values). Notice that since this is an Iterable it is not possible to iterate over it more than once (it works like a Python generator). This is done because the iterable can read directly the list from the file system #without moving and storing the data on the local server // # <8>\n        Context context\n    ) throws IOException, InterruptedException {\n\n        /* \n        Process the input (key, [list of values]) pair and emit a set of (key,value) pairs. \n        context.write(...) is used to emit (key, value) pairs context.write(new outputkey, new outputvalue); \n        */\n\n        #context.write(new outputkey, new outputvalue); // # <9>\n        // Notice context.write(...) has to be invoked a number of times equal to the number of (key, value) pairs have to be returned. Even 0 times is accepted\n        // \"new\" has to be always specified\n\n    } // End of the reduce method\n} // End of class myReducerClass\n\nmypackage\nmyReducerClass\nReducerInputKeyType\nReducerInputValueType\nReducerOutputKeyType\nReducerOutputValueType\nReducerInputKeyType\nReducerInputValueType\noutputkey and outputvalue\n\n\n\nExample of a MapReduce program in Hadoop: Word Count\nThe Word count problem consists of\n\nInput: (unstructured) textual file, where each line of the input file can contains a set of words\nOutput: number of occurrences of each word appearing in the input file\nParameters/arguments of the application:\n\nargs[0]: number of instances of the reducer\nargs[1]: path of the input file\nargs[2]: path of the output folder\n\n\n\n\n\n\n\n\nWord Count input and output examples\n\n\n\n\n\nInput file\nToy example file for Hadoop. Hadoop running example.\nOutput file\n(toy,1)\n(example,2)\n(file,1)\n(for,1)\n(hadoop,2)\n(running,1)\n\n\n\n\nDriver\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\n/* Driver class */\npublic class WordCount extends Configured implements Tool {\n    @Override\n    public intrun(String[] args) throws Exception {\n        Path inputPath;\n        Path outputDir;\n        int numberOfReducers;\n        int exitCode; \n        \n        // Parse input parameters\n        numberOfReducers = Integer.parseInt(args[0]);\n        inputPath = new Path(args[1]);\n        outputDir = new Path(args[2]);\n\n        // Define and configure a new job\n        Configuration conf = this.getConf();\n        Job job = Job.getInstance(conf); \n        \n        // Assign a name to the job\n        job.setJobName(\"WordCounter\");\n\n        // Set path of the input file/folder (if it is a folder, the job reads all the files in the specified folder) for this job\n        FileInputFormat.addInputPath(job, inputPath);\n\n        // Set path of the output folder for this job\n        FileOutputFormat.setOutputPath(job, outputDir);\n\n        // Set input format\n        // TextInputFormat = textual files \n        job.setInputFormatClass(TextInputFormat.class);\n\n        // Set job output format\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        // Specify the class of the Driver for this job\n        job.setJarByClass(WordCount.class);\n\n        // Set mapper class\n        job.setMapperClass(WordCountMapper.class);\n        \n        // Set map output key and value classes\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        // Set reduce class\n        job.setReducerClass(WordCountReducer.class);\n        \n        // Set reduce output key and value classes\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // Set number of reducers\n        job.setNumReduceTasks(numberOfReducers);\n\n        // Execute the job and wait for completion\n        if (job.waitForCompletion(true)==true)\n            exitCode=0;\n        else\n            exitCode=1;\n        return exitCode;\n    } // End of the run method\n\n    /* main method of the driver class */\n    public static void main(String args[]) throws Exception {\n\n        /* Exploit the ToolRunner class to \"configure\" and run the \n        Hadoop application */\n    \n        intres = ToolRunner.run(\n            new Configuration(), \n            new WordCount(), \n            args\n        );\n        System.exit(res);\n    } // End of the main method\n} // End of public class WordCount\n\n\nMapper\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\n/* MapperClass */\nclass WordCountMapper extends Mapper<\n    LongWritable, // Input key type\n    Text, // Input value type\n    Text, // Output key type\n    IntWritable // Output value type\n>{\n    /* Implementation of the map method */\n    protected void map(\n        LongWritable key, // Input key type\n        Text value, // Input value type\n        Context context\n    ) throws IOException, InterruptedException {\n        // Split each sentence in words. Use whitespace(s) as delimiter\n        // The split method returns an array of strings\n        String[] words = value.toString().split(\"\\\\s+\");\n\n        // Iterate over the set of words\n        for(String word : words) {\n            // Transform word case\n            String cleanedWord = word.toLowerCase();\n\n            // emit one pair (word, 1) for each input word\n            context.write(new Text(cleanedWord), new IntWritable(1));\n        }\n    } // End map method\n} // End of class WordCountMapper\n\n\nReducer\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\n/* Reducer Class */\nclass WordCountReducer extends Reducer<\nText, // Input key type\nIntWritable, // Input value type\nText, // Output key type\nIntWritable // Output value type\n>{\n    /* Implementation of the reduce method */\n    protected void reduce(\n        Text key, // Input key type\n        Iterable<IntWritable> values, // Input value type\n        Context context\n    ) throws IOException, InterruptedException{\n        int occurrances= 0;\n        \n        // Iterate over the set of values and sum them \n        for (IntWritable value : values) {\n            occurrances = occurrances+ value.get();\n        }\n\n        // Emit the total number of occurrences of the current word\n        context.write(key, new IntWritable(occurrances));\n    } // End reduce method\n} // End of class WordCountReducer\n\n\n\n\nCombiner\nIn standard MapReduce applications, the (key,value) pairs emitted by the Mappers are sent to the Reducers through the network. However, some pre-aggregations could be performed to limit the amount of network data by using Combiners (also called “minireducers”).\nConsider the standard word count problem, and suppose that the input file is split in two input splits, hence, two Mappers are instantiated (one for each split).\n\n\nWord count without Combiner\n\n\n\nA combiner can be locally called on the output (key, value) pairs of each mapper (it works on data stored in the main-memory or on the local hard disks) to pre-aggregate data, reducing the data moving through the network.\n\n\nWord count with Combiner\n\n\n\nSo, in MapReduce applications that include Combiners after the Mappers, the (key,value) pairs emitted by the Mappers are analyzed in main-memory (or on the local disk) and aggregated by the Combiners. Each Combiner pre-aggregates the values associated with the pairs emitted by the Mappers of a cluster node, limiting the amount of network data generated by each cluster node.\n\n\n\n\n\n\nCombiner scope of application\n\n\n\n\nCombiners work only if the reduce function is commutative and associative.\nThe execution of combiners is not guaranteed: Hadoop decides at runtime if executing a combiner, and so the user cannot be sure of the combiner execution just by checking the code. Because of this, the developer/designer should write MapReduce jobs whose successful executions do not depend on whether the Combiner is executed.\n\n\n\n\nCombiner (instance)\nThe Combiner is an instance of the org.apache.hadoop.mapreduce.Reducer class. Notice that there is not a specific combiner-template class.\n\nIt “implements” a pre-reduce phase that aggregates the pairs emitted in each node by Mappers\nIt is characterized by the reduce() method\nIt processes (key, [list of values]) pairs and emits (key, value) pairs\nIt runs on the cluster\n\n\n\nCombiner class\nThe Combiner class extends the org.apache.hadoop.mapreduce.Reducer class, that is a generic type/generic class with four type parameters:\n\ninput key type\ninput value type\noutput key type\noutput value type\n\nNotice that the output data types are the same as the output data types.\nCombiners and Reducers extend the same class, and the designer/developer implements the reduce() method also for the Combiner instances. The Combiner is automatically called by Hadoop for each (key, [list of values]) pair obtained by aggregating the local output of a Mapper.\nThe Combiner class is specified by using the job.setCombinerClass() method in the run method of the Driver (i.e., in the job configuration part of the code).\n\n\nExample: adding the Combiner to the Word Count problem\nConsider the word count problem (see Section 6.0.3.4 for details), to add the combiner to solution seen before:\n\nSpecify the combiner class in the Driver\nDefine the Combiner class. The reduce method of the combiner aggregates local pairs emitted by the mappers of a single cluster node, and emits partial results (local number of occurrences for each word) from each cluster node that is used to run the application.\n\n\nSpecify combiner class in the Driver\nAdd the call to the combiner class in the Driver, before the return around line 68\n        // Set combiner class\n        job.setCombinerClass(WordCountCombiner.class);\n\n\nDefine the Combiner class\n/* Set package */\npackage it.polito.bigdata.hadoop.wordcount;\n\n/* Import libraries */\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\n/* Combiner Class */\nclass WordCountCombiner extends Reducer<\n    Text, // Input key type\n    IntWritable, // Input value type\n    Text, // Output key type\n    IntWritable // Output value type\n>{\n/* Implementation of the reduce method */\n    protected void reduce(\n        Text key, // Input key type\n        Iterable<IntWritable> values, // Input value type\n        Context context\n    ) throws IOException, InterruptedException{\n        int occurrances= 0;\n        // Iterate over the set of values and sum them \n        for (IntWritable value : values) {\n            occurrances = occurrances+ value.get();\n        }\n        // Emit the total number of occurrences of the current word\n        context.write(key, new IntWritable(occurrances));\n    } // End reduce method\n} // End of class WordCountCombiner\n\n\n\nFinal thoughts\nThe reducer and the combiner classes perform the same computation (the reduce method of the two classes is the same). Indeed, the developer/designer does not really need two different classes: he can simply specify that WordCountReducer is also the combiner class, for example by adding in the driver\n        // Set combiner class\n        job.setCombinerClass(WordCountReducer.class);\n        // \"WordCountReducer.class\" instead of \"WordCountCombiner.class\"\nIn 99% of the Hadoop applications the same class can be used to implement both combiner and reducer.\n\n\n\nPersonalized Data Types\nPersonalized Data Types are useful when the value of a key-value pair is a complex data type. Personalized Data Types are defined by implementing the org.apache.hadoop.io.Writable interface. To properly serialize the input-output data, the following methods must be implemented\n\npublic void readFields(DataInput in)\npublic void write(DataOutput out)\n\nTo properly format the output of the job (i.e., the output of the reducer) usually also the following method is “redefined”\n\npublic String toString()\n\nIf also a constructor is defined, remember to define also an empty constructor, otherwise the system will raise an error at runtime.\n\nExample\nSuppose to be interested in complex values composed of two parts, such as a counter (int) and a sum (float). In this case, an ad-hoc Data Type can be used to implement this complex data type in Hadoop.\n/* Set package */\n\npackage it.polito.bigdata.hadoop.combinerexample;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class SumAndCountWritable implements \norg.apache.hadoop.io.Writable {\n    /* Private variables */\n    private float sum = 0;\n    private int count = 0;\n\n    /* Methods to get and set private variables of the class */\n    public float getSum() {\n        return sum;\n    }\n\n    public void setSum(float sumValue) {\n        sum=sumValue;\n    }\n\n    public int getCount() {\n        return count;\n    }\n\n    public void setCount(int countValue) {\n        count=countValue;\n    }\n\n    /* Methods to serialize and deserialize the contents of the \n    instances of this class */\n    @Override /* Serialize the fields of this object to out */\n    public void write(DataOutput out) throws IOException {\n        out.writeFloat(sum);\n        out.writeInt(count);\n    }\n\n    @Override /* Deserialize the fields of this object from in */\n    public void readFields(DataInput in) throws IOException {\n        sum=in.readFloat(); // <1>\n        count=in.readInt();\n    }\n\n    /* Specify how to convert the contents of the instances of this \n    class to a String\n    * Useful to specify how to store/write the content of this class\n    * in a textual file */\n    public String toString()\n    {\n        String formattedString=\n        new String(\"sum=\"+sum+\",count=\"+count);\n        return formattedString;\n    }\n}\n\nNotice that the order of the read must be coherent with the order of the write.\n\n\n\nComplex keys\nPersonalized Data Types can be used also to manage complex keys. In that case the Personalized Data Type must implement the org.apache.hadoop.io.WritableComparable interface, since keys must be\n\nCompared/sorted: it is possible by implementing the compareTo() method; it is used by the Combiner locally.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n@Override\npublic int compareTo(IntPair ip) {\n    int cmp = compare(first, ip.first);\n    if (cmp != 0) {\n        return cmp;\n    }\n    return compare(second, ip.second);\n}\n/**\n* Convenience method for comparing two ints.\n*/\npublic static int compare(int a, int b) {\n    return (a < b ? -1 : (a == b ? 0 : 1));\n}\n\n\n\n\nSplit in groups: it is possible by implementing the hashCode() method; it is used by the Reducer on the networks.\n\n\n\n\nSharing parameters among Driver, Mappers, and Reducers\nThe configuration object is used to share the (basic) configuration of the Hadoop environment across the driver, the mappers and the reducers of the application/job. It stores a list of (property-name, property-value) pairs.\nAlso, personalized (property-name, property-value) pairs can be specified in the driver, and they can be used to share some parameters of the application with mappers and reducers. The personalized (property-name, property-value) pairs are useful to define shared small (constant) properties that are available only during the execution of the program. The driver sets these parameters, and Mappers and Reducers can access them, however they cannot modify them.\n\nHow to use these parameters\nIn the driver\n\nRetrieve the configuration object\n\nConfiguration conf = this.getConf();\n\nSet personalized properties\n\nconf.set(\"property-name\", \"value\");\nIn the Mapper and/or Reducer\ncontext.getConfiguration().get(\"property-name\")\nThis method returns a String containing the value of the specified property.\n\n\n\nCounters\nHadoop provides a set of basic, built-in, counters to store some statistics about jobs, mappers, reducers, for example\n\nnumber of input and output records (i.e., pairs)\nnumber of transmitted bytes\n\nAlso other ad-hoc, user-defined, counters can be defined to compute global “statistics” associated with the goal of the application.\n\nUser-defined counters\nUser-defined counters are defined by means of Java enum, and each application can define an arbitrary number of enums. The name of the enum is the group name, and each enum has a number of “fields”, which are the counter names.\nCounters are incremented in the Mappers and Reducers by using the increment() method\ncontext.getCounter(countername).increment(value);\nThe global/final value of each counter, which is available at the end of the job, is then stored/printed by the Driver (at the end of the execution of the job). Driver can retrieve the final values of the counters using the getCounters() and findCounter() methods.\nUser-defined counters can be also defined on the fly by using the method incrCounter(\"group name\", \"counter name\", value). Dynamic counters are useful when the set of counters is unknown at design time.\n\nExample\nIn the driver, add\npublic static enum COUNTERS {\n    ERROR_COUNT,\n    MISSING_FIELDS_RECORD_COUNT\n}\nThis enum defines two counters\n\nCOUNTERS.ERROR_COUNT\nCOUNTERS.MISSING_FIELDS_RECORD_COUNT\n\nTo increment the COUNTERS.ERROR_COUNT counter in the mapper or the reducer, use\ncontext.getCounter(COUNTERS.ERROR_COUNT).increment(1);\nTo retrieve the final value of the COUNTERS.ERROR_COUNT counter in the driver, use\nCounter errorCounter = job.getCounters().findCounter(COUNTERS.ERROR_COUNT);\n\n\n\n\nMap-only job\nIn some applications, all the work can be performed by the mapper(s) (e.g., record filtering applications): Hadoop allows executing Map-only jobs, avoiding the reduce phase, and also the shuffle and sort phase.\nThe output of the map job is directly stored in HDFS, since the set of pairs emitted by the map phase is already the final output.\n\nImplementation of a Map-only job\nTo implement a Map-only job\n\nImplement the map method\nSet the number of reducers to 0 during the configuration of the job (in the driver), writing\n\njob.setNumReduceTasks(0);\n\n\n\nIn-Mapper combiner\nMapper classes are also characterized by a setup and a cleanup method, which are empty if they are not overridden.\n\nSetup method\nThe setup method is called once for each mapper prior to the many calls of the map method. It can be used to set the values of in-mapper variables, which are used to maintain in-mapper statistics and preserve the state (locally for each mapper) within and across calls to the map method.\n\n\nCleanup method\nThe map method, invoked many times, updates the value of the in-mapper variables. Each mapper (each instance of the mapper class) has its own copy of the in-mapper variables.\nThe cleanup method is called once for each mapper after the many calls to the map method, and it can be used to emit (key,value) pairs based on the values of the in-mapper variables/statistics.\nAlso the reducer classes are characterized by a setup and a cleanup method.\n\nThe setup method is called once for each reducer prior to the many calls of the reduce method.\nThe cleanup method is called once for each reducer after the many calls of the reduce method.\n\nIn-MapperCombiners are a possible improvement over “standard” Combiners\n\nInitialize a set of in-mapper variables during the instance of the Mapper, in the setup method of the mapper;\nUpdate the in-mapper variables/statistics in the map method. Usually, no (key,value) pairs are emitted in the map method of an in-mapper combiner.\n\nAfter all the input records (input (key, value) pairs) of a mapper have been analyzed by the map method, emit the output (key, value) pairs of the mapper: (key, value) pairs are emitted in the cleanup method of the mapper based on the values of the in-mapper variables\nThe in-mapper variables are used to perform the work of the combiner in the mapper, allowing to improve the overall performance of the application. However, pay attention to the amount of used main memory: each mapper may use a limited amount of main-memory, hence in-mapper variables should be “small” (at least smaller than the maximum amount of memory assigned to each mapper).\n\n\nIn-Mapper combiner: Word count pseudocode\nclass MAPPER\n    method setup\n        A = new AssociativeArray\n    method map(offset key, line l)\n        for all word w ∈ line l do\n            A{w} = A{w} + 1\n    method cleanup\n        for all word w in A do\n            EMIT(term w , count A{w})\n\n\n\nMaven project\n\nStructure\n\nsrc folder: contains the source code. May contain subfolders, but the important point is that it must contain the java files\n\nDriverBigData.java\nMapperBigData.java\nReducerBigData.java\n\ntarget folder:\n\n.jar file: useful to run the application on the cluster. It’s the java archive that collects the three classes of the Hadoop application\n\npom.xml file: used to configure the Hadoop application\n\n\n\nHow to run the project\nUsing Eclipse\n\nselect the Driver .java file\nRight click\nClick “Run As”\nIf the arguments have already been set:\n\nClick “Java Application”\n\nOtherwise\n\nClick “Run Configurations”, to set the arguments\nGo to “Arguments” section, and write the arguments. The arguments are\n\nthe number of reducers: 2\nthe (relative) path of the input folder example_data\nthe (relative) path of the output folder example_data_output\n\n\n\n2 example_data example_data_output\nThe output files are\n\nan empty file “_SUCCESS”, if the application run successfully\none file for each reducer instance: the intersection between the sets of words in each file is empty, which means that all the same words were processed by the same Reducer. For this reason the output is always a folder and not a single file.\n\n\n\nHow to create a .jar file from the project\nUsing Eclipse, to create a .jar file from the project to run the project on the cluster\n\nRight click on the project name (e.g., “MapReduceProject”)\nClick “Runs As”\nClick “Maven build…”\nIn “Goals” write “package”\nClick “Run”\n\n\n\nHow to run the .jar in the BigData@Polito cluster\n\nGo to https://jupyter.polito.it/ (i.e., the server gateway) and connect using the credentials\nCopy the .jar file on server\nUpload the input data in the HDFS\nUse the terminal to run the .jar, using the hadoop command\n\nhadoop jar Exercise1-1.0.0.jar \\\nit.polito.bigdata.hadoop.exercise1.DriverBigData \\\n2 example_data example_data_output\nIn this configuration there are 3 file systems\n\nThe local file system on the personal PC\nThe local file system on the gateway server\nThe distributed file system on the Hadoop cluster (the interface to manage it is https://bigdatalab.polito.it/hue)\n\n\n\n\n\n\n\n\nAn interface is like a template of a class, defining which methods must be implemented to be compliant with the interface↩︎\nSetting the number of Reducers is a balancing problem: having more Reducers decreases the time to aggregate the data, however it also increases the overhead needed to instantiate the Reducers↩︎"
  },
  {
    "objectID": "05_mapreduce_patterns_1.html",
    "href": "05_mapreduce_patterns_1.html",
    "title": "7  MapReduce patterns - 1",
    "section": "",
    "text": "Summarization Patterns\nSummarization Patterns are used to implement applications that produce top-level/summarized view of the data, such as\n\nNumerical summarizations (Statistics)\nInverted index\nCounting with counters\n\n\nNumerical summarizations\nThe goal is to group records/objects by a key field(s) and calculate a numerical aggregate (e.g., average, max, min, standard deviation) per group, to provide a top-level view of large input data sets so that a few high-level statistics can be analyzed by domain experts to identify trends, anomalies, etc.\n\nStructure\n\nMappers output (key, value) pairs where\n\nkey is associated with the fields used to define groups;\nvalue is associated with the fields used to compute the aggregate statistics.\n\nReducers receive a set of numerical values for each “group-by” key and compute the final statistics for each “group”. Combiners can be used to speed up performances, if the computed statistic has specific properties (e.g., it is commutative and associative).\n\n\n\nNumerical summarization structure\n\n\n\n\n\n\n\n\n\nUse cases\n\n\n\n\nWord count\nRecord count (per group)\nMin/Max/Count (per group)\nAverage/Median/Standard deviation (per group)\n\n\n\n\n\n\nInverted index summarization\nThe goal is to build an index from the input data to support faster searches or data enrichment: it maps terms to a list of identifiers to improve search efficiency.\n\nStructure\n\nMappers output (key, value) pairs where\n\nkey is the set of fields to index (a keyword)\nvalue is a unique identifier of the objects to associate with each “keyword”\n\nReducers receive a set of identifiers for each keyword and simply concatenate them\nCombiners are usually not useful when using this pattern, since there are no values to aggregate\n\n\n\nNumerical summarization structure\n\n\n\nA use case is a web search engine (word – List of URLs, i.e. Inverted Index).\n\n\n\nCounting with counters\nThe goal is to compute count summarizations of data sets to provide a top-level view of large data sets, so that few high-level statistics can be analyzed by domain experts to identify trends, anomalies, …\n\nStructure\n\nMappers process each input record and increment a set of counters\nThis is a map-only job: no reducers and no combiners have to be implemented\nThe results are stored/printed by the Driver of the application\n\n\n\nNumerical summarization structure\n\n\n\n\n\n\n\n\n\nUse cases\n\n\n\n\nCount number of records\nCount a small number of unique instances\nSummarizations\n\n\n\n\n\n\n\nFiltering patterns\nAre used to select the subset of input records of interest\n\nFiltering\nTop K\nDistinct\n\n\nFiltering\nThe goal is to filter out input records that are not of interest/keep only the ones that are of interest, to focus the analysis of the records of interest. Indeed, depending on the goals of your application, frequently only a small subset of the input data is of interest for further analyses.\n\nStructure\nThe input of the mapper is a set of records\n\nKey = primary key\nValue = record\n\nMappers output one (key, value) pair for each record that satisfies the enforced filtering rule\n\nKey is associated with the primary key of the record\nValue is associated with the selected record\n\nReducers are useless in this pattern, since a map-only job is executed (number of reduce set to 0).\n\n\nNumerical summarization structure\n\n\n\n\n\n\n\n\n\nUse cases\n\n\n\n\nRecord filtering\nTracking events\nDistributed grep\nData cleaning\n\n\n\n\n\n\nTop K\nThe goal is to select a small set of top K records according to a ranking function to focus on the most important records of the input data set: frequently the interesting records are those ranking first according to a ranking function (i.e., most profitable items, outliers).\n\nStructure\n\nMappers\nEach mapper initializes an in-mapper (local) top k list. k is usually small (e.g., 10), and the current (local) top k-records of each mapper(i.e., instance of the mapper class) can be stored in main memory\n\nThe initialization is performed in the setup method of the mapper\nThe map function updates the current in-mapper top k list\n\nThe cleanup method emits the k (key, value) pairs associated with the in-mapper local top k records\n\nKey is the “null key”\nValue is a in-mapper top k record\n\n\n\nReducer\nA single reducer must be instantiated (i.e., one single instance of the reducer class). One single global view over the intermediate results emitted by the mappers to compute the final top k records. It computes the final top k list by merging the local lists emitted by the mappers. All input (key, value) pairs have the same key, hence the reduce method is called only once\n\n\nNumerical summarization structure\n\n\n\n\n\n\n\n\n\nUse cases\n\n\n\n\nOutlier analysis (based on a ranking function)\nSelect interesting data (based on a ranking function)\n\n\n\n\n\n\n\nDistinct\nThe goal is to find a unique set of values/records, since in some applications duplicate records are useless (actually duplicated records are frequently useless).\n\nMappers emit one (key, value) pair for each input record\n\nKey = input record\nValue = null value\n\nReducers emit one (key, value) pair for each input (key, list of values) pair\n\nKey = input key, (i.e., input record)\nValue = null value\n\n\n\n\nNumerical summarization structure\n\n\n\n\n\n\n\n\n\nUse cases\n\n\n\n\nDuplicate data removal\nDistinct value selection"
  },
  {
    "objectID": "06_mapreduce_advanced_topics.html",
    "href": "06_mapreduce_advanced_topics.html",
    "title": "8  MapReduce and Hadoop Advanced Topics",
    "section": "",
    "text": "Multiple inputs\nIn some applications data are read from two or more datasets, also having different formats.\nHadoop allows reading data from multiple inputs (multiple datasets) with different formats by specifying one mapper for each input dataset. However, the key-value pairs emitted by the mappers must be consistent in terms of data types.\n\n\n\n\n\n\nUse case\n\n\n\nThe input data is collected from different sensors: all sensors measure the same “measure”, but sensors developed by different vendors use a different data format to store the gathered data/measurements.\n\n\nIn the driver use the addInputPath method of the MultipleInputs class multiple times to\n\nAdd one input path at a time\nSpecify the input format class for each input path\nSpecify the Mapper class associated with each input path\n\n\nExample: multiple inputs\nMultipleInputs.addInputPath(\n    job, \n    new Path(args[1]), // <1>\n    TextInputFormat.class, // <2>\n    Mapper1.class // <3>\n);\n\nMultipleInputs.addInputPath(\n    job, \n    new Path(args[2]), // <1>\n    TextInputFormat.class, // <2>\n    Mapper2.class // <4>\n);\n\nSpecify two input paths (args[1] and args[2])\nThe data of both paths are read by using the TextInputFormat class\nMapper1 is the class used to manage the input key-value pairs associated with the first path\nMapper2 is the class used to manage the input key-value pairs associated with the second path\n\n\n\n\nMultiple outputs\nIn some applications it could be useful to store the output key-value pairs of a MapReduce application in different files. Each file contains a specific subset of the emitted key-value pairs, based on some rules (usually this approach is useful for splitting and filtering operations), and each file name has a prefix that is used to specify the “content” of the file.\nAll the files are stored in one single output directory: there aren’t multiple output directories, but only multiple output files with different prefixes.\nHadoop allows specifying the prefix of the output files: the standard prefix is “part-” (see the content of the output directory of some of the previous applications).\nThe MultipleOutputs class is used to specify the prefixes of the output files\n\nOne different prefix for each “type” of output file\nThere will be one output file of each type for each reducer (for each mapperfor map-only jobs)\n\n\nDriver\nUse the method MultipleOutputs.addNamedOutput multiple times in the Driver to specify the prefixes of the output files. This method has 4 parameter\n\nThe job object\nThe “name/prefix” of MultipleOutputs\nThe OutputFormat class\nThe key output data type class\nThe value output data type class\n\nCall this method one time for each “output file type”\n\nExample: multiple outputs\nMultipleOutputs.addNamedOutput(\n    job, \n    \"hightemp\", \n    TextOutputFormat.class, \n    Text.class, \n    NullWritable.class\n);\n\nMultipleOutputs.addNamedOutput(\n    job, \n    \"normaltemp\", \n    TextOutputFormat.class, \n    Text.class, \n    NullWritable.class\n);\nThis example defines two types of output files\n\nThe first type of output files while have the prefix \"hightemp\"\nThe second type of output files while have the prefix \"normaltemp\"\n\n\n\n\nMap-only\nDefine a private MultipleOutputs variable in the mapper if the job is a map-only job (in the reducer otherwise)\nprivate MultipleOutputs<Text, NullWritable> mos = null;\nCreate an instance of the MultipleOutputs class in the setup method of the mapper (or in the reducer)\nmos = new MultipleOutputs<Text, NullWritable>(context);\nUse the write method of the MultipleOutputs object in the map method (or in the reduce method) to write the key-value pairs in the file of interest\n\n\nExample: map-only\nmos.write(\"hightemp\", key, value);\nThis example writes the current key-value pair in a file with the prefix \"hightemp-\"\nmos.write(\"normaltemp\", key, value);\nThis example writes the current key-value pair in a file with the prefix \"normaltemp-\"\nClose the MultipleOutputs object in the cleanup method of the mapper (or of the reducer)\nmos.close();\n\n\n\nDistributed cache\nSome applications need to share and cache (small) read-only files to perform efficiently their task. These files should be accessible by all nodes of the cluster in an efficient way, hence a copy of the shared/cached (HDFS) files should be available locally in all nodes used to run the application.\nDistributedCache is a facility provided by the Hadoop-based MapReduce framework to cache files (e.g., text, archives, jars needed by applications).\n\n\nFigure 8.1: Distributed cache structure\n\n\n\nIn image Figure 8.1, in HDFS disks there are the HDFS file(s) to be shared by means of the distributed cache, while on the disks there are local copies of the file(s) shared by means of the distributed cache. A local copy of the file(s) shared by means of the distributed cache is created only in the servers running the application that uses the shared file(s).\nIn the Driver of the application, the set of shared/cached files are specifiedby using the job.addCacheFile(path) method. During the initialization of the job, Hadoop creates a “local copy” of the shared/cached files in all nodes that are used to execute some tasks (mappers or reducers) of the job (i.e., of the running application). The shared/cache file is read by the mapper (or the reducer), usually in its setup method, since the shared/cached file is available locally in the used nodes/servers, its content can be read efficiently.\nThe efficiency of the distributed cache depends on the number of multiple mappers (or reducers) running on the same node/server: for each node a local copy of the file is copied during the initialization of the job, and the local copy of the file is used by all mappers (reducers) running on the same node/server.\nWithout the distributed cache, each mapper (reducer) should read, in the setup method, the shared HDFS file, hence, more time is needed because reading data from HDFS is more inefficient than reading data from the local file system of the node running the mappers (reducers).\n\nExample: distributed cache\n\nDriver\npublic int run(String[] args) throws Exception {\n    //...\n\n    // Add the shared/cached HDFS file in the \n    // distributed cache\n    job.addCacheFile(new Path(\"hdfs path/filename\").toUri());\n\n    //...\n}\n\n\nMapper/Reducer\nprotected void setup(Context context) throws IOException, InterruptedException{\n\n    String line;\n    // Retrieve the (original) paths of the distributed files\n    URI[] urisCachedFiles = context.getCacheFiles();\n\n    // Read the content of the cached file and process it.\n    // In this example the content of the first shared file is opened.\n    BufferedReaderfile = new BufferedReader(\n        new FileReader(\n            new File(\n                new Path(urisCachedFiles[0].getPath()).getName()\n            )\n        )\n    );\n\n    // Iterate over the lines of the file\n    while ((line = file.readLine()) != null) {\n        // process the current line\n        //...\n    }\n    file.close();\n}\nNotice that .getName() retrieves the name of the file. The shared file is stored in the root of a local temporary folder (one for each server that is used to run the application) associated with the distributed cache. The path of the original folder is different from the one used to store the local copy of the shared file."
  },
  {
    "objectID": "07_mapreduce_patterns_2.html",
    "href": "07_mapreduce_patterns_2.html",
    "title": "9  MapReduce patterns - 2",
    "section": "",
    "text": "Data organization patterns\nData organization patterns are used to reorganize/split in subsets the input data\n\nBinning\nShuffling\n\nThe output of an application based on an organization pattern is usually the input of another application(s)\n\nBinning\nThe goal is to organize/move the input records into categories, to partition a big data set into distinct, smaller data sets (“bins”) containing similar records. Each partition is usually the input of a following analysis.\nThis is done because the input data set contains heterogonous data, but each data analysis usually is focused only on a specific subsets of the data.\n\nStructure\nBinning is based on a Map-only job\n\nDriver sets the list of “bins/output files” by means of MultipleOutputs\nMappers select, for each input (key, value) pair, the output bin/file associated with it and emit a (key,value) in that file\n\nkey of the emitted pair is key of the input pair\nvalue of the emitted pair is value of the input pair\n\nNo Combiner or Reducer is used in this pattern\n\n\n\nBinning structure\n\n\n\n\n\n\nShuffling\nThe goal is to randomize the order of the data (records), for anonymization reasons or for selecting a subset of random data (records).\n\nStructure\n\nMappers emit one (key, value) for each input record\n\nkey is a random key (i.e., a random number)\nvalue is the input record\n\nReducers emit one (key, value) pair for each value in [list-of-values] of the input (key, [list-of-values]) pair\n\n\n\nShuffling structure\n\n\n\n\n\n\n\nMetapatterns\nMetapatterns are used to organize the workflow of a complex application executing many jobs\n\nJob Chaining\n\n\nJob Chaining\nThe goal is to execute a sequence of jobs (synchronizing them). Job chaining allows to manage the workflow of complex applications based on many phases (iterations). Each phase is associated with a different MapReduce Job (i.e., one sub-application), and the output of a phase is the input of the next one. This is done because real application are usually based on many phases.\n\nStructure\n\nThe (single) Driver contains the workflow of the application and executes the jobs in the proper order\nMappers, reducers, and combiners: each phase of the complex application is implement by a MapReduce Job (i.e., it is associated with a mapper, a reducer, and a combiner, if it is useful)\n\n\n\nJob chaining structure\n\n\n\nMore complex workflows, which execute jobs in parallel, can also be implemented, however, the synchronization of the jobs become more complex.\n\n\n\n\nJoin patterns\nAre use to implement the join operators of the relational algebra (i.e., the join operators of traditional relational databases)\n\nReduce side join\nMap side join\n\nThe explanation will focus on the natural join however, the pattern is analogous for the other types of joins (theta-, semi-, outer-join).\n\nReduce side natural join\nThe goal is to join the content of two relations (i.e., relational tables) when both tables are large.\n\nStructure\nThere are two mapper classes, that is one mapper class for each table. Mappers emit one (key, value) pair for each input record\n\nKey is the value of the common attribute(s)\nValue is the concatenation of the name of the table of the current record and the content of the current record\n\n\n\n\nExample: join pattern\nSuppose join the following tables have to be joined\n\nUsers with schema userid, name, surname\nLikes with schema userid, movieGenre\n\nThe values userid=u1, name=Paolo, surname=Garza of the Users table will generate the pair\n(userid=u1, \"Users:name=Paolo, surname=Garza\")\nThe values userid=u1, movieGenre=horror of the Likes table will generate the pair\n(userid=u1, \"Likes:movieGenre=horror\")\nThe reducers iterate over the values associated with each key (value of the common attributes) and compute the “local natural join” for the current key. So, they generate a copy for each pair of values such that one record is a record of the first table and the other is the record of the other table.\n\n\nExample\nThe (key, [list of values]) pair\n(userid=u1, [\"User:name=Paolo, surname=Garza\", \"Likes:movieGenre=horror\", \"Likes:movieGenre=adventure\"]) \nwill generate the following output (key,value) pairs\n(userid=u1, \"name=Paolo, surname=Garza, genre=horror\")\n\n(userid=u1, \"name=Paolo, surname=Garza, genre=adventure\")\n\n\nReduce side natural join structure\n\n\n\n\n\nMap side natural join\nThe goal is to join the content of two relations (i.e., relational tables) when one table is large, while the other is small enough to be completely loaded in main memory (frequently one of the two tables is small).\n\nStructure\nThis is a Map-only job\n\nMapper class processes the content of the large table: it receives one input (key,value) pair for each record of the large table, and joins it with the “small” table.\n\nThe distributed cache approach is used to “provide” a copy of the small table to all mappers: each mapper performs the “local natural join” between the current record (of the large table) it is processing and the records of the small table (that is in the distributed cache).\nNotice that the content of the small table (file) is loaded in the main memory of each mapper during the execution of its setup method.\n\n\nMap side natural join structure\n\n\n\n\n\n\nOther join patterns\nThe SQL language is characterized by many types of joins\n\nTheta-join\nSemi-join\nOuter-join\n\nThe same patterns used for implementing the natural join can be used also for the other SQL joins.\nThe “local join” in the reducer of the reduce side natural join (in the mapper of the map side natural join) is replaced with the type of join of interest (theta-, semi-, or outer-join)."
  },
  {
    "objectID": "08_sql_operators_mapreduce.html",
    "href": "08_sql_operators_mapreduce.html",
    "title": "10  Relational Algebra Operations and MapReduce",
    "section": "",
    "text": "The relational algebra and the SQL language have many useful operators\n\nSelection\nProjection\nUnion, intersection, and difference\nJoin (see Join design patterns)\nAggregations and Group by (see the Summarization design patterns)\n\nThe MapReduce paradigm can be used to implement relational operators, however the MapReduce implementation is efficient only when a full scan of the input table(s) is needed (i.e., when queries are not selective and process all data). Selective queries, which return few tuples/records of the input tables, are usually not efficient when implemented by using a MapReduce approach.\nMost preprocessing activities involve relational operators (e.g., ETL processes in the data warehousing application context).\nRelations/Tables (also the big ones) can be stored in the HDFS distributed file system, broken in blocks and spread across the servers of the Hadoop cluster.\nNotice that in relational algebra, relations/tables do not contain duplicate records by definition, and this constraint must be satisfied by both the input and the output relations/tables.\n\nSelection\n\\[\n\\sigma_C(R)\n\\]\nSelection applies predicate (condition) \\(C\\) to each record of table \\(R\\), and produces a relation containing only the records that satisfy predicate \\(C\\).\nThe selection operator can be implemented by using the filtering pattern.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the table Courses\n\n\n\nCCode\nCName\nSemester\nProfID\n\n\n\n\nM2170\nComputer science\n1\nD102\n\n\nM4880\nDigital systems\n2\nD104\n\n\nF1401\nElectronics\n1\nD104\n\n\nF0410\nDatabases\n2\nD102\n\n\n\nFind the courses held in the second semester\n\\[\n\\sigma_{\\textbf{Semester}=2}(\\textbf{Courses})\n\\]\nThe resulting table is\n\n\n\nCCode\nCName\nSemester\nProfID\n\n\n\n\nM4880\nDigital systems\n2\nD104\n\n\nF0410\nDatabases\n2\nD102\n\n\n\n\n\n\nSelection is a map-only job, where each mapper analyzes one record at a time of its split and, if the record satisfies \\(C\\) then it emits a (key,value) pair with key=record and value=null, otherwise, it discards the record.\n\n\nProjection\n\\[\n\\pi_S(R)\n\\]\nProjection, for each record of table \\(R\\), keeps only the attributes in \\(S\\). It produces a relation with a schema equal to \\(S\\) (i.e., a relation containing only the attributes in \\(S\\)), an it removes duplicates, if any.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the table Professors\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nSmith\nElectronics\n\n\n\nFind the surnames of all professors.\n\\[\n\\pi_{\\textbf{PSurname}}(\\textbf{Professors})\n\\]\nThe resulting table is\n\n\n\nPSurname\n\n\n\n\nSmith\n\n\nJones\n\n\n\nNotice that duplicated values are removed.\n\n\n\nIn a projection\n\nEach mapper analyzes one record at a time of its split, and, for each record \\(r\\) in \\(R\\), it selects the values of the attributes in \\(S\\) and constructs a new record \\(r'\\), and emits a (key,value) pair with key=r' and value=null.\nEach reducer emits one (key, value) pair for each input (key, [list of values]) pair with key=r' and value=null.\n\n\n\nUnion\n\\[\nR \\cup S\n\\]\nGiven that \\(R\\) and \\(S\\) have the same schema, an union produces a relation with the same schema of \\(R\\) and \\(S\\). There is a record \\(t\\) in the output of the union operator for each record \\(t\\) appearing in \\(R\\) or \\(S\\). Duplicated records are removed.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the tables DegreeCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\nand MasterCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD101\nRed\nElectronics\n\n\n\nFind information relative to the professors of degree courses or master’s degrees.\n\\[\n\\textbf{DegreeCourseProf} \\cup \\textbf{MasterCourseProf}\n\\]\nThe resulting table is\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\nD101\nRed\nElectronics\n\n\n\n\n\n\nIn a union\n\nMappers, for each input record \\(t\\) in \\(R\\), emit one (key, value) pair with key=t and value=null, and for each input record \\(t\\) in \\(S\\), emit one (key, value) pair with key=t and value=null.\nReducers emit one (key, value) pair for each input (key, [list of values]) pair with key=t and value=null (i.e., one single copy of each input record is emitted).\n\n\n\nIntersection\n\\[\nR \\cap S\n\\]\nGiven that \\(R\\) and \\(S\\) have the same schema, an intersection produces a relation with the same schema of \\(R\\) and \\(S\\). There is a record \\(t\\) in the output of the intersection operator if and only if \\(t\\) appears in both relations (\\(R\\) and \\(S\\)).\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the tables DegreeCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\nand MasterCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD101\nRed\nElectronics\n\n\n\nFind information relative to professors teaching both degree courses and master’s courses.\n\\[\n\\textbf{DegreeCourseProf} \\cap \\textbf{MasterCourseProf}\n\\]\nThe resulting table is\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\n\n\n\n\nIn an intersection\n\nMappers, for each input record \\(t\\) in \\(R\\), emit one (key, value) pair with key=t and value=\"R\", and For each input record \\(t\\) in \\(S\\), emit one (key, value) pair with key=t and value=\"S\".\nReducers emit one (key, value) pair with key=t and value=null for each input (key, [list of values]) pair with [list of values] containing two values. Notice that it happens if and only if both \\(R\\) and \\(S\\) contain \\(t\\).\n\n\n\nDifference\n\\[\nR-S\n\\]\nGiven that \\(R\\) and \\(S\\) have the same schema, a difference produces a relation with the same schema of \\(R\\) and \\(S\\). There is a record \\(t\\) in the output of the difference operator if and only if \\(t\\) appears in \\(R\\) but not in \\(S\\).\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the tables DegreeCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\nand MasterCourseProf\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD102\nSmith\nComputer Engineering\n\n\nD101\nRed\nElectronics\n\n\n\nFind the professors teaching degree courses but not master’s courses.\n\\[\n\\textbf{DegreeCourseProf} - \\textbf{MasterCourseProf}\n\\]\nThe resulting table is\n\n\n\nProfId\nPSurname\nDepartment\n\n\n\n\nD105\nJones\nComputer Engineering\n\n\nD104\nWhite\nElectronics\n\n\n\n\n\n\nIn a difference\n\nMappers, for each input record \\(t\\) in \\(R\\), emit one (key, value) pair with key=t and value=name of the relation (i.e., \\(R\\)). For each input record \\(t\\) in \\(S\\), emit one (key, value) pair with key=t and value=name of the relation (i.e., \\(S\\)). Notice that two mapper classes are needed: one for each relation.\nReducers emit one (key, value) pair with key=t and value=null for each input (key, [list of values]) pair with [list of values] containing only the value \\(R\\). Notice that it happens if and only if \\(t\\) appears in \\(R\\) but not in \\(S\\).\n\n\n\nJoin\nThe join operators can be implemented by using the Join pattern, using the reduce side or the map side pattern depending on the size of the input relations/tables.\n\n\nAggregations and Group by\nAggregations and Group by are implemented by using the Summarization pattern."
  },
  {
    "objectID": "10b_spark_submit_execute.html",
    "href": "10b_spark_submit_execute.html",
    "title": "11  How to submit/execute a Spark application",
    "section": "",
    "text": "Spark submit\nSpark programs are executed (submitted) by using the spark-submit command. It is a command line program, characterized by a set of parameters (e.g., the name of the jar file containing all the classes of the Spark application we want to execute, the name of the Driver class, the parameters of the Spark application).\nspark-submit has also two parameters that are used to specify where the application is executed.\n\nOptions of spark-submit: --master\n--master\nIt specifies which environment/scheduler is used to execute the application\n\n\n\n\n\n\n\nspark://host:port\nThe spark scheduler is used\n\n\nmesos://host:port\nThe mesos scheduler is used\n\n\nyarn\nThe YARN scheduler (i.e., the one of Hadoop)\n\n\nlocal\nThe application is executed exclusively on the local PC\n\n\n\n\n\nOptions of spark-submit: --deploy-mode\n--deploy-mode\nIt specifies where the Driver is launched/executed\n\n\n\n\n\n\n\nclient\nThe driver is launched locally (in the “local” PC executing spark-submit)\n\n\ncluster\nThe driver is launched on one node of the cluster\n\n\n\n\n\n\n\n\n\nDeployment mode: cluster and client\n\n\n\nIn cluster mode\n\nThe Spark driver runs in the ApplicationMaster on a cluster node.\nThe cluster nodes are used also to store RDDs and execute transformations and actions on the RDDs\nA single process in a YARN container is responsible for both driving the application and requesting resources from YARN.\nThe resources (memory and CPU) of the client that launches the application are not used.\n\n\n\nCluster deployment mode\n\n\n\nIn client mode\n\nThe Spark driver runs on the host where the job is submitted (i.e., the resources of the client are used to execute the Driver)\nThe cluster nodes are used to store RDDs and execute transformations and actions on the RDDs\nThe ApplicationMaster is responsible only for requesting executor containers from YARN.\n\n\n\nClient deployment mode\n\n\n\n\n\n\n\nSetting the executors\nspark-submit allows specifying the characteristics of the executors\n\n\n\noption\nmeaning\ndefault value\n\n\n\n\n--num-executors\nThe number of executors\n2 executors\n\n\n--executor-cores\nThe number of cores per executor\n1 core\n\n\n--executor-memory\nMain memory per executor\n1 GB\n\n\n\nNotice that the maximum values of these parameters are limited by the configuration of the cluster.\n\n\nSetting the drivers\nspark-submit allows specifying the characteristics of the driver\n\n\n\noption\nmeaning\ndefault value\n\n\n\n\n–driver-cores\nThe number of cores for the driver\n1 core\n\n\n–driver-memory\nMain memory for the driver\n1 GB\n\n\n\nAlso the maximum values of these parameters are limited by the configuration of the cluster when the --deploy-mode is set to cluster.\n\n\nExecution examples\nThe following command submits a Spark application on a Hadoop cluster\nspark-submit \\ \n--deploy-mode cluster \\\n--master yarn MyApplication.py arguments\nIt executes/submits the application contained in MyApplication.py, and the application is executed on a Hadoop cluster based on the YARN scheduler. Notice that the Driver is executed in a node of cluster.\nThe following command submits a Spark application on a local PC\nspark-submit \\\n--deploy-mode client \\\n--master local MyApplication.py arguments\nIt executes/submits the application contained in MyApplication.py. Notice that the application is completely executed on the local PC:\n\nBoth Driver and Executors\nHadoop is not needed in this case\nOnly the Spark software is needed"
  },
  {
    "objectID": "10_intro_spark.html",
    "href": "10_intro_spark.html",
    "title": "12  Introduction to Spark",
    "section": "",
    "text": "Apache SparkTM is a fast and general-purpose engine for large-scale data processing. Spark aims at achieving the following goals in the Big data context:\n\nGenerality: diverse workloads, operators, job sizes\nLow latency: sub-second\nFault tolerance: faults are the norm, not the exception\nSimplicity: often comes from generality\n\n\n\n\n\n\n\nHistory\n\n\n\n\n\nOriginally developed at the University of California - Berkeley’s AMPLab\n\n\nSpark history\n\n\n\n\n\n\n\nMotivations\n\nMapReduce and Spark iterative jobs and data I/O\nIterative jobs, with MapReduce, involve a lot of disk I/O for each iteration and stage, and disk I/O is very slow (even if it is local I/O)\n\n\nIterative jobs\n\n\n\n\nMotivation: using MapReduce for complex iterative jobs or multiple jobs on the same data involves lots of disk I/O\nOpportunity: the cost of main memory decreased, hence, large main memories are available in each server\nSolution: keep more data in main memory, and that’s the basic idea of Spark\n\nSo an iterative job in MapReduce makes wide use of disk reading/writing\n\n\nIterative jobs in MapReduce\n\n\n\nInstead, an iterative job in Spark uses the main memory\n\n\nIterative jobs in Spark\n\n\n\nData (or at least part of it) are shared between the iterations by using the main memory , which is 10 to 100 times faster than disk.\nMoreover, to run multiple queries on the same data, in MapReduce the data must be read multiple times (once for each query)\n\n\nAnalysing the same data in MapReduce\n\n\n\nInstead, in Spark the data have to be loaded only once in the main memory\n\n\nAnalysing the same data in Spark\n\n\n\nIn other words, data are read only once from HDFS and stored in main memory, splitting of the data across the main memory of each server.\n\n\nResilient distributed data sets (RDDs)\nIn Spark, data are represented as Resilient Distributed Datasets (RDDs), which are Partitioned/Distributed collections of objects spread across the nodes of a cluster, and are stored in main memory (when it is possible) or on local disk.\nSpark programs are written in terms of operations on resilient distributed data sets.\nRDDs are built and manipulated through a set of parallel transformations (e.g., map, filter, join) and actions (e.g., count, collect, save), and RDDs are automatically rebuilt on machine failure.\nThe Spark computing framework provides a programming abstraction (based on RDDs) and transparent mechanisms to execute code in parallel on RDDs\n\nIt hides complexities of fault-tolerance and slow machines\nIt manages scheduling and synchronization of the jobs\n\n\n\nMapReduce vs Spark\n\n\n\n\nHadoop MapReduce\nSpark\n\n\n\n\nStorage\nDisk only\nIn-memory or on disk\n\n\nOperations\nMap and Reduce\nMap, Reduce, Join, Sample, …\n\n\nExecution model\nBatch\nBatch, interactive, streaming\n\n\nProgramming environments\nJava\nScala, Java, Python, R\n\n\n\nWith respect to MapReduce, Spark has a lower overhead for starting jobs and has less expensive shuffles.\nIn-memory RDDs can make a big difference in performance\n\n\nPerfomance comparison\n\n\n\n\n\n\nMain components\n\n\nSpark main components\n\n\n\nSpark is based on a basic component (the Spark Core component) that is exploited by all the high-level data analytics components: this solution provides a more uniform and efficient solution with respect to Hadoop where many non-integrated tools are available. In this way, when the efficiency of the core component is increased also the efficiency of the other high-level components increases.\n\nSpark Core\nSpark Core contains the basic functionalities of Spark exploited by all components\n\nTask scheduling\nMemory management\nFault recovery\n…\n\nIt provides the APIs that are used to create RDDs and applies transformations and actions on them.\n\n\nSpark SQL\nSpark SQL for structured data is used to interact with structured datasets by means of the SQL language or specific querying APIs (based on Datasets).\nIt exploits a query optimizer engine, and supports also Hive Query Language (HQL). It interacts with many data sources (e.g., Hive Tables, Parquet, Json).\n\n\nSpark Streaming\nSpark Streaming for real-time data is used to process live streams of data in real-time. The APIs of the Streaming real-time components operated on RDDs and are similar to the ones used to process standard RDDs associated with “static” data sources.\n\n\nMLlib\nMLlib is a machine learning/data mining library that can be used to apply the parallel versions of some machine learning/data mining algorithms\n\nData preprocessing and dimensional reduction\nClassification algorithms\nClustering algorithms\nItemset mining\n…\n\n\n\nGraphX and GraphFrames\nGraphX is a graph processing library that provides algorithms for manipulating graphs (e.g., subgraph searching, PageRank). Notice that the Python version is not available.\nGraphFrames is a graph library based on DataFrames and Python.\n\n\nSpark schedulers\nSpark can exploit many schedulers to execute its applications\n\nHadoop YARN: it is the standard scheduler of Hadoop\nMesos cluster: another popular scheduler\nStandalone Spark Scheduler: a simple cluster scheduler included in Spark\n\n\n\n\nBasic concepts\n\nResilient Distributed Data sets (RDDs)\nRDDs are the primary abstraction in Spark: they are distributed collections of objects spread across the nodes of a clusters, which means that they are split in partitions, and each node of the cluster that is running an application contains at least one partition of the RDD(s) that is (are) defined in the application.\nRDDs are stored in the main memory of the executors running in the nodes of the cluster (when it is possible) or in the local disk of the nodes if there is not enough main memory. This allows to execute in parallel the code invoked on eah node: each executor of a worker node runs the specified code on its partition of the RDD.\n\n\n\n\n\n\nExample of an RDD split in 3 partitions\n\n\n\n\n\n\n\nExample of RDD splits\n\n\n\nMore partitions mean more parallelism.\n\n\n\nRDDs are immutable once constructed (i.e., the content of an RDD cannot be modified). Spark tracks lineage information to efficiently recompute lost data in case of failures of some executors: for each RDD, Spark knows how it has been constructed and can rebuilt it if a failure occurs. This information is represented by means of a DAG (Direct Acyclic Graph) connecting input data and RDDs.\nRDDs can be created\n\nby parallelizing existing collections of the hosting programming language (e.g., collections and lists of Scala, Java, Pyhton, or R): in this case the number of partition is specified by the user\nfrom (large) files stored in HDFS: in this case there is one partition per HDFS block\nfrom files stored in many traditional file systems or databases\nby transforming an existing RDDs: in this case the number of partitions depends on the type of transformation\n\nSpark programs are written in terms of operations on resilient distributed data sets\n\nTransformations: map, filter, join, …\nActions: count, collect, save, …\n\nTo summarize, in the Spark framework\n\nSpark manages scheduling and synchronization of the jobs\nSpark manages the split of RDDs in partitions and allocates RDDs partitions in the nodes of the cluster\nSpark hides complexities of fault-tolerance and slow machines (RDDs are automatically rebuilt in case of machine failures)\n\n\n\n\nSpark Programs\n\nSupported languages\nSpark supports many programming languages\n\nScala: this is the language used to develop the Spark framework and all its components (Spark Core, Spark SQL, Spark Streaming, MLlib, GraphX)\nJava\nPython\nR\n\n\n\nStructure of Spark programs\n\n\n\n\n\n\nSpark official terminology\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nApplication\nUser program built on Spark, consisting of a driver program and executors on the cluster.\n\n\nDriver program\nThe process running the main() function of the application and creating the SparkContext.\n\n\nCluster manager\nAn external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN).\n\n\nDeploy mode\nIt distinguishes where the driver process runs: in “cluster” mode (in this case the framework launches the driver inside of the cluster) or in “client” mode (in this case the submitter launches the driver outside of the cluster).\n\n\nWorker node\nAny node of the cluster that can run application code in the cluster.\n\n\nExecutor\nA process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them; each application has its own executors.\n\n\nTask\nA unit of work that will be sent to one executor.\n\n\nJob\nA parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect).\n\n\nStage\nEach job gets divided into smaller sets of tasks called stages, such that the output of one stage is the input of the next stage(s), except for the stages that compute (part of) the final result (i.e., the stages without output edges in the graph representing the workflow of the application). Indeed, the outputs of those stages is stored in HDFS or a database.\n\n\n\nThe shuffle operation is always executed between two stages\n\nData must be grouped/repartitioned based on a grouping criteria that is different with respect to the one used in the previous stage\nSimilar to the shuffle operation between the map and the reduce phases in MapReduce\nShuffle is a heavy operation\n\nSee the official documentation for more.\n\n\nThe Driver program contains the main method. It defines the workflow of the application, and accesses Spark through the SparkContext object, which represents a connection to the cluster.\nThe Driver program defines Resilient Distributed Datasets (RDDs) that are allocated in the nodes of the cluster, and invokes parallel operations on RDDs.\nThe Driver program defines\n\nLocal variables: these are standard variables of the Python programs\nRDDs: these are distributed variables stored in the nodes of the cluster\nThe SparkContext object, which allows to\n\ncreate RDDs\nsubmit executors (processes) that execute in parallel specific operations on RDDs\nperform Transformations and Actions\n\n\nThe worker nodes of the cluster are used to run your application by means of executors. Each executor runs on its partition of the RDD(s) the operations that are specified in the driver.\n\n\nDistributed execution of Spark\n\n\n\nRDDs are distributed across executors (each RDD is split in partitions that are spread across the available executors).\n\n\nLocal execution of Spark\nSpark programs can also be executed locally: local threads are used to parallelize the execution of the application on RDDs on a single PC. Local threads can be seen are “pseudo-worker” nodes, and a local scheduler is launched to run Spark programs locally. It is useful to develop and test the applications before deploying them on the cluster.\n\n\nDistributed execution of Spark\n\n\n\n\n\n\nSpark program examples\n\nCount line program\nThe steps of this program are\n\ncount the number of lines of the input file, whose name is set to “myfile.txt”\nprint the results on the standard output\n\nfrom pyspark import SparkConf, SparkContext\n\nif __name__ == \"__main__\":\n\n    ## Create a configuration object and\n    ## set the name of the application\n    #conf = SparkConf().setAppName(\"Spark Line Count\") # <1>\n\n    ## Create a Spark Context object\n    #sc = SparkContext(conf=conf) # <1>\n\n    ## Store the path of the input file in inputfile\n    #inputFile= \"myfile.txt\" # <1>\n\n    ## Build an RDD of Strings from the input textual file\n    ## Each element of the RDD is a line of the input file\n    #linesRDD = sc.textFile(inputFile) # <2>\n\n    ## Count the number of lines in the input file\n    ## Store the returned value in the local variable numLines\n    #numLines = linesRDD.count() # <1>\n\n    ## Print the output in the standard output\n    print(\"NumLines:\", numLines)\n\n    ## Close the Spark Context object\n    sc.stop()\n\nLocal Python variable: it is allocated in the main memory of the same process instancing the Driver.\nIt is allocated/stored in the main memory or in the local disk of the executors of the worker nodes.\n\n\nLocal variables can be used to store only “small” objects/data (i.e., the maximum size is equal to the main memory of the process associated with the Driver)\nRDDs are used to store “big/large” collections of objects/data in the nodes of the cluster\n\nIn the main memory of the worker nodes, when it is possible\nIn the local disks of the worker nodes, when it is necessary\n\n\n\n\nWord Count program\nIn the Word Count implemented by means of Spark\n\nThe name of the input file is specified by using a command line parameter (i.e., argv[1])\nThe output of the application (i.e., the pairs (word, number of occurrences) are stored in an output folder (i.e., argv[2]))\n\nNotice that there is no need to worry about the details.\nfrom pyspark import SparkConf, SparkContext\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Word count example\n    \"\"\"\n    inputFile= sys.argv[1]\n    outputPath = sys.argv[2]\n\n    ## Create a configuration object and\n    ## set the name of the application\n    conf = SparkConf().setAppName(\"Spark Word Count\")\n    \n    ## Create a Spark Context object\n    sc = SparkContext(conf=conf)\n\n    ## Build an RDD of Strings from the input textual file\n    ## Each element of the RDD is a line of the input file\n    lines = sc.textFile(inputFile)\n\n    ## Split/transform the content of lines in a\n    ## list of words and store them in the words RDD\n    words = lines.flatMap(lambda line: line.split(sep=' '))\n    \n    ## Map/transform each word in the words RDD\n    ## to a pair/tuple (word,1) and store the result \n    ## in the words_one RDD\n    words_one = words.map(lambda word: (word, 1))\n\n    ## Count the num. of occurrences of each word.\n    ## Reduce by key the pairs of the words_one RDD and store\n    ## the result (the list of pairs (word, num. of occurrences)\n    ## in the counts RDD\n    counts = words_one.reduceByKey(lambda c1, c2: c1 + c2)\n\n    ## Store the result in the output folder\n    counts.saveAsTextFile(outputPath)\n\n    ## Close/Stop the Spark Context object\n    sc.stop()"
  },
  {
    "objectID": "11_rdd_based_programming.html",
    "href": "11_rdd_based_programming.html",
    "title": "13  RDD based programming",
    "section": "",
    "text": "Spark Context\nThe “connection” of the driver to the cluster is based on the Spark Context object\n\nIn Python the name of the class is SparkContext\nThe Spark Context is built by means of the constructor of the SparkContext class\nThe only parameter is a configuration object\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Create a configuration object and\n## set the name of the application\nconf = SparkConf().setAppName(\"Application name\")\n\n## Create a Spark Context object\nsc = SparkContext(conf=conf)\n\n\n\nThe Spark Context object can be obtained also by using the SparkContext.getOrCreate(conf) method, whose only parameter is a configuration object. Notice that, if the SparkContext object already exists for this application, the current SparkContext object is returned, otherwise, a new SparkContext object is returned: there is always one single SparkContext object for each application.\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Create a configuration object and\n## set the name of the application\nconf = SparkConf().setAppName(\"Application name\")\n\n## Retrieve the current SparkContext object or\n## create a new one\nsc = SparkContext.getOrCreate(conf=conf)\n\n\n\n\n\nRDD basics\nA Spark RDD is an immutable distributed collection of objects. Each RDD is split in partitions, allowing to parallelize the code based on RDDs (i.e., code is executed on each partition in isolation).\nRDDs can contain any type of Scala, Java, and Python objects, including user-defined classes.\n\n\nRDD: create and save\nRDDs can be created\n\nBy loading an external dataset (e.g., the content of a folder, a single file, a database table)\nBy parallelizing a local collection of objects created in the Driver (e.g., a Java collection)\n\n\nCreate RDDs from files\nTo built an RDD from an input textual file, use the textFile(name) method of the SparkContext class.\n\nThe returned RDD is an RDD of Strings associated with the content of the name textual file;\nEach line of the input file is associated with an object (a string) of the instantiated RDD;\nBy default, if the input file is an HDFS file the number of partitions of the created RDD is equal to the number of HDFS blocks used to store the file, in order to support data locality.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Build an RDD of strings from the input textual file\n## myfile.txt\n## Each element of the RDD is a line of the input file\ninputFile = \"myfile.txt\"\nlines = sc.textFile(inputFile)\nNotice that no computation occurs when sc.textFile() is invoked: Spark only records how to create the RDD, and the data is lazily read from the input file only when the data is needed (i.e., when an action is applied on lines, or on one of its “descendant” RDDs).\n\n\n\nTo build an RDD from a folder containing textual files, use the textFile(name) method of the SparkContext class.\n\nIf name is the path of a folder all files inside that folder are considered;\nThe returned RDD contains one string for each line of the files contained on the name folder.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Build an RDD of strings from all the files stored in\n## myfolder\n## Each element of the RDD is a line of the input files\ninputFolder = \"myfolder/\"\nlines = sc.textFile(inputFolder)\nNotice that all files inside myfolder are considered, also those without suffix or with a suffix different from “.txt”.\n\n\n\nTo set the (minimum) number of partitions, use the textFile(name, minPartitions) method of the SparkContext class.\n\nThis option can be used to increase the parallelization of the submitted application;\nFor the HDFS files, the number of partitions minPartitions must be greater than the number of blocks/chunks.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Build an RDD of strings from the input textual file\n## myfile.txt\n## The number of partitions is manually set to 4\n## Each element of the RDD is a line of the input file\ninputFile = \"myfile.txt“\nlines = sc.textFile(inputFile, 4)\n\n\n\n\n\nCreate RDDs from a local Python collection\nAn RDD can be built from a local Python collection/list of local python objects using the parallelize(c) method of the SparkContext class\n\nThe created RDD is an RDD of objects of the same type of objects of the input python collection c\nIn the created RDD, there is one object for each element of the input collection\nSpark tries to set the number of partitions automatically based on your cluster’s characteristics\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Create a local python list\ninputList = [\n    'First element',\n    'Second element',\n    'Third element'\n]\n\n## Build an RDD of Strings from the local list.\n## The number of partitions is set automatically by Spark\n## There is one element of the RDD for each element\n## of the local list\ndistRDDList = sc.parallelize(inputList)\nNotice that no computation occurs when sc.parallelize(c) is invoked: Spark only records how to create the RDD, and the data is lazily read from the input file only when the data is needed (i.e., when an action is applied on distRDDlist, or on one of its “descendant” RDDs).\n\n\n\nWhen the parallelize(c) is invoked, Spark tries to set the number of partitions automatically based on the cluster’s characteristics, but the developer can set the number of partition by using the method parallelize(c, numSlices) of the SparkContext class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Create a local python list\ninputList = [\n    'First element',\n    'Second element',\n    'Third element'\n]\n\n## Build an RDD of Strings from the local list.\n## The number of partitions is set to 3\n## There is one element of the RDD for each element\n## of the local list\ndistRDDList = sc.parallelize(inputList, 3)\n\n\n\n\n\nSave RDDs\nAn RDD can be easily stored in textual (HDFS) files using the saveAsTextFile(path) method of the RDD class\n\npath is the path of a folder\nThe method is invoked on the RDD to store in the output folder\nEach object of the RDD on which the saveAsTextFile method is invoked is stored in one line of the output files stored in the output folder, and there is one output file for each partition of the input RDD.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Store the content of linesRDD in the output folder\n## Each element of the RDD is stored in one line\n## of the textual files of the output folder\noutputPath=\"risFolder/\"\nlinesRDD.saveAsTextFile(outputPath)\nNotice that saveAsTextFile() is an action, hence Spark computes the content associated with linesRDD when saveAsTextFile() is invoked. Spark computes the content of an RDD only when that content is needed.\nMoreover, notice that the output folder contains one textual file for each partition of linesRDD, such that each output file contains the elements of one partition.\n\n\n\n\n\nRetrieve the content of RDDs and store it local Python variables\nThe content of an RDD can be retrieved from the nodes of the cluster and stored in a local python variable of the Driver using the collect() method of the RDD class.\nThe collect() method of the RDD class is invoked on the RDD to retrieve. It returns a local python list of objects containing the same objects of the considered RDD.\n\n\n\n\n\n\nWarning\n\n\n\nPay attention to the size of the RDD: large RDDs cannot be stored in a local variable of the Driver.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n## Retrieve the content of the linesRDD and store it\n## in a local python list\n## The local python list contains a copy of each\n## element of linesRDD\ncontentOfLines=linesRDD.collect()\n\n\n\n\n\n\n\ncontentOfLines\nLocal python variable: it is allocated in the main memory of the Driver process/task\n\n\nlinesRDD\nRDD of strings: it is distributed across the nodes of the cluster\n\n\n\n\n\n\n\n\n\nTransformations and Actions\nRDD support two types of operations\n\nTransformations\nActions\n\n\nTransformations\nTransformations are operations on RDDs that return a new RDD. This type of operation apply a transformation on the elements of the input RDD(s) and the result of the transformation is stored in/associated with a new RDD.\nRemember that RDDs are immutable, hence the content of an already existing RDD cannot be changed, and it only possible to applied a transformation on the content of an RDD and then store/assign the result in/to a new RDD.\nTransformations are computed lazily, which means that transformations are computed (executed) only when an action is applied on the RDDs generated by the transformation operations. When a transformation is invoked, Spark keeps only track of the dependency between the input RDD and the new RDD returned by the transformation, and the content of the new RDD is not computed.\nThe graph of dependencies between RDDs represents the information about which RDDs are used to create a new RDD. This is called lineage graph, and it is represented as a DAG (Directed Acyclic Graph): it is needed to compute the content of an RDD the first time an action is invoked on it, or to compute again the content of an RDD (or some of its partitions) when failures occur.\nThe lineage graph is also useful for optimization purposes: when the content of an RDD is needed, Spark can consider the chain of transformations that are applied to compute the content of the needed RDD and potentially decide how to execute the chain of transformations. In this way, Spark can potentially change the order of some transformations or merge some of them based on its optimization engine.\n\n\nActions\nActions are operations that\n\nreturn results to the Driver program (i.e., return local python variables). Pay attention to the size of the returned results because they must be stored in the main memory of the Driver program.\nwrite the result in the storage (output file/folder). The size of the result can be large in this case since it is directly stored in the (distributed) file system.\n\n\nExample of lineage graph (DAG)\nConsider the following code\nfrom pyspark import SparkConf, SparkContext\nimport sys\n\nif __name__ == \"__main__\":\nconf = SparkConf().setAppName(\"Spark Application\")\nsc = SparkContext(conf=conf)\n\n## Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n## Select the rows containing the word \"error\"\nerrorsRDD = inputRDD.filter(lambda line: line.find('error')>=0)\n\n## Select the rows containing the word \"warning\"\nwarningRDD = inputRDD.filter(lambda line: line.find('warning')>=0)\n\n## Union of errorsRDD and warningRDD\n## The result is associated with a new RDD: badLinesRDD\nbadLinesRDD = errorsRDD.union(warningRDD)\n\n## Remove duplicates lines (i.e., those lines containing\n## both \"error\" and \"warning\")\nuniqueBadLinesRDD = badLinesRDD.distinct()\n\n## Count the number of bad lines by applying\n## the count() action\nnumBadLines = uniqueBadLinesRDD.count()\n\n## Print the result on the standard output of the driver\nprint(\"Lines with problems:\", numBadLines)\n\n\nVisual representation of the DAG\n\n\n\nNotice that:\n\nThe application reads the input log file only when the count() action is invoked: this is the first action of the program;\nfilter(), union(), and distinct() are transformations, so they are computed lazily;\nAlso textFile() is computed lazily, however it is not a transformation because it is not applied on an RDD.\n\nSpark, similarly to an SQL optimizer, can potentially optimize the execution of some transformations; for instance, in this case the two filters + union + distinct can be potentially optimized and transformed in one single filter applying the constraint (i.e. The element contains the string “error” or “warning”). This optimization improves the efficiency of the application, but Spark can performs this kind of optimizations only on particular types of RDDs: Datasets and DataFrames.\n\n\n\n\nPassing functions to Transformations and Actions\nMany transformations (and some actions) are based on user provided functions that specify which transformation function must be applied on the elements of the input RDD. For example, the filter() transformation selects the elements of an RDD satisfying a user specified constraint, which is a Boolean function applied on each element of the input RDD.\nEach language has its own solution to pass functions to Spark’s transformations and actions. In Python, it is possible to use\n\nLambda functions/expressions: simple functions that can be written as one single expression\nLocal user defined functions (local defs): used for multi-statement functions or statements that do not return a value\n\n\nExample based on the filter transformation\n\nCreate an RDD from a log file;\nCreate a new RDD containing only the lines of the log file containing the word “error”. The filter() transformation applies the filter constraint on each element of the input RDD; the filter constraint is specified by means of a Boolean function that returns true for the elements satisfying the constraint and false for the others.\n\n\nSolution based on lambda expressions (lambda)\n## Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n## Select the rows containing the word \"error\"\nerrorsRDD = inputRDD.filter(lambda l: l.find('error')>=0)\n\n\n\n\n\n\n\nlambda l: l.find('error')>=0\nThis part of the code, which is based on a lambda expression, defines on the fly the function to apply. This part of the code is applied on each object of inputRDD: if it returns true then the current object is “stored” in the new errorsRDD RDD, otherwise the input object is discarded.\n\n\n\n\n\nSolution based on function (def)\n## Define the content of the Boolean function that is applied\n## to select the elements of interest\ndef myFunction(l):\n    if l.find('error')>=0: return True\n    else: return False\n\n## Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n## Select the rows containing the word “error”\nerrorsRDD = inputRDD.filter(myFunction)\n\n\n\n\n\n\n\ndef myFunction(l):\nWhen it is invoked, this function analyses the value of the parameter line and returns True if the string line contains the substring “error”. Otherwise, it returns False.\n\n\n.filter(myFunction)\nFor each object o in inputRDD, the myFunction function is automatically invoked. If myFunction returns True, then o is stored in the new RDD errorsRDD. Otherwise, o is discarded.\n\n\n\n\n\nSolution based on function (def)\n## Define the content of the Boolean function that is applied\n## to select the elements of interest\ndef myFunction(l):\n    return l.find('error')>=0\n\n## Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n## Select the rows containing the word “error”\nerrorsRDD = inputRDD.filter(myFunction)\n\n\n\n\n\n\n\nreturn l.find('error')>=0\nThis part of the code is the same used in the lambda-based solution.\n\n\n.filter(myFunction)\nFor each object o in inputRDD, the myFunction function is automatically invoked. If myFunction returns True, then o is stored in the new RDD errorsRDD. Otherwise, o is discarded.\n\n\n\n\n\nSolution comparison\nThe two solutions are more or less equivalent in terms of efficiency\n\n\n\n\n\n\n\nLambda function-based code (lambda)\nLocal user defined functions (local def)\n\n\n\n\nMore concise\nLess concise\n\n\nMore readable\nLess readable\n\n\nMulti-statement functions or statements that do not return a value are not supported\nMulti-statement functions or statements that do not return a value are supported\n\n\nCode cannot be reused\nCode can be reused (some functions are used in several applications)\n\n\n\n\n\n\n\nBasic Transformations\n\nSome basic transformations analyze the content of one single RDD and return a new RDD (e.g., filter(), map(), flatMap(), distinct(), sample())\nSome other transformations analyze the content of two (input) RDDs and return a new RDD (e.g., union(), intersection(), substract(), cartesian())\n\n\nSingle input RDD transformations\n\nFilter transformation\nThe filter transformation is applied on one single RDD and returns a new RDD containing only the elements of the input RDD that satisfy a user specified condition.\nThe filter transformation is based on the filter(f) method of the RDD class: a function f returning a Boolean value is passed to the filter method, where f contains the code associated with the condition that we want to apply on each element e of the input RDD. If the condition is satisfied then the call method returns true and the input element e is selected, otherwise, it returns false and the e element is discarded.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a log file;\nCreate a new RDD containing only the lines of the log file containing the word “error”.\n\n## Read the content of a log file\ninputRDD = sc.textFile(\"log.txt\")\n\n## Select the rows containing the word “error”\nerrorsRDD = inputRDD.filter(lambda e: e.find('error')>=0)\nNotice that, in this case, the input RDD contains strings, hence, the implemented lambda function is applied on one string at a time and returns a Boolean value.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing only the values greater than 2.\n\nUsing lambda\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n## Select the values greater than 2\ngreaterRDD = inputRDD.filter(lambda num : num>2)\nNotice that the input RDD contains integers, hence, the implemented lambda function is applied on one integer at a time and returns a Boolean value.\nUsing def\n## Define the function to be applied in the filter transformation\ndef greaterThan2(num):\n    return num>2\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n## Select the values greater than 2\ngreaterRDD = inputRDD.filter(greaterThan2)\nIn this case, the function to apply is defined using def and then is passed to the filter transformation.\n\n\n\n\n\nMap transformation\nThe map transformation is used to create a new RDD by applying a function f on each element of the input RDD: the new RDD contains exactly one element y for each element x of the input RDD, in particular the value of y is obtained by applying a user defined function f on x (e.g., y= f(x)). The data type of y can be different from the data type of x.\nThe map transformation is based on the RDD map(f) method of the RDD class: a function f implementing the transformation is passed to the map method, where f contains the code that is applied over each element of the input RDD to create the elements of the returned RDD. For each input element of the input RDD exactly one single new element is returned by f.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing the surnames of a list of users (each line of the file contains one surname);\nCreate a new RDD containing the length of each surname.\n\n## Read the content of the input textual file\ninputRDD = sc.textFile(\"usernames.txt\")\n\n## Compute the lengths of the input surnames\nlenghtsRDD = inputRDD.map(lambda line: len(line))\nNotice that the input RDD is an RDD of strings, hence, also the input of the lambda function is a String. Instead, the new RDD is an RDD of Integers, since the lambda function returns a new Integer for each input element.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing the square of each input element.\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n## Compute the square of each input element\nsquaresRDD = inputRDD.map(lambda element: element*element)\n\n\n\n\n\nFlatMap transformation\nThe flatMap transformation is used to create a new RDD by applying a function f on each element of the input RDD. The new RDD contains a list of elements obtained by applying f on each element x of the input RDD; in other words, the function f applied on an element x of the input RDD returns a list of values [y] (e.g., [y]= f(x)). [y] can be the empty list.\nThe final result is the concatenation of the list of values obtained by applying f over all the elements of the input RDD (i.e., the final RDD contains the concatenation of the lists obtained by applying f over all the elements of the input RDD).\nNotice that\n\nduplicates are not removed\nthe data type of y can be different from the data type of x\n\nThe flatMap transformation is based on the flatMap(f) method of the RDD class A function f implementing the transformation is passed to the flatMap method, where f contains the code that is applied on each element of the input RDD and returns a list of elements which will be included in the new returned RDD: for each element of the input RDD a list of new elements is returned by f. The returned list can be empty.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing a generic text (each line of the input file can contain many words);\nCreate a new RDD containing the list of words, with repetitions, occurring in the input textual document. In other words, each element of the returned RDD is one of the words occurring in the input textual file, and the words occurring multiple times in the input file appear multiple times, as distinct elements, also in the returned RDD.\n\n## Read the content of the input textual file\ninputRDD = sc.textFile(\"document.txt\")\n\n## Compute/identify the list of words occurring in document.txt\nlistOfWordsRDD = inputRDD.flatMap(lambda l: l.split(' '))\nIn this case the lambda function returns a “list” of values for each input element. However, notice that the new RDD (i.e., listOfWordsRDD) contains the “concatenation” of the lists obtained by applying the lambda function over all the elements of inputRDD: the new RDD is an RDD of strings and not an RDD of lists of strings.\n\n\n\n\n\nDistinct information\nThe distinct transformation is applied on one single RDD and returns a new RDD containing the list of distinct elements (values) of the input RDD.\nThe distinct transformation is based on the distinct() method of the RDD class, and no functions are needed in this case.\nA shuffle operation is executed for computing the result of the distinct transformation, so that data from different input partitions gets be compared to remove duplicates. The shuffle operation is used to repartition the input data: all the repetitions of the same input element are associated with the same output partition (in which one single copy of the element is stored), and a hash function assigns each input element to one of the new partitions.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing the names of a list of users (each line of the input file contains one name);\nCreate a new RDD containing the list of distinct names occurring in the input file. The type of the new RDD is the same of the input RDD.\n\n## Read the content of a textual input file\ninputRDD = sc.textFile(\"names.txt\")\n\n## Select the distinct names occurring in inputRDD\ndistinctNamesRDD = inputRDD.distinct()\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing only the distinct values appearing in the input RDD.\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n## Compute the set of distinct words occurring in inputRDD\ndistinctIntRDD = inputRDD.distinct()\n\n\n\n\n\nSortBy transformation\nThe sortBy transformation is applied on one RDD and returns a new RDD containing the same content of the input RDD sorted in ascending order.\nThe sortBy transformation is based on the sortBy(keyfunc) method of the RDD class: each element of the input RDD is initially mapped to a new value by applying the specified function keyfunc, and then the input elements are sorted by considering the values returned by the invocation of keyfunc on the input values.\nThe sortBy(keyfunc, ascending) method of the RDD class allows specifying if the values in the returned RDD are sorted in ascending or descending order by using the Boolean parameter ascending\n\nascending set to True means ascending order\nascending set to False means descending order\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing the names of a list of users (each line of the input file contains one name);\nCreate a new RDD containing the list of users sorted by name (based on the alphabetic order).\n\n## Read the content of a textual input file\ninputRDD = sc.textFile(\"names.txt\")\n\n## Sort the content of the input RDD by name.\n## Store the sorted result in a new RDD\nsortedNamesRDD = inputRDD.sortBy(lambda name: name)\nNotice that each input element of the lambda expression is a string. The goal is sorting the input names (strings) in alphabetic order, which is the standard sort order for strings. For this reason the lambda function returns the input strings without modifying them.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD from a textual file containing the names of a list of users (each line of the input file contains one name);\nCreate a new RDD containing the list of users sorted by the length of their name (i.e., the sort order is based on len(name)).\n\n## Read the content of a textual input file\ninputRDD = sc.textFile(\"names.txt\")\n\n## Sort the content of the input RDD by name.\n## Store the sorted result in a new RDD\nsortedNamesLenRDD = inputRDD.sortBy(lambda name: len(name))\nIn this case, each input element is a string but we are interested in sorting the input names (strings) by length (integer), which is not the standard sort order for strings. For this reason the lambda function returns the length of each input string, and the sort operation is performed on the returned integer values (the lengths of the input names).\n\n\n\n\n\nSample transformation\nThe sample transformation is applied on one single RDD and returns a new RDD containing a random sample of the elements (values) of the input RDD.\nThe sample transformation is based on the sample(withReplacement, fraction) method of RDD class:\n\nwithReplacement specifies if the random sample is with replacement (True) or not (False);\nfraction specifies the expected size of the sample as a fraction of the input RDD’s size (values in the range \\([0, 1]\\)).\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD from a textual file containing a set of sentences (each line of the file contains one sentence);\nCreate a new RDD containing a random sample of sentences, using the “without replacement” strategy and setting fraction to \\(0.2\\) (i.e., \\(20%\\)).\n\n## Read the content of a textual input file\ninputRDD = sc.textFile(\"sentences.txt\")\n\n## Create a random sample of sentences\nrandomSentencesRDD = inputRDD.sample(False,0.2)\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1, 2, 3, 3];\nCreate a new RDD containing a random sample of the input values, using the “replacement” strategy and setting fraction to \\(0.2\\) (i.e., \\(20%\\)).\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1, 2, 3, 3]\ninputRDD = sc.parallelize(inputList)\n\n## Create a sample of the inputRDD\nrandomSentencesRDD = inputRDD.sample(True,0.2)\n\n\n\n\n\n\nSet transformations\nSpark provides also a set of transformations that operate on two input RDDs and return a new RDD. Some of them implement standard set transformations:\n\nUnion\nIntersection\nSubtract\nCartesian\n\nAll these transformations have\n\nTwo input RDDs: one is the RDD on which the method is invoked, while the other RDD is passed as parameter to the method\nOne output RDD\n\nAll the involved RDDs have the same data type when union, intersection, or subtract are used, instead mixed data types can be used with the cartesian transformation.\n\nUnion transformation\nThe union transformation is based on the union(other) method of the RDD class: other is the second RDD to use, and the method returns a new RDD containing the union (with duplicates) of the elements of the two input RDDs.\n\n\n\n\n\n\nWarning\n\n\n\nDuplicates elements are not removed. This choice is related to optimization reasons: removing duplicates means having a global view of the whole content of the two input RDDs, but, since each RDD is split in partitions that are stored in different nodes of the cluster, the contents of all partitions should be shared to remove duplicates, and that’s a computationally costly operation.\nThe shuffle operation is not needed in this case.\n\n\nIf removing duplicates is needed after performing the union transformation, apply the distinct() transformation on the output of the union() transformation, but pay attention that distinct() is a computational costly operation (it is associated with a shuffle operation). Use distinct() if and only if duplicate removal is indispensable for the application.\n\n\nIntersection transformation\nThe intersection transformation is based on the intersection(other) method of the RDD class: other is the second RDD to use, and the method returns a new RDD containing the elements (without duplicates) occurring in both input RDDs.\nDuplicates are removed: a shuffle operation is executed for computing the result of intersection, since elements from different input partitions must be compared to find common elements.\n\n\nSubtract transformation\nThe subtract transformation is based on the subtract(other) method of the RDD class: other is the second RDD to use, and the result contains the elements appearing only in the RDD on which the subtract method is invoked. Notice that in this transformation the two input RDDs play different roles.\nDuplicates are not removed, but a shuffle operation is executed for computing the result of subtract, since elements from different input partitions must be compared.\n\n\nCartesian transformation\nThe cartesian transformation is based on the cartesian(other) method of the RDD class: other is the second RDD to use, the data types of the objects of the two input RDDs can be different, and the returned RDD is an RDD of pairs (tuples) containing all the combinations composed of one element of the first input RDD and one element of the second input RDD (see later what an RDD of pairs is).\nIn this transformation a large amount of data is sent on the network: elements from different input partitions must be combined to compute the returned pairs, but the elements of the two input RDDs are stored in different partitions, which could be even in different servers.\n\n\nExamples of set transformations\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate two RDDs of integers\n\ninputRDD1 contains the values [1, 2, 2, 3, 3]\ninputRDD2 contains the values [3, 4, 5]\n\nCreate four new RDDs\n\noutputUnionRDD contains the union of inputRDD1 and inputRDD2\noutputIntersectionRDD contains the intersection of inputRDD1 and inputRDD2\noutputSubtractRDD contains the result of inputRDD1  inputRDD2\noutputCartesianRDD contains the cartesian product of inputRDD1 and inputRDD2\n\n\n## Create two RDD of integers\ninputList1 = [1,2,2,3,3]\ninputRDD1 = sc.parallelize(inputList1)\n\ninputList2 = [3,4,5]\ninputRDD2 = sc.parallelize(inputList2)\n\n## Create four new RDDs by using union, intersection, \n## subtract, and cartesian\noutputUnionRDD = inputRDD1.union(inputRDD2)\n\noutputIntersectionRDD = inputRDD1.intersection(inputRDD2)\n\noutputSubtractRDD = inputRDD1.subtract(inputRDD2)\n\noutputCartesianRDD = inputRDD1.cartesian(inputRDD2)\n\n\n\n\n\n\n\noutputCartesianRDD\nEach element of the returned RDD is a pair (tuple) of integer elements.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate two RDDs\n\ninputRDD1 contains the Integer values [1, 2, 3]\ninputRDD2 contains the String values [\"A\", \"B\"]\n\nCreate a new RDD containing the cartesian product of inputRDD1 and inputRDD2\n\n## Create an RDD of Integers and an RDD of Strings\ninputList1 = [1,2,3]\ninputRDD1 = sc.parallelize(inputList1)\n\ninputList2 = [\"A\",\"B\"]\ninputRDD2 = sc.parallelize(inputList2)\n\n## Compute the cartesian product\noutputCartesianRDD = inputRDD1.cartesian(inputRDD2)\n\n\n\n\n\n\n\noutputCartesianRDD\nEach element of the returned RDD is a pair (tuple) of integer elements.\n\n\n\n\n\n\n\n\n\nSummary\n\nSingle input RDD transformations\nAll the examples reported in the following tables are applied on an RDD of integers containing the following elements (i.e., values): [1,2,3,3].\n\n\nPurposes\n\n\n\n\n\n\n\nTransformation\nPurpose\n\n\n\n\nfilter(f)\nReturn an RDD consisting only of the elements of the input RDD that pass the condition passed to filter(). The input RDD and the new RDD have the same data type.\n\n\nmap(f)\nApply a function to each element in the RDD and return an RDD of the result. The applied function return one element for each element of the input RDD. The input RDD and the new RDD can have a different data type.\n\n\nflatMap(f)\nApply a function to each element in the RDD and return an RDD of the result. The applied function return a set of elements (from 0 to many) for each element of the input RDD. The input RDD and the new RDD can have a different data type.\n\n\ndistinct()\nRemove duplicates.\n\n\nsortBy(keyfunc)\nReturn a new RDD containing the same values of the input RDD sorted in ascending order.\n\n\nsample(withReplacement, fraction)\nSample the content of the input RDD, with or without replacement and return the selected sample. The input RDD and the new RDD have the same data type.\n\n\n\nExamples\n\n\n\n\n\n\n\n\nTransformation\nExample function\nExample result\n\n\n\n\nfilter(f)\nfilter(lambda x: x != 1)\n[2,3,3]\n\n\nmap(f)\nmap(lambda x: x+1)\nFor each input element x, the element with value x+1 is included in the new RDD\n[2,3,4,4]\n\n\nflatMap(f)\nflatMap(lambda x: list(range(x,4))\nFor each input element x, the set of elements with values from x to 3 are returned\n[1,2,3,2,3,3,3]\n\n\ndistinct()\ndistinct()\n[1,2,3]\n\n\nsortBy(keyfunc)\nsortBy(lambda v: v)\nSort the input integer values in ascending order by using the standard integer sort order\n[1,2,3,3]\n\n\nsample(withReplacement, fraction)\nsample(True, 0.2)\nNon deterministic\n\n\n\n\n\n\n\nTwo input RDD transformations\nAll the examples reported in the following tables are applied on the following two RDDs of integers\n\ninputRDD1: [1,2,2,3,3]\ninputRDD2: [3,4,5]\n\n\n\nPurposes\n\n\n\n\n\n\n\nTransformation\nPurpose\n\n\n\n\nunion(other)\nReturn a new RDD containing the union of the elements of the input RDD and the elements of the one passed as parameter to union(). Duplicate values are not removed. All the RDDs have the same data type.\n\n\nintersection(other)\nReturn a new RDD containing the intersection of the elements of the input RDD and the elements of the one passed as parameter to intersection(). All the RDDs have the same data type.\n\n\nsubtract(other)\nReturn a new RDD the elements appearing only in the input RDD and not in the one passed as parameter to subtract(). All the RDDs have the same data type.\n\n\ncartesian(other)\nReturn a new RDD containing the cartesian product of the elements of the input RDD and the elements of the one passed as parameter to cartesian(). All the RDDs have the same data type.\n\n\n\nExamples\n\n\n\n\n\n\n\n\nTransformation\nExample function\nExample result\n\n\n\n\nunion(other)\ninputRDD1.union(inputRDD2)\n[1,2,2,3,3,3,4,5]\n\n\nintersection(other)\ninputRDD1.intersection(inputRDD2)\n[3]\n\n\nsubtract(other)\ninputRDD1.subtract(inputRDD2)\n[1,2,2]\n\n\ncartesian(other)\ninputRDD1.cartesian(inputRDD2)\n[(1,3),(1,4),...,(3,5)]\n\n\n\n\n\n\n\n\n\nBasic Actions\nSpark actions can retrieve the content of an RDD or the result of a function applied on an RDD and\n\nStore it in a local Python variable of the Driver program\n\nPay attention to the size of the returned value\nPay attentions that date are sent on the network from the nodes containing the content of RDDs and the executor running the Driver\n\nStore the content of an RDD in an output folder or database\n\nThe Spark actions that return a result that is stored in local (Python) variables of the Driver 1. Are executed locally on each node containing partitions of the RDD on which the action is invoked, and so local results are generated in each node; 2. Local results are sent on the network to the Driver that computes the final result and store it in local variables of the Driver.\nThe basic actions returning (Python) objects to the Driver are\n\ncollect()\ncount()\ncountByValue()\ntake()\ntop()\ntakeSample()\nreduce()\nfold()\naggregate()\nforeach()\n\n\nCollect action\nThe collect action returns a local Python list of objects containing the same objects of the considered RDD.\n\n\n\n\n\n\nWarning\n\n\n\nPay attention to the size of the RDD: large RDD cannot be memorized in a local variable of the Driver.\n\n\nThe collect action is based on the collect() method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,2,3,3];\nRetrieve the values of the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputList = [1,2,3,3]\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve the elements of the inputRDD and store them in\n## a local python list\nretrievedValues = inputRDD.collect()\n\n\n\n\n\n\n\ninputRDD\nIt is distributed across the nodes of the cluster. It can be large and it is stored in the local disks of the nodes if it is needed.\n\n\nretrievedValues\nIt is a local python variable. It can only be stored in the main memory of the process/task associated with the Driver. Pay attention to the size of the list. Use the collect() action if and only if sure that the list is small. Otherwise, store the content of the RDD in a file by using the saveAsTextFile method.\n\n\n\n\n\n\n\n\nCount action\nCount the number of elements of an RDD.\nThe count action is based on the count() method of the RDD class: it returns the number of elements of the input RDD.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nConsider the textual files “document1.txt” and “document2.txt”;\nPrint the name of the file with more lines.\n\n## Read the content of the two input textual files\ninputRDD1 = sc.textFile(\"document1.txt\")\ninputRDD2 = sc.textFile(\"document2.txt\")\n\n## Count the number of lines of the two files = number of elements\n## of the two RDDs\nnumLinesDoc1 = inputRDD1.count()\nnumLinesDoc2 = inputRDD2.count()\n\nif numLinesDoc1> numLinesDoc2: \n    print(\"document1.txt\")\nelif numLinesDoc2> numLinesDoc1: \n    print(\"document2.txt\")\nelse: \n    print(\"Same number of lines\")\n\n\n\n\n\nCountByValue action\nThe countByValue action returns a local python dictionary containing the information about the number of times each element occurs in the RDD\n\nThe keys of the dictionary are associated with the input elements\nThe values are the frequencies of the elements\n\nThe countByValue action is based on the countByValue() method of the RDD class. The amount of used main memory in the Driver is related to the number of distinct elements/keys.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing the first names of a list of users (each line contain one name);\nCompute the number of occurrences of each name and store this information in a local variable of the Driver.\n\n## Read the content of the input textual file\nnamesRDD = sc.textFile(\"names.txt\")\n\n## Compute the number of occurrencies of each name\nnamesOccurrences = namesRDD.countByValue()\n\n\n\n\n\n\n\nnamesOccurrences = namesRDD.countByValue()\nAlso in this case, pay attention to the size of the returned dictionary (that is related to the number of distinct names in this case). Use the countByValue() action if and only if you are sure that the returned dictionary is small. Otherwise, use an appropriate chain of Spark’s transformations and write the final result in a file by using the saveAsTextFile method.\n\n\n\n\n\n\n\n\nTake action\nThe take(num) action returns a local python list of objects containing the first num elements of the considered RDD. The order of the elements in an RDD is consistent with the order of the elements in the file or collection that has been used to create the RDD.\nThe take action is based on the take(num) method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,5,3,3,2];\nRetrieve the first two values of the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of integers. Load the values 1, 5, 3, 3,2 in this RDD\ninputList = [1,5,3,3,2]\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve the first two elements of the inputRDD and store them in\n## a local python list\nretrievedValues = inputRDD.take(2)\n\n\n\n\n\nFirst action\nThe first() action returns a local python object containing the first element of the considered RDD. The order of the elements in an RDD is consistent with the order of the elements in the file or collection that has been used to create the RDD.\nThe first action is based on the first() method of the RDD class.\nNotice that the only difference between first() and take(1) is given by the fact that first() returns a single element (the returned element is the first element of the RDD), while take(1) returns a list of elements containing one single element (the only element of the returned list is the first element of the RDD).\n\n\nTop action\nThe top(num) action returns a local python list of objects containing the top num (largest) elements of the considered RDD. The ordering is the default one of class associated with the objects stored in the RDD (the descending order is used).\nThe top action is based on the top(num) method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,5,3,4,2];\nRetrieve the top-2 greatest values of the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of integers. Load the values 1, 5, 3, 4,2 in this RDD\ninputList = [1,5,3,4,2]\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve the top-2 elements of the inputRDD and store them in\n## a local python list\nretrievedValues = inputRDD.top(2)\n\n\n\nThe top(num, key) action returns a local python list of objects containing the num largest elements of the considered RDD sorted by considering a user specified sorting function.\nThe top action is based on the top(num, key) method of the RDD class\n\nnum is the number of elements to be selected;\nkey is a function that is applied on each input element before comparing them.\n\nThe comparison between elements is based on the values returned by the invocations of this function.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of strings containing the values ['Paolo','Giovanni','Luca'];\nRetrieve the 2 longest names (longest strings) of the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of strings. Load the values 'Paolo', 'Giovanni', 'Luca']\n## in the RDD\ninputList = ['Paolo','Giovanni','Luca']\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve the 2 longest names of the inputRDD and store them in\n## a local python list\nretrievedValues = inputRDD.top(2,lambda s:len(s))\n\n\n\n\n\nTakeOrdered action\nThe takeOrdered(num) action returns a local python list of objects containing the num smallest elements of the considered RDD. The ordering is the default one of class associated with the objects stored in the RDD (the ascending order is used).\nThe takeOrdered action is based on the takeOrdered(num) method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,5,3,4,2];\nRetrieve the 2 smallest values of the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of integers. Load the values 1, 5, 3, 4,2 in this RDD\ninputList = [1,5,3,4,2]\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve the 2 smallest elements of the inputRDD and store them in\n## a local python list\nretrievedValues = inputRDD.takeOrdered(2)\n\n\n\nThe takeOrdered(num, key) action returns a local python list of objects containing the num smallest elements of the considered RDD sorted by considering a user specified sorting function.\nThe takeOrdered action is based on the takeOrdered(num, key) method of the RDD class\n\nnum is the number of elements to be selected;\nkey is a function that is applied on each input element before comparing them.\n\nThe comparison between elements is based on the values returned by the invocations of this function.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of strings containing the values ['Paolo','Giovanni','Luca'];\nRetrieve the 2 shortest names (shortest strings) of the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of strings. Load the values 'Paolo', 'Giovanni', 'Luca']\n## in the RDD\ninputList = ['Paolo','Giovanni','Luca']\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve the 2 shortest names of the inputRDD and store them in\n## a local python list\nretrievedValues = inputRDD.takeOrdered(2,lambda s:len(s))\n\n\n\n\n\nTakeSample action\nThe takeSample(withReplacement, num) action returns a local python list of objects containing num random elements of the considered RDD.\nThe takeSampleaction is based on the takeSample(withReplacement, num) method of the RDD class, where withReplacement specifies if the random sample is with replacement (True) or not (False).\nThe takeSample(withReplacement, num, seed) method of the RDD class is used when the seed has to be set.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,5,3,3,2];\nRetrieve randomly, without replacement, 2 values from the created RDD and store them in a local python list that is instantiated in the Driver.\n\n## Create an RDD of integers. Load the values 1, 5, 3, 3,2 in this RDD\ninputList = [1,5,3,3,2]\ninputRDD = sc.parallelize(inputList)\n\n## Retrieve randomly two elements of the inputRDD and store them in\n## a local python list\nrandomValues= inputRDD.takeSample(True, 2)\n\n\n\n\n\nReduce\nReturn a single python object obtained by combining all the objects of the input RDD by using a user provide function. The provided function must be associative and commutative, otherwise the result depends on the content of the partitions and the order used to analyze the elements of the RDD’s partitions. The returned object and the ones of the input RDD are all instances of the same data type/class.\nThe reduce action is based on the reduce(f) method of the RDD class: a function f is passed to the reduce method\n\nGiven two arbitrary input elements, f is used to combine them in one single value\nf is recursively invoked over the elements of the input RDD until the input values are reduced to one single value\n\nSuppose \\(L\\) contains the list of elements of the input RDD. To compute the final element/value, the reduce action operates as follows\n\nApply the user specified function on a pair of elements \\(e_1\\) and \\(e_2\\) occurring in \\(L\\) and obtain a new element \\(e_\\textbf{new}\\);\nRemove the original elements \\(e_1\\) and \\(e_2\\) from \\(L\\) and then insert the element \\(e_\\textbf{new}\\) in \\(L\\);\nIf \\(L\\) contains only one value, then return it as final result of the reduce action, otherwise, return to step 1.\n\nFunction \\(f\\) must be associative and commutative, so that the computation of the reduce action can be performed in parallel without problems, otherwise the result depends on how the input RDD is partitioned (i.e., for the functions that are not associative and commutative the output depends on how the RDD is split in partitions and how the content of each partition is analyzed).\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,2,3,3];\nCompute the sum of the values occurring in the RDD and store the result in a local python integer variable in the Driver.\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputListReduce = [1,2,3,3]\ninputRDDReduce = sc.parallelize(inputListReduce)\n\n## Compute the sum of the values\nsumValues = inputRDDReduce.reduce(lambda e1, e2: e1+e2)\n\n\n\n\n\n\n\nlambda e1, e2: e1+e2\nThis lambda function combines two input integer elements at a time and returns theirs sum.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,2,3,3];\nCompute the maximum value occurring in the RDD and store the result in a local python integer variable in the Driver.\n\nSolution 1\n## Define the function for the reduce action\ndef computeMax(v1,v2):\n    if v1>v2:\n        return v1\n    else:\n        return v2\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputListReduce = [1,2,3,3]\ninputRDDReduce = sc.parallelize(inputListReduce)\n\n## Compute the maximum value\nmaxValue = inputRDDReduce.reduce(computeMax)\nSolution 2\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputListReduce = [1,2,3,3]\ninputRDDReduce = sc.parallelize(inputListReduce)\n\n## Compute the maximum value\nmaxValue = inputRDDReduce.reduce(lambda e1, e2: max(e1, e2))\n\n\n\n\n\nFold action\nReturn a single python object obtained by combining all the objects of the input RDD and a “zero” value by using a user provided function, which must be associative (otherwise the result depends on how the RDD is partitioned), but it is not required to be commutative. An initial neutral “zero” value is also specified.\nThe fold action is based on the fold(zeroValue, op) method of the RDD class. A function op is passed to the fold method; given two arbitrary input elements, op is\n\nused to combine them in one single value\nused to combine input elements with the “zero” value\nrecursively invoked over the elements of the input RDD until the input values are reduced to one single value\n\nThe “zero” value is the neutral value for the used function op (i.e., “zero” combined with any value \\(v\\) by using op is equal to \\(v\\)).\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate an RDD of strings containing the values ['This ','is ','a ','test'];\nCompute the concatenation of the values occurring in the RDD (from left to right) and store the result in a local python string variable in the Driver.\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputListFold = ['This ','is ','a ','test']\ninputRDDFold = sc.parallelize(inputListFold)\n\n## Concatenate the input strings\nfinalString = inputRDDFold.fold('', lambda s1, s2: s1+s2)\n\n\n\n\n\n\n\n\n\nFold vs Reduce\n\n\n\n\nFold is characterized by the “zero” value;\nFold can be used to parallelize functions that are associative but non-commutative (e.g., concatenation of a list of strings).\n\n\n\n\n\nAggregate action\nReturn a single python object obtained by combining the objects of the RDD and an initial “zero” value by using two user provide functions. The provided functions must be associative, otherwise the result depends on how the RDD is partitioned. The returned objects and the ones of the input RDD can be instances of different classes (this is the main difference with respect to reduce() and fold()).\nThe aggregate action is based on the aggregate(zeroValue,seqOp,combOp) method of the RDD class. The input RDD contains objects of type T while the returned object is of type U, with U different from T.\n\nOne function is needed for merging an element of type T with an element of type U to return a new element of type U: it is used to merge the elements of the input RDD and the accumulator of each partition;\nOne function is needed for merging two elements of type U to return a new element of type U: it is used to merge two elements of type U obtained as partial results generated by two different partitions.\n\nThe seqOp function contains the code that is applied to combine the accumulator value (one accumulator for each partition) with the elements of each partition: one local result per partition is computed by recursively applying seqOp.\nThe combOp function contains the code that is applied to combine two elements of type U returned as partial results by two different partitions: the global final result is computed by recursively applying combOp.\n\nHow it works\nSuppose that \\(L\\) contains the list of elements of the input RDD and this RDD is split in a set of partitions (i.e., a set of lists \\({L_1,..., L_n}\\)). The aggregate action computes a partial result in each partition and then combines/merges the results. It operates as follows:\n\nAggregate the partial results in each partition, obtaining a set of partial results (of type U) \\(P={p_1, .., p_n}\\);\nApply the combOp function on a pair of elements \\(p_1\\) and \\(p_2\\) in \\(P\\) and obtain a new element \\(p_\\textbf{new}\\);\nRemove the original elements \\(p1\\) and \\(p2\\) from \\(P\\) and then insert the element \\(p_\\textbf{new}\\) in \\(P\\);\nIf \\(P\\) contains only one value then return it as final result of the aggregate action. Otherwise, return to step 2.\n\nSuppose that \\(L_i\\) is the list of elements on the \\(i\\)-th partition of the input RDD, and zeroValue is the initial zero value. To compute the partial result over the elements in \\(L_i\\) the aggregate action operates as follows\n\nSet accumulator to zeroValue (accumulator=zeroValue);\nApply the seqOp function on accumulator and an elements \\(ej\\) in \\(L_i\\) and update accumulator with the value returned by seqOp;\nRemove the original elements \\(e_j\\) from \\(L_i\\);\nIf \\(L_i\\) is empty return accumulator as (final) partial result \\(p_i\\) of the \\(i\\)-th partition. Otherwise, return to step 2.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nCreate an RDD of integers containing the values [1,2,3,3];\nCompute both\n\nthe sum of the values occurring in the input RDD\nand the number of elements of the input RDD\n\nStore in a local python variable of the Driver the average computed over the values of the input RDD.\n\n## Create an RDD of integers. Load the values 1, 2, 3, 3 in this RDD\ninputListAggr = [1,2,3,3]\ninRDD = sc.parallelize(inputListAggr)\n\n## Instantiate the zero value\n## We use a tuple containing two values:\n## (sum, number of represented elements)\nzeroValue = (0,0)\n\n## Compute the sum of the elements in inputRDDAggr and count them\nsumCount = inRDD.aggregate(\n    zeroValue,\n    lambda acc, e: (acc[0]+e, acc[1]+1),\n    lambda p1, p2: (p1[0]+p2[0], p1[1]+p2[1])\n)\n\n\n\n\n\n\n\nzeroValue(0,0)\nInstantiate the “zero” value\n\n\nlambda acc, e: (acc[0]+e, acc[1]+1)\nGiven a partition \\(p\\) of the input RDD, this is the function that is used to combine the elements of partition \\(p\\) with the accumulator of partition \\(p\\)\n\n\nacc\nIt is a tupe object, initialized to the “zero” value\n\n\ne\nIt is an integer\n\n\nlambda p1, p2: (p1[0]+p2[0], p1[1]+p2[1])\nThis is the function that is used to combine the partial results emitted by the RDD partitions\n\n\np1 and p2\nThese are tuple objects\n\n\n\n## Compute the average value\nmyAvg = sumCount[0]/sumCount[1]\n\n## Print the average on the standard output of the driver\nprint('Average:', myAvg)\n\n\n\n\n\nAggregate action: visual simulation\n\ninRDD = [1,2,3,3];\nSuppose inRDD is split in the following two partitions, [1,2] and [3,3].\n\n\n\nVisual simulation of the aggregate action\n\n\n\n\n\n\nSummary\nAll the examples reported in the following tables are applied on inputRDD that is an RDD of integers containing the following elements (i.e., values): [1,2,3,3].\n\n\nPurposes\n\n\n\n\n\n\n\nAction\nPurpose\n\n\n\n\ncollect()\nReturn a python list containing all the elements of the RDD on which it is applied. The objects of the RDD and objects of the returned list are objects of the same class.\n\n\ncount()\nReturn the number of elements of the RDD.\n\n\ncountByValue()\nReturn a Map object containing the information about the number of times each element occurs in the RDD.\n\n\ntake(num)\nReturn a Python list containing the first num elements of the RDD. The objects of the RDD and objects of the returned list are objects of the same class.\n\n\nfirst()\nReturn the first element of the RDD.\n\n\ntop(num)\nReturn a Python list containing the top num elements of the RDD based on the default sort order/comparator of the objects. The objects of the RDD and objects of the returned list are objects of the same class.\n\n\ntakeSample(withReplacement, num)\ntakeSample(withReplacement, num, seed)\nReturn a (Python) List containing a random sample of size n of the RDD. The objects of the RDD and objects of the returned list are objects of the same class.\n\n\nreduce(f)\nReturn a single Python object obtained by combining the values of the objects of the RDD by using a user provide function. The provided function must be associative and commutative The object returned by the method and the objects of the RDD belong to the same class.\n\n\nfold(zeroValue, op)\nSame as reduce but with the provided zero value.\n\n\nAggregate(zeroValue, seqOp, combOp)\nSimilar to reduce() but used to return a different type.\n\n\n\nExamples\n\n\n\n\n\n\n\n\nAction\nExample\nResult\n\n\n\n\ncollect()\ninputRDD.collect()\n[1,2,3,3]\n\n\ncount()\ninputRDD.count()\n4\n\n\ncountByValue()\ninputRDD.countByValue()\n[(1,1), (2,1), (3,2)]\n\n\ntake(num)\ninputRDD.take(2)\n[1,2]\n\n\nfirst()\nfirst()\n1\n\n\ntop(num)\ninputRDD.top(2)\n[3,3]\n\n\ntakeSample(withReplacement, num)\ntakeSample(withReplacement, num, seed)\ninputRDD.takeSample(False,1)\nNondeterministic\n\n\nreduce(f)\ninputRDD.reduce(lambda e1,e2: e1+e2)\nThe passed function is the sum\n9\n\n\nfold(zeroValue, op)\ninputRDD.fold(0, lambda v1, v2: v1+v2)\nThe passed function is the sum and the passed zeroValue is 0.\n9\n\n\nAggregate(zeroValue, seqOp, combOp)\ninputRDD.aggregate( zeroValue, lambda acc, e: (acc[0]+e, acc[1]+1), lambda p1, p2: (p1[0]+p2[0], p1[1]+p2[1]) )\nCompute a pair of integers where the first one is the sum of the values of the RDD and the second the number of elements\n(9,4)"
  },
  {
    "objectID": "12_rdd_keyvalue_pairs.html",
    "href": "12_rdd_keyvalue_pairs.html",
    "title": "14  RDDs and key-value pairs",
    "section": "",
    "text": "Spark supports also RDDs of key-value pairs. Key-value pairs in python are represented by means of python tuples:\n\nThe first value is the key part of the pair\nThe second value is the value part of the pair\n\nRDDs of key-value pairs are sometimes called “pair RDDs”.\nRDDs of key-value pairs are characterized by\n\nspecific operations (e.g., reduceByKey(), join()), which analyze the content of one group (key) at a time;\noperations available for the standard RDDs (e.g., filter(), map(), reduce()).\n\nMany applications are based on RDDs of key-value pairs: the operations available for RDDs of key-value pairs allow grouping data by key and performing computation by key (i.e., by group). The basic idea is similar to the one of the MapReduce-based programs in Hadoop, but there are more operations already available.\n\nCreating RDDs of key-value pairs\nRDDs of key-value pairs can be built\n\nFrom other RDDs by applying the map() or the flatMap() transformation on other RDDs;\nFrom a Python in-memory collection of tuple (key-value pairs) by using the parallelize() method of the SparkContext class.\n\nKey-value pairs are represented as standard built-in Python tuples composed of two elements\n\nKey\nValue\n\n\n\nRDDs of key-value pairs by using the Map transformation\nThe goal is to define an RDD of key-value pairs by using the map transformation: apply a function f on each element of the input RDD that returns one tuple for each input element. The new RDD of key-value pairs contains one tuple y for each element x of the input RDD.\nThe standard map(f) transformation is used, and the new RDD of key-value pairs contains one tuple y for each input element x of the input RDD (\\(y=f(x)\\)).\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing the first names of a list of users; each line of the file contains one first name;\nCreate an RDD of key-value pairs containing a list of pairs (first name, 1).\n\n## Read the content of the input textual file\nnamesRDD = sc.textFile(\"first_names.txt\")\n\n## Create an RDD of key-value pairs\nnameOnePairRDD = namesRDD.map(lambda name: (name, 1))\n\n\n\n\n\n\n\nnameOnePairRDD\nIt contains key-value pairs (i.e., tuples) of type (string, integer)\n\n\n\n\n\n\n\n\nRDDs of key-value pairs by using the flatMap transformation\nDefine an RDD of key-value pairs by using the flatMap transformation: apply a function f on each element of the input RDD that returns a list of tuples for each input element. The new PairRDD contains all the pairs obtained by applying f on each element x of the input RDD.\nThe standard flatMap(f) transformation is used, and the new RDD of key-value pairs contains the tuples returned by the execution of f on each element x of the input RDD.\n\\[\n[y]= f(x)\n\\]\n\nGiven a element \\(x\\) of the input RDD, \\(f\\) applied on \\(x\\) returns a list of pairs \\([y]\\);\nThe new RDD is a list of pairs contains all the pairs of the returned list of pairs. It is not an RDD of lists.\n\n\\([y]\\) can be the empty list.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file; each line of the file contains a set of words;\nCreate a PairRDD containing a list of pairs (word, 1): one pair for each word occurring in the input document (with repetitions).\n\nVersion 1\n## Define the function associated with the flatMap transformation\ndef wordsOnes(line):\n    pairs = []\n    for word in line.split(' '):\n        pairs.append( (word, 1))\n    return pairs\n\n## Read the content of the input textual file\nlinesRDD = sc.textFile(\"document.txt\")\n\n## Create an RDD of key-value pairs based on the input document\n## One pair (word,1) for each input word\nwordOnePairRDD = linesRDD.flatMap(wordsOnes)\nVersion 2\n## Read the content of the input textual file\nlinesRDD = sc.textFile(\"document.txt\")\n\n## Create an RDD of key-value pairs based on the input document\n## One pair (word,1) for each input word\nwordOnePairRDD = linesRDD.flatMap(\n    lambda line: map(lambda w: (w, 1), line.split(' '))\n)\n\n\n\n\n\n\n\nmap(lambda w: (w, 1), line.split(' '))\nThis is the map of python. It is not the Spark’s map transformation.\n\n\n\n\n\n\n\n\nRDDs of key-value pairs by using parallelize\nUse the parallelize method to create an RDD of key-value pairs from a local python in-memory collection of tuples.\nIt is based on the standard parallelize(c) method of the SparkContext class: each element (tuple) of the local python collection becomes a key-vaue pair of the returned RDD.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCreate an RDD from a local python list containing the following key-value pairs\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n\n\n\n\n\n\nnameAge\nThis is a local in-memory python list of key-value pairs (tuples), that is stored in the main memory of the Driver.\n\n\nnameAgePairRDD\nThis is an RDD or key-value pairs based on the content of the local in-memory python list. The RDD is stored in the “distributed” main memory of the cluster servers\n\n\n\n\n\n\n\n\nTransformations on RDDs of key-value pairs\nAll the standard transformations can be applied, where the specified functions operate on tuples, but also specific transformations are available (e.g., reduceByKey(), groupyKey(), mapValues(), join()).\n\nReduceByKey transformation\nThe goal is to create a new RDD of key-value pairs where there is one pair for each distinct key k of the input RDD of key-value pairs:\n\nThe value associated with key k in the new RDD of key-value pairs is computed by applying a function f on the values associated with k in the input RDD of key-value pairs; the function f must be associative and commutative, otherwise the result depends on how data are partitioned and analyzed;\nThe data type of the new RDD of key-value pairs is the same of the input RDD of key-value pairs.\n\nThe reduceByKey transformation is based on the reduceByKey(f) method of the RDD class. A function f is passed to the reduceByKey method\n\nGiven the values of two input pairs, f is used to combine them in one single value;\nf is recursively invoked over the values of the pairs associated with one key at a time until the input values associated with one key are reduced to one single value.\n\nThe retuned RDD contains a number of key-value pairs equal to the number of distinct keys in the input key-value pair RDD.\nSimilarly to the reduce() action, the reduceByKey() transformation aggregate values, however reduceByKey() is executed on RDDs of key-value pairs and returns a set of key-value pairs, while reduce() is executed on an RDD and returns one single value (stored in a local python variable). Moreover, reduceByKey() is a transformation, and so it is executed lazily and its result is stored in another RDD, whereas reduce() is an action.\nA shuffle operation is executed for computing the result of the reduceByKey() transformation. The result/value for each group/key is computed from data stored in different input partitions.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the pairs, where the key is the first name of a user and the value is his/her age\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\nCreate a new RDD of key-value pairs containing one pair for each name. In the returned RDD, associate each name with the age of the youngest user with that name.\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Select for each name the lowest age value\nyoungestPairRDD= nameAgePairRDD.reduceByKey(lambda age1, age2: min(age1, age2))\n\n\n\n\n\n\n\nyoungestPairRDD\nThe returned RDD of key-value pairs contains one pair for each distinct input key (i.e., for each distinct name in this example)\n\n\n\n\n\n\n\n\nFoldByKey transformation\nThe foldByKey() has the same goal of the reduceBykey() transformation, however\n\nIt is characterized also by a “zero” value\nFunctions must be associative but are not required to be commutative\n\nThe foldByKey transformation is based on the foldByKey(zeroValue, op) method of the RDD class. A function op is passed to the fold method:\n\nGiven values of two input pairs, op is used to combine them in one single value\nop is also used to combine input values with the “zero” value\nop is recursively invoked over the values of the pairs associated with one key at a time until the input values are reduced to one single value\n\nThe “zero” value is the neutral value for the used function op (i.e., “zero” combined with any value \\(v\\) by using op is equal to \\(v\\)).\nA shuffle operation is executed for computing the result of the foldByKey() transformation. The result/value for each group/key is computed from data stored in different input partitions.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the pairs, where the key is the first name of a user and the value is a message published by him/her\n\n(\"Paolo\", \"Message1\")\n(\"Giorgio\", \"Message2\")\n(\"Paolo\", \"Message3\")\n\nCreate a new RDD of key-value pairs containing one pair for each name. In the returned RDD, associate each name the concatenation of its messages (preserving the order of the messages in the input RDD).\n\n## Create the local python list\nnameMess = [\n    (\"Paolo\",\"Message1\"),\n    (\"Giorgio\",\"Message2\"),\n    (\"Paolo\",\"Message3\")\n]\n\n## Create the RDD of pairs from the local collection\nnameMessPairRDD = sc.parallelize(nameMess)\n\n## Concatenate the messages of each user\nconcatPairRDD= nameMessPairRDD.foldByKey('', lambda m1, m2: m1+m2)\n\n\n\n\n\nCombineByKey transformation\nThe goal is to create a new RDD of key-value pairs where there is one pair for each distinct key \\(k\\) of the input RDD of key-value pairs. The value associated with the key \\(k\\) in the new RDD of key-value pairs is computed by applying user-provided functions on the values associated with \\(k\\) in the input RDD of key-value pairs: the user-provided function must be associative, otherwise the result depends how data are partitioned and analyzed.\nThe data type of the new RDD of key-value pairs can be different with respect to the data type of the input RDD of key-value pairs.\nThe combineByKey transformation is based on the combineByKey(createCombiner, mergeValue, mergeCombiner) method of the RDD class\n\nThe values of the input RDD of pairs are of type \\(V\\)\nThe values of the returned RDD of pairs are of type \\(U\\)\nThe type of the keys is \\(K\\) for both RDDs of pairs\n\nThe createCombiner function contains the code that is used to transform a single value (type \\(V\\)) of the input RDD of key-value pairs into a value of the data type (type \\(U\\)) of the output RDD of key-value pairs. It is used to transform the first value of each key in each partition to a value of type \\(U\\).\nThe mergeValue function contains the code that is used to combine one value of type \\(U\\) with one value of type \\(V\\): it is used in each partition to combine the initial values (type \\(V\\)) of each key with the intermediate ones (type \\(U\\)) of each key.\nThe mergeCombiner function contains the code that is used to combine two values of type \\(U\\): it is used to combine intermediate values of each key returned by the analysis of different partitions.\nThe combineByKey function is more general than reduceByKey and foldByKey because the data types of the values of the input and the returned RDD of pairs can be different; for this reason, more functions must be implemented in this case.\nA shuffle operation is executed for computing the result of the combineByKey() transformation: the result/value for each group/key is computed from data stored in different input partitions.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the the following pairs: the key is the first name of a user and the value is his/her age\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\nStore the results in an output HDFS folder. The output contains one line for each name followed by the average age of the users with that name\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Compute the sum of ages and\n## the number of input pairs for each name (key)\nsumNumPerNamePairRDD = nameAgePairRDD.combineByKey(\n    lambda inputElem: (inputElem, 1),\n    lambda intermediateElem, inputElem: (\n        intermediateElem[0]+inputElem, \n        intermediateElem[1]+1\n    ),\n    lambda intermediateElem1, intermediateElem2: (\n        intermediateElem1[0]+intermediateElem2[0], \n        intermediateElem1[1]+intermediateElem2[1]\n    )\n)\n\n## Compute the average for each name\navgPerNamePairRDD = sumNumPerNamePairRDD.map(\n    lambda pair: (pair[0], pair[1][0]/pair[1][1])\n)\n\n## Store the result in an output folder\navgPerNamePairRDD.saveAsTextFile(outputPath)\n\n\n\n\n\n\n\nlambda inputElem:\nGiven an input value (an age), it returns a tuple containing (age,1)\n\n\nlambda intermediateElem, inputElem:\nGiven an input value (an age) and an intermediate value (sum ages, num represented values), it combines them and returns a new updated tuple (sum ages, num represented values)\n\n\nlambda intermediateElem1, intermediateElem2:\nGiven two intermediate result tuples (sum ages, num represented values), it combines them and returns a new updated tuple (sum ages, num represented values)\n\n\nlambda pair:\nCompute the average age for each key (i.e., for each name) by combining sum ages and num represented values. Each input pair is characterized by a value that is a tuple containing (sum ages, num represented values).\n\n\n\n\n\n\n\n\nGroupByKey transformation\nThe goal is to create a new RDD of key-value pairs where there is one pair for each distinct key \\(k\\) of the input RDD of key-value pairs: the value associated with key \\(k\\) in the new RDD of key-value pairs is the list of values associated with \\(k\\) in the input RDD of key-value pairs.\nThe groupByKey transformation is based on the groupByKey() method of the RDD class.\nIf grouping values per key to perform then an aggregation such as sum or average over the values of each key then groupByKey is not the right choice: reduceByKey, aggregateByKey, or combineByKey provide better performances for associative and commutative aggregations; groupByKey is useful if an aggregation or compute a function that is not associative must be applied.\nA shuffle operation is executed for computing the result of the groupByKey() transformation: each group/key is associated with/is composed of values which are stored in different partitions of the input RDD.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the following pairs: the key is the first name of a user and the value is his/her age.\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\nStore the results in an output HDFS folder. The output contains one line for each name followed by the ages of all the users with that name.\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Create one group for each name with the list of associated ages\nagesPerNamePairRDD = nameAgePairRDD.groupByKey()\n\n## Store the result in an output folder\nagesPerNamePairRDD\\\n    .mapValues(lambda listValues: list(listValues))\\\n    .saveAsTextFile(outputPath)\n\n\n\n\n\n\n\nagesPerNamePairRDD\nIn this RDD of key-value pairs each tuple is composed of a string (key of the pair) and a collection of integers (the value of the pair - a ResultIterable object)\n\n\n.mapValues(lambda listValues: list(listValues))\nThis part is used to format the content of the value part of each pair before storing the result in the output folder: this transforms a ResultIterable object to a Python list. Without this map the output will contain the pointers to ResultIterable objects instead of a readable list of integer values\n\n\n\n\n\n\n\n\nMapValues transformation\nThe goal is to apply a function \\(f\\) over the value of each pair of an input RDD or key-value pairs and return a new RDD of key-value pairs: one pair is created in the returned RDD for each input pair\n\nThe key of the created pair is equal to the key of the input pair\nThe value of the created pair is obtained by applying the function \\(f\\) on the value of the input pair\n\nThe data type of the values of the new RDD of key-value pairs can be different from the data type of the values of the input RDD of key-value pairs. The data type of the key is the same.\nThe mapValues transformation is based on the mapValues(f) method of the RDD class: a function f is passed to the mapValues method, where f contains the code that is applied to transform each input value into the a new value that is stored in the RDD of key-value pairs. The retuned RDD of pairs contains a number of key-value pairs equal to the number of key-value pairs of the input RDD of pairs (the key part is not changed).\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the following pairs: the key is the first name of a user and the value is his/her age.\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\nIncrease the age of each user (+1 year) and store the result in the HDFS file system, one output line per user.\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Increment age of all users\nplusOnePairRDD = nameAgePairRDD.mapValues(lambda age: age+1)\n\n## Save the result on disk\nplusOnePairRDD.saveAsTextFile(outputPath)\n\n\n\n\n\nFlatMapValues transformation\nThe goal is to apply a function \\(f\\) over the value of each pair of an input RDD or key-value pairs and return a new RDD of key-value pairs (\\(f\\) returns a list of values for each input value). A list of pairs is inserted in the returned RDD for each input pair\n\nThe key of the created pairs is equal to the key of the input pair\nThe values of the created pairs are obtained by applying the function \\(f\\) on the value of the input pair\n\nThe data type of the values of the new RDD of key-value pairs can be different from the data type of the values of the input RDD of key-value pairs. The data type of the key is the same.\nThe flatMapValues transformation is based on flatMapValues(f) method of the RDD class: a function f is passed to the mapValues method, where f contains the code that is applied to transform each input value into a set of new values that are stored in the new RDD of key-value pairs. The keys of the input pairs are not changed.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the pairs\n\n(\"Sentence#1\", \"Sentence test\")\n(\"Sentence#2\", \"Sentence test number 2\")\n(\"Sentence#3\", \"Sentence test number 3\")\n\nSelect the words of each sentence and store in the HDFS file system one pair (senteceId, word) per line.\n\n## Create the local python list\nsentences = [\n    (\"Sentence#1\", \"Sentence test\"),\n    (\"Sentence#2\", \"Sentence test number 2\"),\n    (\"Sentence#3\", \"Sentence test number 3\")\n]\n\n## Create the RDD of pairs from the local collection\nsentPairRDD = sc.parallelize(sentences)\n\n## “Extract” words from each sentence\nsentIdWord = sentPairRDD.flatMapValues(lambda s: s.split(' '))\n\n## Save the result on disk\nsentIdWord.saveAsTextFile(outputPath)\n\n\n\n\n\nKeys transformation\nThe goal is to return the list of keys of the input RDD of pairs and store them in a new RDD. The returned RDD is not an RDD of key-value pairs, instead it is a standard RDD of single elements, with duplicate keys not removed.\nThe keys transformation is based on the keys() method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the following pairs: the key is the first name of a user and the value is his/her age\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\nStore the names of the input users in an output HDFS folder. The output contains one name per line (duplicate names are removed).\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Select the key part of the input RDD of key-value pairs\nnamesRDD = nameAgePairRDD.keys().distinct()\n\n## Store the result in an output folder\nnamesRDD.saveAsTextFile(outputPath)\n\n\n\n\n\nValues transformation\nThe goal is to return the list of values of the input RDD of pairs and store them in a new RDD. The returned RDD is not an RDD of key-value pairs, instead it is a standard RDD of single elements, with duplicate values are not removed.\nThe values transformation is based on the values() method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the pairs: the key is the first name of a user and the value is his/her age\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 22)\n\nStore the ages of the input users in an output HDFS folder, containing one age per line and duplicate ages/values are not removed.\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",22)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Select the value part of the input RDD of key-value pairs\nagesRDD = nameAgePairRDD.values()\n\n## Store the result in an output folder\nagesRDD.saveAsTextFile(outputPath)\n\n\n\n\n\nSortByKey transformation\nThe goal is to return a new RDD of key-value pairs obtained by sorting, in ascending order, the pairs of the input RDD by key (notice that the final order is related to the default sorting function of the data type of the input keys). The content of the new RDD of key-value pairs is the same of the input RDD but the pairs are sorted by key in the new returned RDD.\nThe sortByKey transformation is based on the sortByKey() method of the RDD class (pairs are sorted by key in ascending order). The sortByKey(ascending) method of the RDD class is also available: this method allows to specify if the sort order is ascending or descending by means of a Boolean parameter (True for ascending, False for descending).\nA shuffle operation is executed for computing the result of the sortByKey() transformation, since pairs from different partitions of the input RDD must be compared to sort the input pairs by key.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a local python list containing the pairs: the key is the first name of a user and the value is his/her age\n\n(\"Paolo\", 40)\n(\"Giorgio\", 22)\n(\"Paolo\", 35)\n\nSort the users by name and store the result in the HDFS file system.\n\n## Create the local python list\nnameAge = [\n    (\"Paolo\",40),\n    (\"Giorgio\",22),\n    (\"Paolo\",35)\n]\n\n## Create the RDD of pairs from the local collection\nnameAgePairRDD = sc.parallelize(nameAge)\n\n## Sort by name the content of the input RDD of key-value pairs\nsortedNameAgePairRDD = nameAgePairRDD.sortByKey()\n\n## Store the result in an output folder\nsortedNameAgePairRDD.saveAsTextFile(outputPath)\n\n\n\n\n\nSummary\nAll the examples reported in the following tables are applied on an RDD of pairs containing the following tuples (pairs): [(\"k1\", 2), (\"k3\", 4), (\"k3\", 6)].\n\nThe key of each tuple is a string\nThe value of each tuple is an integer\n\n\n\nPurposes\n\n\n\n\n\n\n\nTransformation\nPurpose\n\n\n\n\nreduceByKey(f)\nReturn an RDD of pairs containing one pair for each key of the input RDD of pairs. The value of each pair of the new RDD of pairs is obtained by combining the values of the input RDD associated with the same key. The input RDD of pairs and the new RDD of pairs have the same data type.\n\n\nfoldByKey(zeroValue, op)\nSimilar to the reduceByKey() transformation, however foldByKey() is characterized also by a zero value.\n\n\ncombineByKey( createCombiner, mergeValue, mergeCombiner )\nReturn an RDD of key-value pairs containing one pair for each key of the input RDD of pairs. The value of each pair of the new RDD is obtained by combining the values of the input RDD associated with the same key. The values of the input RDD of pairs and the values of the new (returned) RDD of pairs can be characterized by different data types.\n\n\ngroupByKey()\nReturn an RDD of pairs containing one pair for each key of the input RDD of pairs. The value of each pair of the new RDD of pairs is a collection containing all the values of the input RDD associated with one of the input keys.\n\n\nmapValues(f)\nApply a function over each pair of an RDD of pairs and return a new RDD of pairs. The applied function returns one pair for each pair of the input RDD of pairs. The function is applied only on the value part without changing the key. The values of the input RDD and the values of new RDD can have different data types.\n\n\nflatMapValues(f)\nApply a function over each pair of an RDD of pairs and return a new RDD of pairs. The applied function returns a set of pairs (from 0 to many) for each pair of the input RDD of pairs. The function is applied only on the value part without changing the key. The values of the input RDD and the values of new RDD can have different data types.\n\n\nkeys()\nReturn an RDD containing the keys of the input pairRDD.\n\n\nvalues()\nReturn an RDD containing the values of the input pairRDD.\n\n\nsortByKey()\nReturn a PairRDD sorted by key. The input PairRDD and the new PairRDD have the same data type.\n\n\n\nExamples\n\n\n\n\n\n\n\n\nTransformation\nExample\nResult\n\n\n\n\nreduceByKey(f)\nreduceByKey(lambda v1, v2: v1+v2)\nSum values per key\n[(\"k1\",2),(\"k3\",10)]\n\n\nfoldByKey(zeroValue, op)\nfoldByKey(0,lambda v1, v2: v1+v2)\nSum values per key. The zero value is 0\n[(\"k1\",2),(\"k3\",10)]\n\n\ncombineByKey( createCombiner, mergeValue, mergeCombiner )\ncombineByKey( lambda e: (e, 1), lambda c, e: (c[0]+e, c[1]+1), lambda c1, c2: (c1[0]+c2[0], c1[1]+c2[1]) )\nSum values by key and count the number of pairs by key in one single step\n[(\"k1\",(2,1)),(\"k3\",(10,2))]\n\n\ngroupByKey()\ngroupByKey()\n[(\"k1\",[2]),(\"k3\",[4,6])]\n\n\nmapValues(f)\nmapValues(lambda v: v+1)\nIncrement the value part by 1\n[(\"k1\",3),(\"k3\",5),(\"k3\",7)]\n\n\nflatMapValues(f)\nflatMapValues(lambda v: list(range(v,6)))\n[ (\"k1\",2), (\"k1\",3), (\"k1\",4), (\"k1\",5), (\"k3\",4), (\"k3\",5) ]\n\n\nkeys()\nkeys()\n[\"k1\",\"k3“,\"k3\"]\n\n\nvalues()\nvalues()\n[2, 4, 6]\n\n\nsortByKey()\nsortByKey()\n[(\"k1\",2),(\"k3\",4),(\"k3\",6)]\n\n\n\n\n\n\n\n\nRDD-based programming\n\nTransformations on two RDDs of key-value pairs\nSpark supports also some transformations that are applied on two RDDs of key-value pairs at the same time (e.g., subtractByKey, join, coGroup).\n\n\nSubtractByKey transformation\nThe goal is to create a new RDD of key-value pairs containing only the pairs of the first input RDD of pairs associated with a key that is not appearing as key in the pairs of the second input RDD or pairs. The two input RDD of pairs must have the same type of keys, but the data type of the values can be different. The data type of the new RDD of pairs is the same of the first input RDD of pairs.\nThe subtractByKey transformation is based on the subtractByKey(other) method of the RDD class. The two input RDDs of pairs analyzed by subtractByKey are the one which the method is invoked on and the one passed as parameter (i.e., other).\nA shuffle operation is executed for computing the result of the subtractByKey() transformation, since keys from different partitions of the two input RDDs must be compared.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate two RDDs of key-value pairs from two local python lists\n\nFirst list – Profiles of the users of a blog (username,age): [(\"PaoloG\",40),(\"Giorgio\",22),(\"PaoloB\",35)]\nSecond list – Banned users (username,motivation): [(\"PaoloB\",\"spam\"),(\"Giorgio\",\"Vandalism\")]\n\nCreate a new RDD of pairs containing only the profiles of the non-banned users.\n\n## Create the first local python list\nprofiles = [(\"PaoloG\",40),(\"Giorgio\",22),(\"PaoloB\",35)]\n\n## Create the RDD of pairs from the profiles local list\nprofilesPairRDD = sc.parallelize (profiles)\n\n## Create the second local python list\nbanned = [(\"PaoloB\",\"spam\"),(\"Giorgio\",\"Vandalism\")]\n\n## Create the RDD of pairs from the banned local list\nbannedPairRDD = sc.parallelize (banned)\n\n## Select the profiles of the “good” users\nselectedProfiles = profilesPairRDD.subtractByKey(bannedPairRDD)\n\n\n\n\n\nJoin transformation\nThe goal is to Join the key-value pairs of two RDDs of key-value pairs based on the value of the key of the pairs: each pair of the input RDD of pairs is combined with all the pairs of the other RDD of pairs with the same key. The new RDD of key-value pairs has the same key data type of the input RDDs of pairs, and has a tuple as value (the pair of values of the two joined input pairs). The two input RDDs of key-value pairs must have the same type of keys, but the data types of the values can be different.\nThe join transformation is based on the join(other) method of the RDD class: the two input RDDs of pairs analyzed by join are the one which the method is invoked on and the one passed as parameter (i.e., other).\nA shuffle operation is executed for computing the result of the join() transformation, since keys from different partitions of the two input RDDs must be compared and values from different partitions must be retrieved.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate two RDDs of key-value pairs from two local python lists\n\nFirst list – List of questions (QuestionId, Text of the question): [(1,\"What is ...?\"),(2,\"Who is ...?\")]\nSecond list – List of answers (QuestionId, Text of the answer): [(1,\"It is a car\"),(1,\"It is a byke\"),(2,\"She is Jenny\")]\n\nCreate a new RDD of pairs to associate each question with its answers: one pair for each possible pair (question-answer)\n\n## Create the first local Python list\nquestions= [(1,\"What is ...?\"),(2,\"Who is ...?\")]\n\n## Create the RDD of pairs from the local list\nquestionsPairRDD = sc.parallelize(questions)\n\n## Create the second local python list\nanswers = [(1,\"It is a car\"),(1,\"It is a byke\"),(2,\"She is Jenny\")]\n\n## Create the RDD of pairs from the local list\nanswersPairRDD = sc.parallelize(answers)\n\n## Join questions with answers\njoinPairRDD = questionsPairRDD.join(answersPairRDD)\n\n\n\n\n\n\n\njoinPairRDD\nThe key part of the returned RDD of pairs is an integer number. The value part of the returned RDD of pairs is a tuple containing two values: (question,answer).\n\n\n\n\n\n\n\n\nCoGroup transformation\nThe goal is to associate each key \\(k\\) of the two input RDDs of key-value pairs with\n\nThe list of values associated with \\(k\\) in the first input RDD of pairs\nThe list of values associated with \\(k\\) in the second input RDD of pairs\n\nThe new RDD of key-value pairs has the same key data type of the two input RDDs of pairs, and has a tuple as value (the two lists of values of the two input RDDs of pairs). The two input RDDs of key-value pairs must have the same type of keys, but the data types of the values can be different.\nThe cogroup transformation is based on the cogroup(other) method of the RDD class: the two input RDDs of pairs analyzed by cogroup are the one which the method is invoked on and the one passed as parameter (i.e., other).\nA shuffle operation is executed for computing the result of the cogroup() transformation, since keys from different partitions of the two input RDDs must be compared and values from different partitions must be retrieved.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate two RDDs of key-value pairs from two local python lists\n\nFirst list – List of liked movies (userId,likedMovies): [(1,\"Star Trek\"),(1,\"Forrest Gump\"),(2,\"Forrest Gump\")]\nSecond list – List of liked directors (userId,likedDirector): [(1,\"Woody Allen\"),(2,\"Quentin Tarantino\"),(2,\"Alfred Hitchcock\")]\n\nCreate a new RDD of pairs containing one pair for each userId (key) associated with\n\nThe list of liked movies\nThe list of liked directors\n\n\nInputs - [(1,\"Star Trek\"),(1,\"Forrest Gump\"),(2,\"Forrest Gump\")] - [(1,\"Woody Allen\"),(2,\"Quentin Tarantino\"),(2,\"Alfred Hitchcock\")]\nOutput - (1,([\"Star Trek\",\"Forrest Gump\"],[\"Woody Allen\"])) - (2,([\"Forrest Gump\"],[\"Quentin Tarantino\",\"Alfred Hitchcock\"]))\n## Create the first local python list\nmovies= [\n    (1,\"Star Trek\"),\n    (1,\"Forrest Gump\"),\n    (2,\"Forrest Gump\")\n]\n\n## Create the RDD of pairs from the first local list\nmoviesPairRDD = sc.parallelize(movies)\n\n## Create the second local python list\ndirectors = [\n    (1,\"Woody Allen\"),\n    (2,\"Quentin Tarantino\"),\n    (2,\"Alfred Hitchcock\")\n]\n\n## Create the RDD of pairs from the second local list\ndirectorsPairRDD = sc.parallelize(directors)\n\n## Cogroup movies and directors per user\ncogroupPairRDD = moviesPairRDD.cogroup(directorsPairRDD)\n\n\n\n\n\n\n\ncogroupPairRDD\nNotice that the value part of the returned tuples is a tuple containing two lists: the first value contains the list of movies (iterable) liked by a user; the second value contains the list of directors (iterable) liked by a user.\n\n\n\n\n\n\n\n\nSummary\nAll the examples reported in the following tables are applied on the following two RDDs of key-value pairs\n\ninputRDD1: [('k1',2),('k3',4),('k3',6)]\ninputRDD2: [('k3',9)]\n\n\n\nPurposes\n\n\n\n\n\n\n\nTransformation\nPurpose\n\n\n\n\nsubtractByKey(other)\nReturn a new RDD of key-value pairs. The returned pairs are those of input RDD on which the method is invoked such that the key part does not occur in the keys of the RDD that is passed as parameter. The values are not considered to take the decision.\n\n\njoin(other)\nReturn a new RDD of pairs corresponding to join of the two input RDDs. The join is based on the value of the key.\n\n\ncogroup(other)\nFor each key \\(k\\) in one of the two input RDDs of pairs, return a pair \\((k, tuple)\\), where tuple contains:\n\nthe list (iterable) of values of the first input RDD associated with key \\(k\\);\nthe list (iterable) of values of the second input RDD associated with key \\(k\\).\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\nTransformation\nExample\nResult\n\n\n\n\nsubtractByKey(other)\ninputRDD1.subtractByKey(inputRDD2)\n[('k1',2)]\n\n\njoin(other)\ninputRDD1.join(inputRDD2)\n[('k3',(4,9)),('k3',(6,9))]\n\n\ncogroup(other)\ninputRDD1.cogroup(inputRDD2)\n[('k1',([2],[])),('k3',([4,6],[9]))]\n\n\n\n\n\n\n\n\nActions on RDDs of key-value pairs\nSpark supports also some specific actions on RDDs of key-value pairs (e.g., countByKey, collectAsMap, lookup).\n\nCountByKey action\nThe countByKey action returns a local python dictionary containing the information about the number of elements associated with each key in the input RDD of key-value pairs (i.e., the number of times each key occurs in the input RDD).\n\n\n\n\n\n\nWarning\n\n\n\nPay attention to the number of distinct keys of the input RDD of pairs: if the number of distinct keys is large, the result of the action cannot be stored in a local variable of the Driver.\n\n\nThe countBykey action is based on the countBykey() method of the RDD class. Notice that data are sent on the network to compute the final result.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of pairs from the following python list (each pair contains a movie and the rating given by someone to that movie): [(\"Forrest Gump\",4),(\"Star Trek\",5),(\"Forrest Gump\",3)];\nCompute the number of ratings for each movie.\n\n## Create the local python list\nmovieRating= [\n    (\"Forrest Gump\",4),\n    (\"Star Trek\",5),\n    (\"Forrest Gump\",3)\n]\n\n## Create the RDD of pairs from the local collection\nmovieRatingRDD = sc.parallelize(movieRating)\n\n## Compute the number of rating for each movie\nmovieNumRatings = movieRatingRDD.countByKey()\n\n## Print the result on the standard output\nprint(movieNumRatings)\n\n\n\n\n\n\n\nmovieNumRatings\nPay attention to the size of the returned local python dictionary (i.e., in this case, the number of distinct movies).\n\n\n\n\n\n\n\n\nCollectAsMap action\nThe collectAsMap action returns a local dictionary containing the same pairs of the considered input RDD of pairs. Pay attention to the size of the returned RDD: data are sent on the network.\nThe collectAsMap action is based on the collectAsMap() method of the RDD class.\n\n\n\n\n\n\nWarning\n\n\n\nPay attention that the collectAsMap action returns a dictionary object, and a dictionary cannot contain duplicate keys: each key can be associated with at most one value, and if the input RDD of pairs contains more than one pair with the same key, only one of those pairs is stored in the returned local python dictionary (usually, the last one occurring in the input RDD of pairs).\n\n\nUse collectAsMap only if sure that each key appears only once in the input RDD of key-value pairs.\nThe collectAsMap() method returns a local dictionary while collect() return a list of key-value pairs (i.e., a list of tuples). Notice that the list of pairs returned by collect() can contain more than one pair associated with the same key.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of pairs from the following python list (each pair contains a userId and the name of the user): [(\"User1\",\"Paolo\"),(\"User2\",\"Luca\"),(\"User3\",\"Daniele\")];\nRetrieve the pairs of the created RDD of pairs and store them in a local python dictionary that is instantiated in the Driver.\n\n## Create the local python list\nusers = [\n    (\"User1\",\"Paolo\"),\n    (\"User2\",\"Luca\"),\n    (\"User3\",\"Daniele\")\n]\n\n#Create the RDD of pairs from the local list\nusersRDD = sc.parallelize(users)\n\n## Retrieve the content of usersRDD and store it in a\n## local python dictionary\nretrievedPairs = usersRDD.collectAsMap()\n\n## Print the result on the standard output\nprint(retrievedPairs)\n\n\n\n\n\n\n\nretrievedPairs\nPay attention to the size of the returned local python dictionary (i.e., in this case, the number of distinct users).\n\n\n\n\n\n\n\n\nLookup action\nThe lookup(\\(k\\)) action returns a local python list containing the values of the pairs of the input RDD associated with the key \\(k\\) specified as parameter.\nThe lookup action is based on the lookup(key) method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD of pairs from the following python list (each pair contains a movie and the rating given by someone to that movie): [(\"Forrest Gump\",4),(\"Star Trek\",5),(\"Forrest Gump\",3)]\nRetrieve the ratings associated with the movie “Forrest Gump” and store them in a local python list in the Driver.\n\n## Create the local python list\nmovieRating= [(\"Forrest Gump\",4),(\"Star Trek\",5),(\"Forrest Gump\",3)]\n\n## Create the RDD of pairs from the local collection\nmovieRatingRDD = sc.parallelize(movieRating)\n\n## Select the ratings associated with “Forrest Gump”\nmovieRatings = movieRatingRDD.lookup(\"Forrest Gump\")\n\n## Print the result on the standard output\nprint(movieRatings)\n\n\n\n\n\n\n\nmovieRatings\nPay attention to the size of the returned local list (i.e., in this case, the number of ratings associated with “Forrest Gump”).\n\n\n\n\n\n\n\n\nSummary\nAll the examples reported in the following tables are applied on the following RDD of key-value pairs:\n\ninputRDD: [('k1',2),('k3',4),('k3',6)]\n\n\n\nPurposes\n\n\n\n\n\n\n\nTransformation\nPurpose\n\n\n\n\ncountByKey()\nReturn a local python dictionary containing the number of elements in the input RDD for each key of the input RDD of pairs.\n\n\ncollectAsMap()\nReturn a local python dictionary containing the pairs of the input RDD of pairs.\n\n\nlookup(key)\nReturn a local python list containing all the values associated with the key specified as parameter.\n\n\n\nExamples\n\n\n\n\n\n\n\n\nTransformation\nExample\nResult\n\n\n\n\ncountByKey()\ninputRDD.countByKey()\n{('k1',1),('K3',2)}\n\n\ncollectAsMap()\ninputRDD.collectAsMap()\n{('k1',2),('k3',6)}\nOr\n{('k1',2),('k3',4)}\nDepending on the order of the pairs in the input RDD of pairs\n\n\nlookup(key)\ninputRDD.lookup('k3')\n[4,6]"
  },
  {
    "objectID": "13_rdd_numbers.html",
    "href": "13_rdd_numbers.html",
    "title": "15  RDD of numbers",
    "section": "",
    "text": "Spark provides specific actions for RDD containing numerical values (integers or floats).\nRDDs of numbers can be created by using the standard methods\n\nparallelize\ntransformations that return and RDD of numbers\n\nThe following specific actions are also available on this type of RDDs\n\nsum()\nmean()\nstdev()\nvariance()\nmax()\nmin()\n\n\nSummary\nAll the examples reported in the following are applied on inputRDD that is an RDD containing the following double values: [1.5,3.5,2.0]\n\n\n\n\n\n\n\n\n\n\n\nAction\nPurpose\nExample\nResult\n\n\n\n\nsum()\nReturn the sum of the values of the input RDD.\ninputRDD.sum()\n7.0\n\n\nmean()\nReturn the mean computed over the values of the input RDD.\ninputRDD.mean()\n2.3333\n\n\nstdev()\nReturn the standard deviation computed over the values of the input RDD.\ninputRDD.stdev()\n0.8498\n\n\nvariance()\nReturn the variance computed over the values of the input RDD.\ninputRDD.variance()\n0.7223\n\n\nmax()\nReturn the maximum value.\ninputRDD.max()\n3.5\n\n\nmin()\nReturn the minimum value.\ninputRDD.min()\n1.5\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD containing the following float values: [1.5,3.5,2.0]\nPrint on the standard output the following statistics\n\nsum\nmean\nstandard deviation\nvariance\nmaximum value\nminimum value\n\n\n## Create an RDD containing a list of float values\ninputRDD = sc.parallelize([1.5,3.5,2.0])\n\n## Compute the statistics of interest and print them on\n## the standard output\nprint(\"sum:\", inputRDD.sum())\nprint(\"mean:\", inputRDD.mean())\nprint(\"stdev:\", inputRDD.stdev())\nprint(\"variance:\", inputRDD.variance())\nprint(\"max:\", inputRDD.max())\nprint(\"min:\", inputRDD.min())"
  },
  {
    "objectID": "14_cache_accumulators_broadcast.html",
    "href": "14_cache_accumulators_broadcast.html",
    "title": "16  Cache, Accumulators, Broadcast Variables",
    "section": "",
    "text": "Persistence and Cache\nSpark computes the content of an RDD each time an action is invoked on it. If the same RDD is used multiple times in an application, Spark recomputes its content every time an action is invoked on the RDD, or on one of its descendants, but this is expensive, especially for iterative applications.\nSo, it is possible to ask Spark to persist/cache RDDs: in this way, each node stores the content of its partitions in memory and reuses them in other actions on that RDD/dataset (or RDDs derived from it).\n\nThe first time the content of a persistent/cached RDD is computed in an action, it will be kept in the main memory of the nodes;\nThe next actions on the same RDD will read its content from memory (i.e., Spark persists/caches the content of the RDD across operations). This allows future actions to be much faster, often by more than ten times faster.\n\nSpark supports several storage levels, which are used to specify if the content of the RDD is stored\n\nIn the main memory of the nodes\nOn the local disks of the nodes\nPartially in the main memory and partially on disk\n\n\nStorage levels\n\n\n\n\n\n\nStorage Level\nMeaning\n\n\n\n\nMEMORY_ONLY\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.\n\n\nMEMORY_AND_DISK\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on (local) disk, and read them from there when they’re needed.\n\n\nDISK_ONLY\nStore the RDD partitions only on disk.\n\n\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.\nSame as the levels above, but replicate each partition on two cluster nodes.\n\n\nOFF_HEAP (experimental)\nSimilar to MEMORY_ONLY, but store the data in off-heap memory. This requires off-heap memory to be enabled.\n\n\n\nSee here for more details.\nIt is possible to mark an RDD to be persisted by using the persist(storageLevel) method of the RDD class. The parameter of persist can assume the following values\n\npyspark.StorageLevel.MEMORY_ONLY\npyspark.StorageLevel.MEMORY_AND_DISK\npyspark.StorageLevel.DISK_ONLY\npyspark.StorageLevel.NONE\npyspark.StorageLevel.OFF_HEAP\npyspark.StorageLevel.MEMORY_ONLY_2\npyspark.StorageLevel.MEMORY_AND_DISK_2\n\nThe storage level *_2 replicate each partition on two cluster nodes, so that, ff one node fails, the other one can be used to perform the actions on the RDD without recomputing the content of the RDD.\nIt is possible to cache an RDD by using the cache() method of the RDD class: it corresponds to persist the RDD with the storage level 'MEMORY_ONLY' (i.e., it is equivalent to inRDD.persist(pyspark.StorageLevel.MEMORY_ONLY))\n\n\n\n\n\n\nImportant\n\n\n\nNotice that both persist and cache return a new RDD, since RDDs are immutable.\n\n\nThe use of the persist/cache mechanism on an RDD provides an advantage if the same RDD is used multiple times (i.e., multiples actions are applied on it or on its descendants).\nThe storage levels that store RDDs on disk are useful if and only if\n\nthe size of the RDD is significantly smaller than the size of the input dataset;\nthe functions that are used to compute the content of the RDD are expensive.\n\nOtherwise, recomputing a partition may be as fast as reading it from disk.\n\nRemove data from cache\nSpark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. It is also possible to manually remove an RDD from the cache by using the unpersist() method of the RDD class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing a list of words (one word for each line);\nPrint on the standard output\n\nThe number of lines of the input file\nThe number of distinct words\n\n\n## Read the content of a textual file\n## and cache the associated RDD\ninputRDD = sc.textFile(\"words.txt\").cache()\n\nprint(\"Number of words: \",inputRDD.count())\nprint(\"Number of distinct words: \", inputRDD.distinct().count())\n\n\n\n\n\n\n\n.cache()\nThe cache method is invoked, hence inputRDD is a cached RDD\n\n\ninputRDD.count()\nThis is the first time an action is invoked on the inputRDD RDD. The content of the RDD is computed by reading the lines of the words.txt file and the result of the count action is returned. The content of inputRDD is also stored in the main memory of the nodes of the cluster.\n\n\ninputRDD.distinct().count()\nThe content of inputRDD is in the main memory if the nodes of the cluster. Hence the computation of distinct() and count() is performed by reading the data from the main memory and not from the input (HDFS) file words.txt.\n\n\n\n\n\n\n\n\n\nAccumulators\nWhen a function passed to a Spark operation is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each node of the cluster, and no updates to the variables on the nodes are propagated back to the driver program.\nSpark provides a type of shared variables called accumulators: accumulators are shared variables that are only “added” to through an associative operation and can therefore be efficiently supported in parallel, and they can be used to implement counters or sums.\nAccumulators are usually used to compute simple statistics while performing some other actions on the input RDD, avoiding to use actions like reduce() to compute simple statistics (e.g., count the number of lines with some characteristics).\n\nHow to use accumulators\n\nThe driver defines and initializes the accumulator\nThe code executed in the worker nodes increases the value of the accumulator (i.e., the code in the functions associated with the transformations)\nThe final value of the accumulator is returned to the driver node\n\nOnly the driver node can access the final value of the accumulator\nThe worker nodes cannot access the value of the accumulator: they can only add values to it\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPay attention that the value of the accumulator is increased in the functions associated with transformations, and, since transformations are lazily evaluated, the value of the accumulator is computed only when an action is executed on the RDD on which the transformations increasing the accumulator are applied.\n\n\nSpark natively supports numerical accumulators (integers and floats), but programmers can add support for new data types: accumulators are pyspark.accumulators.Accumulator objects.\nAccumulators are defined and initialized by using the accumulator(value) method of the SparkContext class: the value of an accumulator can be increased by using the add(value) method of the Accumulator class, that adds value to the current value of the accumulator. The final value of an accumulator can be retrieved in the driver program by using value of the Accumulator class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing a list of email addresses (one email for each line);\nSelect the lines containing a valid email and store them in an HDFS file (in this example, an email is considered a valid email if it contains the @ symbol);\nPrint also, on the standard output, the number of invalid emails.\n\n## Define an accumulator. Initialize it to 0\ninvalidEmails = sc.accumulator(0)\n\n## Read the content of the input textual file\nemailsRDD = sc.textFile(\"emails.txt\")\n\n#Define the filtering function\ndef validEmailFunc(line):\n    if (line.find('@')<0):\n        invalidEmails.add(1)\n        return False\n    else:\n        return True\n\n## Select only valid emails\n## Count also the number of invalid emails\nvalidEmailsRDD = emailsRDD.filter(validEmailFunc)\n\n## Store valid emails in the output file\nvalidEmailsRDD.saveAsTextFile(outputPath)\n\n## Print the number of invalid emails\nprint(\"Invalid email addresses: \", invalidEmails.value)\n\n\n\n\n\n\n\ninvalidEmails = sc.accumulator(0)\nDefinition of an accumulator of type integer.\n\n\ninvalidEmails.add(1)\nThis function increments the value of the invalidEmails accumulator if the email is invalid.\n\n\ninvalidEmails.value\nRead the final value of the accumulator. Pay attention that the value of the accumulator is correct only because an action (saveAsTextFile) has been executed on the validEmailsRDD and its content has been computed (the function validEmailFunc has been executed on each element of emailsRDD)\n\n\n\n\n\n\n\n\nPersonalized accumulators\nProgrammers can define accumulators based on new data types (different from integers and floats): to define a new accumulator data type of type \\(T\\), the programmer must define a class subclassing the AccumulatorParam interface. The AccumulatorParam interface has two methods\n\nzero for providing a zero value for your data type\naddInPlace for adding two values together\n\n\n\n\nBroadcast variables\nSpark supports broadcast variables. A broadcast variable is a read-only (small/medium) shared variable\n\nIt is instantiated in the driver: the broadcast variable is stored in the main memory of the driver in a local variable;\nIt is sent to all worker nodes that use it in one or more Spark operations: the broadcast variable is also stored in the main memory of the executors (which are instantiated in the used worker nodes).\n\nA copy each broadcast variable is sent to all executors that are used to run a task executing a Spark operation based on that variable (i.e., the variable is sent num.executors times). A broadcast variable is sent only one time to each executor that uses that variable in at least one Spark operation (i.e., in at least one of its tasks). Each executor can run multiples tasks associated with the same broadcast variable, but the broadcast variable is sent only one time for each executor, hence the amount of data sent on the network is limited by using broadcast variables instead of standard variables.\nBroadcast variables are usually used to share (small/medium) lookup-tables, and, since they are stored in local variables, they must the small enough to be stored in the main memory of the driver and also in the main memory of the executors.\nBroadcast variables are objects of type Broadcast: a broadcast variable (of type \\(T\\)) is defined in the driver by using the broadcast(value) method of the SparkContext class. The value of a broadcast variable (of type \\(T\\)) is retrieved (usually in transformations) by using value of the Broadcast class.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing a dictionary of pairs (word, integer value), one pair for each line. Suppose the content of this first file is large but can be stored in main-memory;\nCreate an RDD from a textual file containing a set of words, in particular a sentence (set of words) for each line; Transform the content of the second file mapping each word to an integer based on the dictionary contained in the first file; then, store the result in an HDFS file.\n\n\nFirst file (dictionary)\n\njava 1\nspark 2\ntest 3\n\nSecond file (the text to transform)\n\njava spark\nspark test java\n\nOutput file\n\n12\n231\n## Read the content of the dictionary from the first file and\n## map each line to a pair (word, integer value)\ndictionaryRDD = sc \\\n    .textFile(\"dictionary.txt\") \\\n    .map(lambda line: (\n        line.split(\" \")[0], \n        line.split(\" \")[1]\n    ))\n\n## Create a broadcast variable based on the content of dictionaryRDD.\n## Pay attention that a broadcast variable can be instantiated only\n## by passing as parameter a local variable and not an RDD.\n## Hence, the collectAsMap method is used to retrieve the content of the\n## RDD and store it in the dictionary variable\ndictionary = dictionaryRDD.collectAsMap()\n\n## Broadcast dictionary\ndictionaryBroadcast = sc.broadcast(dictionary)\n\n## Read the content of the second file\ntextRDD = sc.textFile(\"document.txt\")\n\n## Define the function that is used to map strings to integers\ndef myMapFunc(line):\n    transformedLine=''\n    for word in line.split(' '):\n        intValue = dictionaryBroadcast.value[word]\n        transformedLine = transformedLine+intValue+' '\n    return transformedLine.strip()\n\n## Map words in textRDD to the corresponding integers and concatenate\n## them\nmappedTextRDD= textRDD.map(myMapFunc)\n\n## Store the result in an HDFS file\nmappedTextRDD.saveAsTextFile(outputPath)\n\n\n\n\n\n\n\nsc.broadcast(dictionary)\nDefine a broadcast variable.\n\n\ndictionaryBroadcast.value[word]\nRetrieve the content of the broadcast variable and use it.\n\n\n\n\n\n\n\n\nRDDs and Partitions\nThe content of each RDD is split in partitions: the number of partitions and the content of each partition depend on how RDDs are defined/created. The number of partitions impacts on the maximum parallelization degree of the Spark application, but pay attention that the amount of resources is limited (there is a maximum number of executors and parallel tasks).\n\n\n\n\n\n\nHow many partitions are good?\n\n\n\nDisadvantages of too few partitions\n\nLess concurrency/parallelism: there could be worker nodes that are idle and could be used to speed up the execution of your application;\nData skewing and improper resource utilization: data might be skewed on one partition (i.e., one partition with many data, many partitions with few data). The worker node that processes the largest partition needs more time than the other workers, becoming the bottleneck of your application.\n\nDisadvantages of too many partitions\n\nTask scheduling may take more time than actual execution time if the amount of data in some partitions is too small\n\n\n\nOnly some specific transformations set the number of partitions of the returned RDD: parallelize(), textFile(), repartition(), coalesce(). The majority of the Spark transformations do not change the number of partitions, preserving the number of partitions of the input RDD (i.e., the returned RDD has the same number of partitions of the input RDD).\n\n\n\n\n\n\n\nTransformation\nEffect\n\n\n\n\nparallelize(collection)\nThe number of partitions of the returned RDD is equal to sc.defaultParallelism.\nSparks tries to balance the number of elements per partition in the returned RDD; notice that elements are not assigned to partitions based on their value.\n\n\nparallelize(collection, numSlices)\nThe number of partitions of the returned RDD is equal to numSlices.\nSparks tries to balance the number of elements per partition in the returned RDD; notice that elements are not assigned to partitions based on their value.\n\n\ntextFile(pathInputData)\nThe number of partitions of the returned RDD is equal to the number of input chunks/blocks of the input HDFS data.\nEach partition contains the content of one of the input blocks.\n\n\ntextFile(pathInputData, minPartitions)\nThe user specified number of partitions must be greater than the number of input blocks.\nThe number of partitions of the returned RDD is greater than or equal to the specified value minPartitions, and each partition contains a part of one input blocks.\n\n\nrepartition(numPartitions)\nnumPartitions can be greater or smaller than the number of partitions of the input RDD, and the number of partitions of the returned RDD is equal to numPartitions.\nSparks tries to balance the number of elements per partition in the returned RDD; notice that elements are not assigned to partitions based on their value.\nA shuffle operation is executed to assign input elements to the partitions of the returned RDD.\n\n\ncoalesce(numPartitions)\nnumPartitions is smaller than the number of partitions of the input RDD, and the number of partitions of the returned RDD is equal to numPartitions.\nSparks tries to balance the number of elements per partition in the returned RDD; notice that elements are not assigned to partitions based on their value.\nUsually no shuffle operation is executed to assign input elements to the partitions of the returned RDD, so coalesce is more efficient than repartition to reduce the number of partitions.\n\n\n\nSpark allows specifying how to partition the content of RDDs of key-value pairs: he input tpairs are grouped in partitions based on the integer value returned by a function applied on the key of each input pair. This operation can be useful to improve the efficiency of the next transformations by reducing the amount of shuffle operations and the amount of data sent on the network in the next steps of the application (Spark can optimize the execution of the transformations if the input RDDs of pairs are properly partitioned).\n\npartitionBy(numPartitions)\nPartitioning is based on the partitionBy() transformation. The input pairs are grouped in partitions based on the integer value returned by a default hash function applied on the key of each input pair. A shuffle operation is executed to assign input elements to the partitions of the returned RDD.\nSuppose that the number of partition of the returned Pair RDD is \\(\\textbf{numPart}\\). The default partition function is portable_hash: given an input pair \\((key, value)\\) a copy of that pair will be stored in the partition number \\(n\\) of the returned RDD, where\n\\[\nn = portable\\_hash(key) \\% numPart\n\\]\nNotice that \\(portable\\_hash(key)\\) returns and integer.\n\npartitionBy(numPartitions, partitionFunc)\nThe input pairs are grouped in partitions based on the integer value returned by the user provided partitionFunc function. A shuffle operation is executed to assign input elements to the partitions of the returned RDD.\nSuppose that the number of partition of the returned Pair RDD is \\(\\textbf{numPart}\\). The custom partition function is partitionFunc: given an input pair \\((key, value)\\) a copy of that pair will be stored in the partition number \\(n\\) of the returned RDD, where\n\\[\nn = partitionFunc(key) \\% numPart\n\\]\n\n\n\n\n\n\nUse case scenario\n\n\n\nPartitioning Pair RDDs by using partitionBy() is useful only when the same partitioned RDD is cached and reused multiple times in the application in time and network consuming key-oriented transformations.\nFor example, the same partitioned RDD is used in many join(), cogroup(), groupyByKey(), … transformations in different paths/branches of the application (different paths/branches of the DAG).\nPay attention to the amount of data that is actually sent on the network: partitionBy() can slow down the application instead of speeding it up.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate an RDD from a textual file containing a list of pairs (pageID, list of linked pages);\nImplement the (simplified) PageRank algorithm and compute the pageRank of each input page;\nPrint the result on the standard output.\n\n## Read the input file with the structure of the web graph\ninputData = sc.textFile(\"links.txt\")\n\n## Format of each input line\n## PageId,LinksToOtherPages - e.g., P3 [P1,P2,P4,P5]\ndef mapToPairPageIDLinks(line):\n    fields = line.split(' ')\n    pageID = fields[0]\n    links = fields[1].split(',')\n    return (pageID, links)\n\nlinks = inputData.map(mapToPairPageIDLinks) \\\n    .partitionBy(inputData.getNumPartitions()) \\\n    .cache()\n\n## Initialize each page's rank to 1.0; since we use mapValues,\n## the resulting RDD will have the same partitioner as links\nranks = links.mapValues(lambda v: 1.0)\n\n## Function that returns a set of pairs from each input pair\n## input pair: (pageid, (linked pages, current page rank of pageid) )\n## one output pair for each linked page. Output pairs:\n## (pageid linked page,\n## current page rank of the linking page pageid / number of linked pages)\ndef computeContributions(pageIDLinksPageRank):\n    pagesContributions = []\n    currentPageRank = pageIDLinksPageRank[1][1]\n    linkedPages = pageIDLinksPageRank[1][0]\n    numLinkedPages = len(linkedPages)\n    contribution = currentPageRank/numLinkedPages\n\n    for pageidLinkedPage in linkedPages:\n        pagesContributions.append((pageidLinkedPage, contribution))\n    \n    return pagesContributions\n\n## Run 30 iterations of PageRank\nfor x in range(30):\n    ## Retrieve for each page its current pagerank and\n    ## the list of linked pages by using the join transformation\n\n    pageRankLinks = links.join(ranks)\n    ## Compute contributions from linking pages to linked pages\n    ## for this iteration\n    \n    contributions = pageRankLinks.flatMap(computeContributions)\n    ## Update current pagerank of all pages for this iteration\n    ranks = contributions\\\n        .reduceByKey(lambda contrib1, contrib2: contrib1+contrib2)\n\n## Print the result\nranks.collect()\n\n\n\n\n\n\n\n.partitionBy()...cache()\nNotice that the returned Pair RDD is partitioned and cached.\n\n\nlinks\nThe join transformation is invoked many times on the links Pair RDD. The content of links is constant (it does not change during the loop iterations). Hence, caching it and also partitioning its content by key is useful: its content is computed one time and cached in the main memory of the executors, and it is shuffled and sent on the network only one time because partitionBy was applied on it\n\n\n\n\n\n\n\n\n\nDefault partitioning behavior of the main transformations\n\n\n\n\n\n\n\n\n\n\nTransformation\nNumber of partitions\nPartitioner\n\n\n\n\nsc.parallelize()\nsc.defaultParallelism\nNONE\n\n\nsc.textFile()\nthe maximum between sc.defaultParallelism and the number of file blocks\nNONE\n\n\nfilter(), map(), flatMap(), distinct()\nsame as parent RDD\nNONE except filter preserve parent RDD’s partitioner\n\n\nrdd.union(otherRDD)\nrdd.partitions.size + otherRDD.partitions.size\nNONE except filter preserve parent RDD’s partitioner\n\n\nrdd.intersection(otherRDD)\nmax(rdd.partitions.size, otherRDD.partitions.size)\nNONE except filter preserve parent RDD’s partitioner\n\n\nrdd.subtract(otherRDD)\nrdd.partitions.size\nNONE except filter preserve parent RDD’s partitioner\n\n\nrdd.cartesian(otherRDD)\nrdd.partitions.size * otherRDD. partitions.size\nNONE except filter preserve parent RDD’s partitioner\n\n\nreduceByKey(), foldByKey(), combineByKey(), groupByKey()\nsame as parent RDD\nHashPartitioner\n\n\nsortByKey()\nsame as parent RDD\nRangePartitioner\n\n\nmapValues(), flatMapValues()\nsame as parent RDD\nparent RDD’s partitioner\n\n\ncogroup(), join(),leftOuterJoin(), rightOuterJoin()\ndepends upon input properties of two involved RDDs\nHashPartitioner\n\n\n\n\n\n\n\n\nBroadcast join\nThe join transformation is expensive in terms of execution time and amount of data sent on the network. If one of the two input RDDs of key-value pairs is small enough to be stored in the main memory, then it is possible to use a more efficient solution based on a broadcast variable.\n\nBroadcast hash join (or map-side join)\nThe smaller the small RDD, the higher the speed up\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a large RDD from a textual file containing a list of pairs (userID, post); notice that each user can be associated to several posts;\nCreate a small RDD from a textual file containing a list of pairs (userID, (name, surname, age)); notice that each user can be associated to one single line in this second file;\nCompute the join between these two files.\n\n## Read the first input file\nlargeRDD = sc \\\n    .textFile(\"post.txt\") \\\n    .map(lambda line: (\n        int(line.split(',')[0]), \n        line.split(',')[1]\n    ))\n\n## Read the second input file\nsmallRDD = sc \\\n    .textFile(\"profiles.txt\") \\\n    .map(lambda line: (\n        int(line.split(',')[0]), \n        line.split(',')[1]\n    ))\n\n## Broadcast join version\n## Store the \"small\" RDD in a local python variable in the driver\n## and broadcast it\nlocalSmallTable = smallRDD.collectAsMap()\nlocalSmallTableBroadcast = sc.broadcast(localSmallTable)\n\n## Function for joining a record of the large RDD with the matching\n## record of the small one\ndef joinRecords(largeTableRecord):\n    returnedRecords = []\n    key = largeTableRecord[0]\n    valueLargeRecord = largeTableRecord[1]\n\n    if key in localSmallTableBroadcast.value:\n        returnedRecords.append((\n            key,\n            (valueLargeRecord,localSmallTableBroadcast.value[key])\n        ))\n\n    return returnedRecords\n\n## Execute the broadcast join operation by using a flatMap\n## transformation on the \"large\" RDD\nuserPostProfileRDDBroadcatJoin = largeRDD.flatMap(joinRecords)"
  },
  {
    "objectID": "15b_pagerank.html",
    "href": "15b_pagerank.html",
    "title": "17  Introduction to PageRank",
    "section": "",
    "text": "PageRank is the original famous algorithm used by the Google Search engine to rank vertexes (web pages) in a graph by order of importance. For the Google search engine, vertexes are web pages in the World Wide Web, and edges are hyperlinks among web pages: PageRank works by assigning a numerical weighting (importance) to each node.\nIn other words, it computes a likelihood that a person randomly clicking on links will arrive at any particular web page. So, to have a high PageRank, it is important to have many in-links, and be liked by relevant pages (pages characterized by a high PageRank).\n\n\nPageRank basic idea\n\n\n\n\n\n\n\n\n\nBasic idea\n\n\n\n\nThe vote of each link is proportional to the importance of its source page \\(p\\);\nIf page \\(p\\) with importance \\(\\textbf{PageRank}(p)\\) has \\(n\\) out-links, each out-link gets \\(\\frac{\\textbf{PageRank}(p)}{n}\\) votes;\nPage \\(p\\) importance is the sum of the votes on its in-links.\n\n\n\n\nPageRank formulations\n\nSimple recursive formulation\n\nInitialize each page rank to \\(1.0\\): for each \\(p\\) in pages set \\(\\textbf{PageRank}(p)\\) to \\(1.0\\)\nIterate for \\(max\\) iterations\n\nPage \\(p\\) sends a contribution \\(\\frac{\\textbf{PageRank}(p)}{\\textbf{numOutLinks}(p)}\\) to its neighbors (the pages it links);\nUpdate each page rank \\(\\textbf{PageRank}(p)\\) with the sum of the received contributions.\n\n\n\n\nRandom jumps formulation\nThe PageRank algorithm simulates the “random walk” of a user on the web. Indeed, at each step of the random walk, the random surfer has two options:\n\nwith probability \\(1-\\alpha\\), follow a link at random among the ones in the current page;\nwith probability \\(\\alpha\\), jump to a random page.\nInitialize each page rank to \\(1.0\\): for each \\(p\\) in pages set \\(\\textbf{PageRank}(p)\\) to \\(1.0\\)\nIterate for max iterations\n\nPage \\(p\\) sends a contribution \\(\\frac{\\textbf{PageRank}(p)}{\\textbf{numOutLinks}(p)}\\) to its neighbors (the pages it links);\nUpdate each page rank \\(\\textbf{PageRank}(p)\\) to \\(\\alpha + (1 - \\alpha)\\) times the sum of the received contributions.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\\(\\alpha=0.15\\)\nInitialization: \\(\\forall{p}, \\textbf{PageRank}(p) = 1.0\\)\n\n\n\nInitialization\n\n\n\n\nFigure 17.1: Iterations\n\n\n\n\n\nIteration #1\n\n\n\n\n\n\n\nIteration #2\n\n\n\n\n\n\n\n\n\nIteration #3\n\n\n\n\n\n\n\nIteration #4\n\n\n\n\n\n\n\n\n\nIteration #5\n\n\n\n\n\n\n\nIteration #50"
  },
  {
    "objectID": "16_sparksql_dataframes.html",
    "href": "16_sparksql_dataframes.html",
    "title": "18  Spark SQL and DataFrames",
    "section": "",
    "text": "Spark SQL\nSpark SQL is the Spark component for structured data processing. It provides a programming abstraction called Dataframe and can act as a distributed SQL query engine: the input data can be queried by using\n\nAd-hoc methods\nOr an SQL-like language\n\n\nSpark SQL vs Spark RDD APIs\nThe interfaces provided by Spark SQL provide more information about the structure of both the data and the computation being performed. Spark SQL uses this extra information to perform extra optimizations based on a “SQL-like” optimizer called Catalyst, and so programs based on Dataframe are usually faster than standard RDD-based programs.\n\n\nSpark SQL vs Spark RDD APIs\n\n\n\n\n\nDataFrames\nA DataFrame is a distributed collection of structured data. It is conceptually equivalent to a table in a relational database, and it can be created reading data from different types of external sources (CSV files, JSON files, RDBMs,…). A DataFrame benefits from Spark SQL optimized execution engine exploiting the information about the data structure.\nAll the Spark SQL functionalities are based on an instance of the pyspark.sql.SparkSession class\nTo import it in a standalone application use\nfrom pyspark.sql import SparkSession\nTo instance a SparkSession object use\nspark = SparkSession.builder.getOrCreate()\nTo close a SparkSession use the SparkSession.stop() method\nspark.stop()\n\n\n\nDataFrames\nA DataFrame is a distributed collection of data organized into named columns, equivalent to a relational table: DataFrames are lists of Row objects.\nThe classes used to define DataFrames are\n\npyspark.sql.DataFrame\npyspark.sql.Row\n\nDataFrames can be created from different sources\n\nStructured (textual) data files (e.g., csv files, json files);\nExisting RDDs;\nHive tables;\nExternal relational databases.\n\n\nCreating DataFrames from csv files\nSpark SQL provides an API that allows creating DataFrames directly from CSV files. The creation of a DataFrame from a csv file is based the load(path) method of the pyspark.sql.DataFrameReader class, where path is the path of the input file. To get a DataFrameReader using the the read() method of the SparkSession class.\ndf = spark.read.load(path, options)\n\n\n\n\n\n\nExample\n\n\n\n\n\nCreate a DataFrame from a csv file (“people.csv”) containing the profiles of a set of people. Each line of the file contains name and age of a person, and age can assume the null value (i.e., it can be missing). The first line contains the header (i.e., the names of the attributes/columns).\nExample of csv file\nname,age\nAndy,30\nMichael,\nJustin,19\nNotice that the age of the second person is unknown.\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n\n\n\n\n\n\nformat=\"csv\"\nThis is used to specify the format of the input file\n\n\nheader=True\nThis is used to specify that the first line of the file contains the name of the attributes/columns\n\n\ninferSchema=True\nThis method is used to specify that the system must infer the data types of each column. Without this option all columns are considered strings\n\n\n\n\n\n\n\n\nCreating DataFrames from JSON files\nSpark SQL provides an API that allows creating a DataFrame directly from a textual file where each line contains a JSON object. Hence, the input file is not a standard JSON file: it must be properly formatted in order to have one JSON object (tuple) for each line. So, the format of the input file must be compliant with the JSON Lines text format, also called newline-delimited JSON.\nThe creation of a DataFrame from JSON files is based on the same method used for reading csv files, that is the load(path) method of the pyspark.sql.DataFrameReader class, where path is the path of the input file. To get a DataFrameReader use the read() method of the SparkSession class.\ndf = spark.read.load(path, format=\"json\", options)\nor\ndf = spark.read.json(path, options)\nThe same API allows also reading standard multiline JSON files by setting the multiline option to true by setting the argument multiLine=True on the defined DataFrameReader for reading standard JSON files (this feature is available since Spark 2.2.0).\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that reading a set of small JSON files from HDFS is very slow.\n\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\nCreate a DataFrame from a JSON Lines text formatted file (“people.json”) containing the profiles of a set of people: each line of the file contains a JSON object containing name and age of a person. Age can assume the null value.\nExample of JSON file\n{\"name\":\"Michael\"}\n{\"name\":\"Andy\", \"age\":30}\n{\"name\":\"Justin\", \"age\":19}\nNotice that the age of the first person is unknown.\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.json\",\n    format=\"json\"\n)\n\n\n\n\n\n\n\nformat=\"json\"\nThis method is used to specify the format of the input file.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\nCreate a DataFrame from a folder containing a set of standard multiline JSON files: each input JSON file contains the profile of one person, in particular each file contains name and age of a person. Age can assume the null value.\nExample of JSON file\n{\"name\":\"Andy\", \"age\":30}\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"folder_JSONFiles/\",\n    format=\"json\",\n    multiLine=True\n)\n\n\n\n\n\n\n\nmultiLine=True\nThis multiline option is set to true to specify that the input files are standard multiline JSON files.\n\n\n\n\n\n\n\n\nCreating DataFrames from other data sources\nThe DataFrameReader class (the same we used for reading a json file and store it in a DataFrame) provides other methods to read many standard (textual) formats and read data from external databases:\n\nApache parquet files\nexternal relational database, through a JDBC connection\nHive tables\n…\n\n\n\nCreating DataFrames from RDDs or Python lists\nThe content of an RDD of tuples or the content of a Python list of tuples can be stored in a DataFrame by using the spark.createDataFrame(data,schema) method, where data is a RDD of tuples or Rows, Python list of tuples or Rows, or pandas DataFrame, and schema is a list of string with the names of the columns/attributes. schema is optional, and if not specified the column names are set to _1, _2, …, _n for input RDDs/lists of tuples.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCreate a DataFrame from the following Python list\n[\n    (19,\"Justin\"),\n    (30,\"Andy\"),\n    (None,\"Michael\")\n]\nThe column names must be set to “age” and “name”.\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a Python list of tuples\nprofilesList = [\n    (19,\"Justin\"),\n    (30,\"Andy\"),\n    (None,\"Michael\")\n]\n\n## Create a DataFrame from the profilesList\ndf = spark.createDataFrame(profilesList,[\"age\",\"name\"])\n\n\n\n\n\nFrom DataFrame to RDD\nThe rdd method of the DataFrame class returns an RDD of Row objects containing the content of the DataFrame which it is invoked on. Each Row object is like a dictionary containing the values of a record: it contains column names in the keys and column values in the values.\n\nUsage of the Row class\nThe fields in it can be accessed:\n\nlike attributes: row.key, where key is a column name;\nlike dictionary values: row[\"key\"];\nusing for key in row will search through row keys.\n\nAlso the asDict() method returns the Row content as a Python dictionary.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from a csv file containing the profiles of a set of people: each line of the file contains name and age of a person, but the first line contains the header (i.e., the name of the attributes/columns);\nTransform the input DataFrame into an RDD, select only the name field/column and store the result in the output folder.\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Define an RDD based on the content of\n## the DataFrame\nrddRows = df.rdd\n\n## Use the map transformation to extract\n## the name field/column\nrddNames = rddRows.map(lambda row: row.name)\n\n## Store the result\nrddNames.saveAsTextFile(outputPath)\n\n\n\n\n\n\n\nOperations on DataFrames\nA set of specific methods are available for the DataFrame class (e.g., show(), printSchema(), count(), distinct(), select(), filter()), also the standard collect() and count() actions are available.\n\nShow method\nThe show(n) method of the DataFrame class prints on the standard output the first n of the input DataFrame. Default value of n is 20.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from a csv file containing the profiles of a set of people;\nPrint the content of the first 2 people (i.e., the first 2 rows of the DataFrame).\n\nThe content of people.csv is\nname,age\nAndy,30\nMichael,\nJustin,19\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ndf.show(2)\n\n\n\n\n\nPrintSchema method\nThe printSchema() method of the DataFrame class prints on the standard output the schema of the DataFrame (i.e., the name of the attributes of the data stored in the DataFrame).\n\n\nCount method\nThe count() method of the DataFrame class returns the number of rows in the input DataFrame.\n\n\nDistinct method\nThe distinct() method of the DataFrame class returns a new DataFrame that contains only the unique rows of the input DataFrame. A shuffle phase is needed.\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that the distinct operation is always an heavy operation in terms of data sent on the network.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from a csv file containing the names of a set of people. The first line is the header.\nCreate a new DataFrame without duplicates.\n\nThe content of “names.csv” is\nname\nAndy\nMichael\nJustin\nMichael\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from names.csv\ndf = spark.read.load(\n    \"names.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ndf_distinct = df.distinct()\n\n\n\n\n\nSelect method\nThe select(col1, ..., coln) method of the DataFrame class returns a new DataFrame that contains only the specified columns of the input DataFrame. Use * as special column to select all columns\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that the select method can generate errors at runtime if there are mistakes in the names of the columns.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from the “people2.csv” file that scontains the profiles of a set of people\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: one line per person, and each line contains name, age, and gender of a person.\n\nCreate a new DataFrame containing only name and age of the people.\n\nThe “people2.csv” has the following structure\nname,age,gender\nPaul,40,male\nJohn,40,male\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people2.csv\ndf = spark.read.load(\n    \"people2.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ndfNamesAges = df.select(\"name\",\"age\")\n\n\n\n\n\nSelectExpr method\nThe selectExpr(expression1,...,expressionN) method of the DataFrame class is a variant of the select method, where expr can be a SQL expression.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate a DataFrame from the “people2.csv” file that scontains the profiles of a set of people\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: one line per person, and each line contains name, age, and gender of a person.\n\nCreate a new DataFrame containing only name and age of the people.\nCreate a new DataFrame containing only the name of the people and their age plus one. Call the age column as “new_age”.\n\nThe “people2.csv” has the following structure\nname,age,gender\nPaul,40,male\nJohn,40,male\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people2.csv\ndf = spark.read.load(\n    \"people2.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ndfNamesAges = df.selectExpr(\"name\",\"age\")\n\ndfNamesAgesMod = df.selectExpr(\"name\", \"age + 1 AS new_age\")\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate a DataFrame from the “people2.csv” file that contains the profiles of a set of people\n\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: each line contains name, age, and gender of a person.\n\n\nCreate a new DataFrame containing the same columns of the initial dataset plus an additional column called “newAge” containing the value of age incremented by one.\n\nThe “people2.csv” has the following structure\nname,age,gender\nPaul,40,male\nJohn,40,male\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people2.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Create a new DataFrame with four columns:\n## name, age, gender, newAge = age +1\ndfNewAge = df.selectExpr(\n    \"name\",\n    \"age\",\n    \"gender\",\n    \"age+1 as newAge\"\n)\n\n\n\n\n\n\n\n\"... as newAge\"\nThis part of the expression is used to specify the name of the column associated with the result of the first part of the expression in the returned DataFrame. Without this part of the expression, the name of the returned column would be “age+1”.\n\n\n\n\n\n\n\n\nFilter method\nThe filter(conditionExpr) method of the DataFrame class returns a new DataFrame that contains only the rows satisfying the specified condition. The condition is expressed as a Boolean SQL expression.\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that this version of the filter method can generate errors at runtime if there are errors in the filter expression: the parameter is a string and the system cannot check the correctness of the expression at compile time.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: each line contains name and age of a person.\n\nCreate a new DataFrame containing only the people with age between 20 and 31.\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ndf_filtered = df.filter(\"age>=20 and age<=31\")\n\n\n\n\n\nWhere method\nThe where(expression) method of the DataFrame class is an alias of the filter(conditionExpr) method.\n\n\nJoin\nThe join(right, on, how) method of the DataFrame class is used to join two DataFrames. It returns a DataFrame that contains the join of the tuples of the two input DataFrames based on the on join condition.\non specifies the join condition. It can be:\n\na string: the column to join\na list of strings: multiple columns to join\na condition/an expression on the columns (e.g., joined_df = df.join(df2, df.name == df2.name))\n\nhow specifies the type of join\n\ninner (default type of join)\ncross\nouter\nfull\nfull_outer\nleft\nleft_outer\nright\nright_outer\nleft_semi\nleft_anti\n\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that this method: can generate errors at runtime if there are errors in the join expression.\n\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate two DataFrames\n\nOne based on the “people_id.csv” file that contains the profiles of a set of people, the schema is: uid, name, age;\nOne based on the liked_sports.csv file that contains the liked sports for each person, the schema is: uid, sportname. 2.Join the content of the two DataFrames (uid is the join column) and show it on the standard output.\n\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Read people_id.csv and store it in a DataFrame\ndfPeople = spark.read.load(\n    \"people_id.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Read liked_sports.csv and store it in a DataFrame\ndfUidSports = spark.read.load(\n    \"liked_sports.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Join the two input DataFrames\ndfPersonLikes = dfPeople.join(\n    dfUidSports,\n    dfPeople.uid == dfUidSports.uid\n)\n\n## Print the result on the standard output\ndfPersonLikes.show()\n\n\n\n\n\n\n\ndfPeople.uid == dfUidSports.uid\nSpecify the join condition on the uid columns.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate two DataFrames\n\nOne based on the “people_id.csv” file that contains the profiles of a set of people, the schema is: uid, name, age;\nOne based on the banned.csv file that contains the banned users, the schema is: uid, bannedmotivation.\n\nSelect the profiles of the non-banned users and show them on the standard output.\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Read people_id.csv and store it in a DataFrame\ndfPeople = spark.read.load(\n    \"people_id.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Read banned.csv and store it in a DataFrame\ndfBannedUsers = spark.read.load(\n    \"banned.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Apply the Left Anti Join on the two input DataFrames\ndfSelectedProfiles = dfPeople.join(\n    dfBannedUsers,\n    dfPeople.uid == dfBannedUsers.uid,\n    \"left_anti\"\n)\n\n## Print the result on the standard output\ndfSelectedProfiles.show()\n\n\n\n\n\n\n\ndfPeople.uid == dfUidSports.uid\nSpecify the (anti) join condition on the uid columns.\n\n\n\"left_anti\"\nUse Left Anti Join.\n\n\n\n\n\n\n\n\nAggregate functions\nAggregate functions are provided to compute aggregates over the set of values of columns. Some of the provided aggregate functions/methods are\n\navg(column)\ncount(column)\nsum(column)\nabs(column)\n…\n\nEach aggregate function returns one value computed by considering all the values of the input column.\nThe agg(expr) method of the DataFrame class is used to specify which aggregate function we want to apply on one input column. The result is a DataFrame containing one single row and one single column, and the name of the return column is “function_name(column)”.\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that this methods can generate errors at runtime (e.g., wrong attribute name, wrong data type).\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people (each line contains name and age of a person)\n\nThe first line contains the header;\nThe others lines contain the users’ profiles.\n\nCreate a Dataset containing the average value of age.\n\nInput file example\nname,age\nAndy,30\nMichael,15\nJustin,19\nAndy,40\nExpected output example\navg(age)\n26.0\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Compute the average of age\naverageAge = df.agg({\"age\": \"avg\"})\n\n\n\n\n\ngroupBy and aggregate functions\nThe method groupBy(col1, ..., coln) method of the DataFrame class combined with a set of aggregate methods can be used to split the input data in groups and compute aggregate function over each group.\n\n\n\n\n\n\nDanger\n\n\n\nPay attention that this methods can generate errors at runtime if there are semantic errors (e.g., wrong attribute names, wrong data types).\n\n\nIt is possible to specify which attributes are used to split the input data in groups by using the groupBy(col1, ..., coln) method, and then, apply the aggregate functions to compute by final result (the result is a DataFrame).\nSome of the provided aggregate functions/methods are\n\navg(column)\ncount(column)\nsum(column)\nabs(column)\n…\n\nOtherwise, the agg() method can be used to apply multiple aggregate functions at the same time over each group.\nSee the static methods of the pyspark.sql.GroupedData class for a complete list.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people\n\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: each line contains name and age of a person.\n\n\nCreate a DataFrame containing the for each name the average value of age.\n\nInput file example\nname,age\nAndy,30\nMichael,15\nJustin,19\nAndy,40\nExpected output example\nname,avg(age)\nAndy,35\nMichael,15\nJustin,19\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ngrouped = df.groupBy(\"name\").avg(\"age\")\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people\n\nThe first line contains the header\nThe others lines contain the users’ profiles: each line contains name and age of a person\n\nCreate a DataFrame containing the for each name the average value of age and the number of person with that name\n\nInput file example\nname,age\nAndy,30\nMichael,15\nJustin,19\nAndy,40\nExpected output example\nname,avg(age),count(name)\nAndy,35,2\nMichael,15,1\nJustin,19,1\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ngrouped = df.groupBy(\"name\") \\\n    .agg({\"age\": \"avg\", \"name\": \"count\"})\n\n\n\n\n\nSort method\nThe sort(col1, ..., coln, ascending=True) method of the DataFrame class returns a new DataFrame that contains the same data of the input one, but whose content is sorted by col1, ..., coln. ascending determines if the sort should be ascending (True) or descending (False).\n\n\n\nDataFrames and the SQL language\nSparks allows querying the content of a DataFrame also by using the SQL language, but in order to do this a table name must be assigned to a DataFrame. The createOrReplaceTempView(tableName) method of the DataFrame class can be used to assign a tableName as table name to the DataFrame which it is invoked on.\nOnce the DataFrame has been mapped to table names, SQL-like queries can be executed (the executed queries return DataFrame objects). The sql(query) method of the SparkSession class can be used to execute a SQL-like query, where query is a SQL-like query. Currently some SQL features are not supported (e.g., nested subqueries in the “WHERE” clause are not allowed).\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate a DataFrame from a JSON file containing the profiles of a set of people: each line of the file contains a JSON object containing name, age, and gender of a person;\nCreate a new DataFrame containing only the people with age between 20 and 31 and print them on the standard output (use the SQL language to perform this operation).\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.json\",\n    format=\"json\"\n)\n\n## Assign the “table name” people to the df DataFrame\ndf.createOrReplaceTempView(\"people\")\n\n## Select the people with age between 20 and 31\n## by querying the people table\nselectedPeople = spark.sql(\n    \"SELECT * FROM people WHERE age>=20 and age<=31\"\n)\n\n## Print the result on the standard output\nselectedPeople.show()\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate two DataFrames\n\nOne based on the “people_id.csv” file that contains the profiles of a set of people, the schema is: uid, name, age;\nOne based on the “liked_sports.csv” file that contains the liked sports for each person, the schema is: uid, sportname.\n\nJoin the content of the two DataFrames and show it on the standard output.\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Read people_id.csv and store it in a DataFrame\ndfPeople = spark.read.load(\n    \"people_id.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Assign the “table name” people to the dfPerson\ndfPeople.createOrReplaceTempView(\"people\")\n\n## Read liked_sports.csv and store it in a DataFrame\ndfUidSports = spark.read.load(\n    \"liked_sports.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Assign the “table name” liked to dfUidSports\ndfUidSports.createOrReplaceTempView(\"liked\")\n\n## Join the two input tables by using the\n#SQL-like syntax\ndfPersonLikes = spark.sql(\n    \"SELECT * from people, liked where people.uid=liked.uid\"\n)\n\n## Print the result on the standard output\ndfPersonLikes.show()\n\n\n\n\n\n\n\n\n\nExample 3\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: each line contains name and age of a person.\n\nCreate a DataFrame containing for each name the average value of age and the number of person with that name. Print its content on the standard output.\n\nInput file example\nname,age\nAndy,30\nMichael,15\nJustin,19\nAndy,40\nExpected output example\nname,avg(age),count(name)\nAndy,35,2\nMichael,15,1\nJustin,19,1\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.json\",\n    format=\"json\"\n)\n\n## Assign the “table name” people to the df DataFrame\ndf.createOrReplaceTempView(\"people\")\n\n## Define groups based on the value of name and\n## compute average and number of records for each group\nnameAvgAgeCount = spark.sql(\n    \"SELECT name, avg(age), count(name) FROM people GROUP BY name\"\n)\n\n## Print the result on the standard output\nnameAvgAgeCount.show()\n\n\n\n\n\nSave DataFrames\nThe content of DataFrames can be stored on disk by using two approches\n\nConvert DataFrames to traditional RDDs by using the rdd method of the DataFrame, and then use saveAsTextFile(outputFolder);\nUse the write() method of DataFrames, that returns a DatFrameWriter class instance.\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: each line contains name, age, and gender of a person.\n\nStore the DataFrame in the output folder by using the saveAsTextFile() method.\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Save it\ndf.rdd.saveAsTextFile(outputPath)\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nCreate a DataFrame from the “people.csv” file that contains the profiles of a set of people\n\nThe first line contains the header;\nThe others lines contain the users’ profiles: each line contains name, age, and gender of a person.\n\nStore the DataFrame in the output folder by using the write() method, with the CSV format.\n\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Create a DataFrame from people.csv\ndf = spark.read.load(\n    \"people.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Save it\ndf.write.csv(outputPath, header=True)\n\n\n\n\n\nUDFs: User Defines Functions\nSpark SQL provides a set of system predefined functions, which can be used in some transformations (e.g., selectExpr(), sort()) but also in the SQL queries. Some examples are\n\nhour(Timestamp)\nabs(Integer)\n…\n\nHowever, users can also define custom functions, which are called User Defined Functions (UDFs).\nUDFs are defined/registered by invoking the udf().register(name, function, datatype) on the SparkSession, where\n\nname is the name of the defined UDF\nfunction is a lambda function used to specify how the parameters of the function are used to generate the returned value\n\nOne of more input parameters are accepted\nOne single returned value is accepted\n\ndatatype is the SQL data type of the returned value\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nDefine a UDFs that, given a string, returns the length of the string.\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Define the UDF\n## name: length\n## output: integer value\nspark.udf.register(\"length\", lambda x: len(x))\nUse of the defined UDF in a selectExpr transformation.\nresult = inputDF.selectExpr(\"length(name) as size\")\nUse of the defined UDF in a SQL query.\nresult = spark.sql(\"SELECT length(name) FROM profiles\")\n\n\n\n\n\nOther notes\n\nData warehouse methods: cube and rollup\nThe method cube(col1, ..., coln) of the DataFrame class can be used to create a multi-dimensional cube for the input DataFrame, on top of which aggregate functions can be computed for each group.\nThe method rollup(col1, ..., coln) of the DataFrame class can be used to create a multi-dimensional rollup for the input DataFrame, on top of which aggregate functions can be computed for each group.\nSpecify which attributes are used to split the input data in groups by using cube(col1, ..., coln) or rollup(col1, ..., coln), respectively, then, apply the aggregate functions to compute for each group of the cube/rollup. The result is a DataFrame. The same aggregate functions/methods already discussed for groupBy can be used also for cube and rollup.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nCreate a DataFrame from the “purchases.csv” file\n\nThe first line contains the header;\nThe others lines contain the quantities of purchased products by users: each line contains userid, productid, quantity.\n\nCreate a first DataFrame containing the result of the cube method. Define one group for each pair userid, productid and compute the sum of quantity in each group; 3.Create a second DataFrame containing the result of the rollup method. Define one group for each pair userid, productid and compute the sum of quantity in each group.\n\nInput file\nuserid,productid,quantity\nu1,p1,10\nu1,p1,20\nu1,p2,20\nu1,p3,10\nu2,p1,20\nu2,p3,40\nu2,p3,30\nExpected output - cube\nuserid,productid,sum(quantity)\nnull    null    150\nnull    p1      50\nnull    p2      20\nnull    p3      80\nu1      null    60\nu1      p1      30\nu1      p2      20\nu1      p3      10\nu2      null    90\nu2      p1      20\nu2      p3      70\nExpected output - rollup\nuserid,productid,sum(quantity)\nnull    null    150\nu1      null    60\nu1      p1      30\nu1      p2      20\nu1      p3      10\nu2      null    90\nu2      p1      20\nu2      p3      70\n## Create a Spark Session object\nspark = SparkSession.builder.getOrCreate()\n\n## Read purchases.csv and store it in a DataFrame\ndfPurchases = spark.read.load(\n    \"purchases.csv\",\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\ndfCube=dfPurchases \\\n    .cube(\"userid\",\"productid\") \\\n    .agg({\"quantity\": \"sum\"})\n\ndfRollup=dfPurchases \\\n    .rollup(\"userid\",\"productid\")\\\n    .agg({\"quantity\": \"sum\"})\n\n\n\n\n\nSet methods\nSimilarly to RDDs also DataFrames can be combined by using set transformations\n\ndf1.union(df2)\ndf1.intersect(df2)\ndf1.subtract(df2)\n\n\n\nBroadcast join\nSpark SQL automatically implements a broadcast version of the join operation if one of the two input DataFrames is small enough to be stored in the main memory of each executor.\nIt is possible to suggest/force it by creating a broadcast version of a DataFrame.\n\n\n\n\n\n\nExample\n\n\n\n\n\ndfPersonLikesBroadcast = dfUidSports\\\n    .join(\n        broadcast(dfPersons),\n        dfPersons.uid == dfUidSports.uid\n    )\n\n\n\n\n\n\n\nbroadcast(dfPersons)\nIn this case we specify that dfPersons must be broadcasted and hence Spark will execute the join operation by using a broadcast join.\n\n\n\n\n\n\n\n\nExecution plan\nThe method explain() can be invoked on a DataFrame to print on the standard output the execution plan of the part of the code that is used to compute the content of the DataFrame on which explain() is invoked."
  },
  {
    "objectID": "18a_spark_mllib.html",
    "href": "18a_spark_mllib.html",
    "title": "19  Spark MLlib",
    "section": "",
    "text": "Spark MLlib is the Spark component providing the machine learning/data mining algorithms\n\nPre-processing techniques\nClassification (supervised learning)\nClustering (unsupervised learning)\nItemset mining\n\nMLlib APIs are divided into two packages:\n\npyspark.mllib: It contains the original APIs built on top of RDDs. This version of the APIs is in maintenance mode and will be probably deprecated in the next releases of Spark.\npyspark.ml: It provides higher-level API built on top of DataFrames (i.e, Dataset<Row>) for constructing ML pipelines. It is recommended because the DataFrame-based API is more versatile and flexible, also providing the pipeline concept. This is the one explained in this course.\n\n\nData types\nSpark MLlib is based on a set of basic local and distributed data types:\n\nLocal vector\nLocal matrix\nDistributed matrix\n…\n\nDataFrames for ML-based applications contain objects based on these basic data types.\n\nLocal vectors\nLocal pyspark.ml.linalg.Vector objects in MLlib are used to store vectors (in dense and sparse representations) of double values. The MLlib algorithms work on vectors of doubles, used to represent the input records/data (one vector for each input record). Non double attributes/values must be mapped to double values before applying MLlib algorithms.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the vector of doubles [1.0, 0.0, 3.0]. It can be represented\n\nIn dense format as [1.0, 0.0, 3.0]\nIn sparse format as (3, [0, 2], [1.0, 3.0]), where\n\n3 is the size of the vector\nThe array [0, 2] contains the indexes of the non-zero cells\nThe array [1.0, 3.0] contains the values of the non-zero cells\n\n\nThe following code shows how dense and sparse vectors can be created in Spark\nfrom pyspark.ml.linalg import Vectors\n\n## Create a dense vector [1.0, 0.0, 3.0]\ndv = Vectors.dense([1.0, 0.0, 3.0])\n\n## Create a sparse vector [1.0, 0.0, 3.0] by specifying\n## its indices and values corresponding to non-zero entries\n## by means of a dictionary\nsv = Vectors.sparse(3, { 0:1.0, 2:3.0 })\nIn the sparse vector\n\n\n\n3\nSize of the vector\n\n\n2:3.0\nIndex and value of a non-empty cell\n\n\n{ 0:1.0, 2:3.0 }\nDictionary of \\(index:value\\) pairs\n\n\n\n\n\n\n\n\nLocal matrices\nLocal pyspark.ml.linalg.Matrix objects in MLlib are used to store matrices (in dense and sparse representations) of double values. The column-major order is used to store the content of the matrix in a linear way.\n\n\nLocal matrices\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe following code shows how dense and sparse matrices can be created in Spark.\nfrom pyspark.ml.linalg import Matrices\n\n## Create a dense matrix with two rows and three columns\n## 3.0 0.0 0.0\n## 1.0 1.5 2.0\ndm =Matrices.dense(2,3,[3.0, 1.0, 0.0, 1.5, 0.0, 2.0])\n\n## Create a sparse version of the same matrix\nsm = Matrices.sparse(2,3, [0, 2, 3, 4], [0, 1, 1, 1] , [3, 1, 1.5, 2])\nIn the dense matrix vector\n\n\n\n2\nNumber of rows\n\n\n3\nNumber of columns\n\n\n[3.0, 1.0, 0.0, 1.5, 0.0, 2.0]\nValues in column/major order\n\n\n\nIn the sparse matrix vector\n\n\n\n\n\n\n\n2\nNumber of rows\n\n\n3\nNumber of columns\n\n\n[0, 2, 3, 4]\nOne element per column that encodes the offset in the array of non-zero values where the values of the given column start. The last element is the number of non-zero values.\n\n\n[0, 1, 1, 1]\nRow index of each non-zero value\n\n\n[3, 1, 1.5, 2]\nArray of non-zero values of the represented matrix\n\n\n\n\n\n\n\n\n\nMain concepts\nSpark MLlib uses DataFrames as input data: the input of the MLlib algorithms are structured data (i.e., tables), and all input data must be represented by means of tables before applying the MLlib algorithms; also document collections must be transformed in a tabular format before applying the MLlib algorithms.\nThe DataFrames used and created by the MLlib algorithms are characterized by several columns, and each column is associated with a different role/meaning\n\nlabel: the target of a classification/regression analysis;\nfeatures: the vector containing the values of the attributes/features of the input record/data points;\ntext: the original text of a document before being transformed in a tabular format;\nprediction: the predicted value of a classification/regression analysis.\n\n\nTransformer\nA Transformer is an ML algorithm/procedure that transforms one DataFrame into another DataFrame by means of the method transform(inputDataFrame).\n\n\n\n\n\n\nExample 1\n\n\n\n\n\nA feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\nA classification model is a Transformer that can be applied on a DataFrame with features and transforms it into a DataFrame with also the prediction column.\n\n\n\n\n\nEstimator\nAn Estimator is an ML algorithm/procedure that is fit on an input (training) DataFrame to produce a Transformer: each Estimator implements a fit() method, which accepts a DataFrame and produces a Model of type Transformer.\nAn Estimator abstracts the concept of a learning algorithm or any algorithm that fits/trains on an input dataset and returns a model\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe Logistic Regression classification algorithm is an Estimator: calling fit(input training DataFrame) on it a Logistic Regression Model is built, which is a Model/a Transformer.\n\n\n\n\n\nPipeline\nA Pipeline chains multiple Transformers and Estimators together to specify a Machine learning/Data Mining workflow. In a pipeline, the output of a transformer/estimator is the input of the next one.\n\n\n\n\n\n\nExample\n\n\n\n\n\nA simple text document processing workflow aiming at building a classification model includes several steps\n\nSplit each document into a set of words;\nConvert each set of words into a numerical feature vector;\nLearn a prediction model using the feature vectors and the associated class labels.\n\n\n\n\n\n\nParameters\nTransformers and Estimators share common APIs for specifying the values of their parameters.\nIn the new APIs of Spark MLlib the use of the pipeline approach is preferred/recommended. This approach is based on the following steps\n\nThe set of Transformers and Estimators that are needed are instantiated;\nA pipeline object is created and the sequence of transformers and estimators associated with the pipeline are specified;\nThe pipeline is executed and a model is trained;\n(optional) The model is applied on new data.\n\n\n\n\nData preprocessing\nInput data must be preprocessed before applying machine learning and data mining algorithms\n\nTo organize data in a format consistent with the one expected by the applied algorithms;\nTo define good (predictive) features;\nTo remove bias (e.g., normalization);\nTo remove noise and missing values.\n\n\nExtracting, transformings, and selecting features\nMLlib provides a set of transformers than can be used to extract, transform and select features from DataFrames\n\nFeature Extractors (e.g., TF-IDF, Word2Vec)\nFeature Transformers (e.g., Tokenizer, StopWordsRemover, StringIndexer, IndexToString, OneHotEncoderEstimator, Normalizer)\nFeature Selectors (e.g., VectorSlicer)\n\nSee the up-to-date list here.\n\n\n\nFeature transformations\nSeveral algorithms are provided by MLlib to transform features. They are used to create new columns/features by combining or transforming other features It is possible to perform feature transformations and feature creations by using the standard methods for DataFrames and RDDs.\n\nVectorAssembler\nVectorAssembler (pyspark.ml.feature.VectorAssembler) is a transformer that combines a given list of columns into a single vector column. It is useful for combining features into a single feature vector before applying ML algorithms.\nGiven VectorAssembler(inputCols, outputCol)\n\ninputCols: the list of original columns to include in the new column of type Vector. The following input column types are accepted\n\nall numeric types, boolean type, and vector type\nBoolean values are mapped to 1 (True) and 0 (False)\n\noutputCol: the name of the new output column\n\nWhen the transform method of VectorAssembler is invoked on a DataFrame the returned DataFrame has a new column (outputCol): for each record, the value of the new column is the concatenation of the values of the input columns. It has also all the columns of the input DataFrame.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider an input DataFrame with three columns: create a new DataFrame with a new column containing the concatenation of “colB” and “colC” in a new vector column. Set the name of the new column to “features”.\nOriginal DataFrame\n\n\n\ncolA\ncolB\ncolC\n\n\n\n\n\\(1\\)\n\\(4.5\\)\nTrue\n\n\n\\(2\\)\n\\(0.6\\)\nTrue\n\n\n\\(3\\)\n\\(1.5\\)\nFalse\n\n\n\\(4\\)\n\\(12.1\\)\nTrue\n\n\n\\(5\\)\n\\(0.0\\)\nTrue\n\n\n\nTransformed DataFrame\n\n\n\ncolA\ncolB\ncolC\nfeatures\n\n\n\n\n\n\\(1\\)\n\\(4.5\\)\nTrue\n\\([4.5,1.0]\\)\n\n\n\n\\(2\\)\n\\(0.6\\)\nTrue\n\\([0.6,1.0]\\)\n\n\n\n\\(3\\)\n\\(1.5\\)\nFalse\n\\([1.5,0.0]\\)\n\n\n\n\\(4\\)\n\\(12.1\\)\nTrue\n\\([12.1,1.0]\\)\n\n\n\n\\(5\\)\n\\(0.0\\)\nTrue\n\\([0.0,1.0]\\)\n\n\n\n\nNotice that columns of DataFrames can also be vectors.\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n## input and output folders\ninputPath = \"data/exampleDataAssembler.csv“\n\n## Create a DataFrame from the input data\ninputDF = spark.read.load(\n    inputPath,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Create a VectorAssembler that combines columns colB and colC\n## The new vetor column is called features\nmyVectorAssembler = VectorAssembler(\n    inputCols = ['colB', 'colC'],\n    outputCol = 'features'\n)\n\n## Apply myVectorAssembler on the input DataFrame\ntransformedDF = myVectorAssembler.transform(inputDF)\n\n\n\n\n\nData Normalization\nMLlib provides a set of normalization algorithms (called scalers)\n\nStandardScaler\nMinMaxScaler\nNormalizer\nMaxAbsScaler\n\n\nStandardScaler\nStandardScaler (pyspark.ml.feature.StandardScaler) is an Estimator that returns a Transformer (pyspark.ml.feature.StandardScalerModel). StandardScalerModel transforms a vector column of an input DataFrame normalizing each feature of the input vector column to have unit standard deviation and/or zero mean.\nGiven StandardScaler(inputCol, outputCol)\n\ninputCol: the name of the input vector column (of doubles) to normalize\noutputCol: the name of the new output normalized vector column\n\nInvoke the fit method of StandardScaler on the input DataFrame to infer a StandardScalerModel. The returned model is a Transformer.\nInvoke the transform method of StandardScalerModel on the input DataFrame to create a new DataFrame that has a new column (outputCol): for each record, the value of the new column is the normalized version of the input vector column. It has also all the columns of the input DataFrame.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider an input DataFrame with four columns: create a new DataFrame with a new column containing the normalized version of the vector column features. Set the name of the new column to “scaledFeatures”.\nOriginal DataFrame\n\n\n\ncolA\ncolB\ncolC\nfeatures\n\n\n\n\n\n\\(1\\)\n\\(4.5\\)\nTrue\n\\([4.5,1.0]\\)\n\n\n\n\\(2\\)\n\\(0.6\\)\nTrue\n\\([0.6,1.0]\\)\n\n\n\n\\(3\\)\n\\(1.5\\)\nFalse\n\\([1.5,0.0]\\)\n\n\n\n\\(4\\)\n\\(12.1\\)\nTrue\n\\([12.1,1.0]\\)\n\n\n\n\\(5\\)\n\\(0.0\\)\nTrue\n\\([0.0,1.0]\\)\n\n\n\n\nTransformed DataFrame\n\n\n\ncolA\ncolB\ncolC\nfeatures\nscaledFeatures\n\n\n\n\n\n\\(1\\)\n\\(4.5\\)\nTrue\n\\([4.5,1.0]\\)\n\\([0.903,2.236]\\)\n\n\n\n\\(2\\)\n\\(0.6\\)\nTrue\n\\([0.6,1.0]\\)\n\\([0.120,2.236]\\)\n\n\n\n\\(3\\)\n\\(1.5\\)\nFalse\n\\([1.5,0.0]\\)\n\\([0.301,0.0]\\)\n\n\n\n\\(4\\)\n\\(12.1\\)\nTrue\n\\([12.1,1.0]\\)\n\\([2.428,2.236]\\)\n\n\n\n\\(5\\)\n\\(0.0\\)\nTrue\n\\([0.0,1.0]\\)\n\\([0.0,2.236]\\)\n\n\n\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\n## input and output folders\ninputPath = \"data/exampleDataAssembler.csv“\n\n## Create a DataFrame from the input data\ninputDF = spark.read.load(\n    inputPath,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Create a VectorAssembler that combines columns colB and colC\n## The new vetor column is called features\nmyVectorAssembler = VectorAssembler(\n    inputCols = ['colB', 'colC'],\n    outputCol = 'features'\n)\n\n## Apply myVectorAssembler on the input DataFrame\ntransformedDF = myVectorAssembler.transform(inputDF)\n\n## Create a Standard Scaler to scale the content of features\nmyScaler = StandardScaler(\n    inputCol=\"features\",\n    outputCol=\"scaledFeatures\"\n)\n\n## Compute summary statistics by fitting the StandardScaler\n## Before normalizing the content of the data we need to compute mean and\n## standard deviation of the analyzed data\nscalerModel = myScaler.fit(transformedDF)\n\n## Apply myScaler on the input column features\nscaledDF = scalerModel.transform(transformedDF)\n\n\n\n\n\n\nCategorical columns\nFrequently the input data are characterized by categorical attributes (i.e., string columns), and the class label of the classification problem is a categorical attribute. The Spark MLlib classification and regression algorithms work only with numerical values, so categorical columns must be mapped to double values.\n\nStringIndexer\nA StringIndexer (pyspark.ml.feature.StringIndexer) is an Estimator that returns a Transformer of type pyspark.ml.feature.StringIndexerModel. StringIndexerModel encodes a string column of “labels” to a column of “label indices”: each distinct value of the input string column is mapped to an integer value in \\([0, \\textbf{number of distinct values})\\).\nStringIndexer(inputCol, outputCol)\n\ninputCol: the name of the input string column to map to a set of integers\noutputCol: the name of the new output column\n\nInvoke the fit method of StringIndexer on the input DataFrame to infer a StringIndexerModel. The returned model is a Transformer.\nInvoke the transform method of StringIndexerModel on the input DataFrame to create a new DataFrame that has a new column (outputCol): for each record, the value of the new column is the integer (casted to a double) associated with the value of the input string column. It has also all the columns of the input DataFrame.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider an input DataFrame with two columns: create a new DataFrame with a new column containing the integer version of the string column category. Set the name of the new column to “categoryIndex”.\nOriginal DataFrame\n\n\n\nid\ncategory\n\n\n\n\n\\(1\\)\na\n\n\n\\(2\\)\nb\n\n\n\\(3\\)\nc\n\n\n\\(4\\)\nc\n\n\n\\(5\\)\na\n\n\n\nTransformed DataFrame\n\n\n\nid\ncategory\ncategoryIndex\n\n\n\n\n\\(1\\)\na\n\\(0.0\\)\n\n\n\\(2\\)\nb\n\\(2.0\\)\n\n\n\\(3\\)\nc\n\\(1.0\\)\n\n\n\\(4\\)\nc\n\\(1.0\\)\n\n\n\\(5\\)\na\n\\(0.0\\)\n\n\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import StringIndexer\n\n## input DataFrame\ndf = spark.createDataFrame(\n    [(1,\"a\"),(2,\"b\"),(3,\"c\"),(4,\"c\"),(5,\"a\")],\n    [\"id\",\"category\"]\n)\n\n## Create a StringIndexer to map the content of category\n##  to a set of \"integers\"\nindexer = StringIndexer(\n    inputCol=\"category\", \n    outputCol=\"categoryIndex\"\n)\n\n## Analyze the input data to define the mapping string -> integer\nindexerModel = indexer.fit(df)\n\n## Apply indexerModel on the input column category\nindexedDF = indexerModel.transform(df)\n\n\n\n\n\nIndexToString\nIndexToString (pyspark.ml.feature.IndexToString), which is symmetrical to StringIndexer, is a Transformer that maps a column of “label indices” back to a column containing the original “labels” as strings. Classification models return the integer version of the predicted label values. To obtain human readable results, remap those values to the original ones.\nGiven IndexToString(inputCol, outputCol, labels)\n\ninputCol: the name of the input numerical column to map to the original a set of string “labels”;\noutputCol: the name of the new output column;\nlabels: the list of original “labels”/strings; the mapping with integer values is given by the positions of the strings inside labels.\n\nInvoke the transform method of IndexToString on the input DataFrame to create a new DataFrame that has a new column (outputCol): for each record, the value of the new column is the original string associated with the value of the input numerical column. It has also all the columns of the input DataFrame.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider an input DataFrame with two columns: create a new DataFrame with a new column containing the integer version of the string column category and then map it back to the string version in a new column.\nOriginal DataFrame\n\n\n\nid\ncategory\n\n\n\n\n\\(1\\)\na\n\n\n\\(2\\)\nb\n\n\n\\(3\\)\nc\n\n\n\\(4\\)\nc\n\n\n\\(5\\)\na\n\n\n\nTransformed DataFrame\n\n\n\nid\ncategory\ncategoryIndex\noriginalCategory\n\n\n\n\n\\(1\\)\na\n\\(0.0\\)\na\n\n\n\\(2\\)\nb\n\\(2.0\\)\nb\n\n\n\\(3\\)\nc\n\\(1.0\\)\nc\n\n\n\\(4\\)\nc\n\\(1.0\\)\nc\n\n\n\\(5\\)\na\n\\(0.0\\)\na\n\n\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import IndexToString\n\n## input DataFrame\ndf = spark.createDataFrame(\n    [(1,\"a\"),(2,\"b\"),(3,\"c\"),(4,\"c\"),(5,\"a\")],\n    [\"id\",\"category\"]\n)\n\n## Create a StringIndexer to map the content of category \n## to a set of integers\nindexer = StringIndexer(\n    inputCol=\"category\", \n    outputCol=\"categoryIndex\"\n)\n\n## Analyze the input data to define the mapping \n## string -> integer\nindexerModel = indexer.fit(df)\n\n## Apply indexerModel on the input column category\nindexedDF = indexerModel.transform(df)\n\n## Create an IndexToString to map the content of numerical \n## attribute categoryIndex to the original string value\nconverter = IndexToString(\n    inputCol=\"categoryIndex\", \n    outputCol=\"originalCategory\",\n    labels=indexerModel.labels\n)\n\n## Apply converter on the input column categoryIndex\nreconvertedDF = converter.transform(indexedDF)\n\n\n\n\n\nSQLTransformer\nSQLTransformer (pyspark.ml.feature.SQLTransformer) is a transformer that implements the transformations which are defined by SQL queries. Currently, the syntax of the supported (simplified) SQL queries is\nSELECT attributes, function(attributes) FROM __THIS__\nWhere __THIS__ represents the DataFrame on which the SQLTransformer is invoked.\nSQLTransformer executes an SQL query on the input DataFrame and returns a new DataFrame associated with the result of the query.\nGiven SQLTransformer(statement)\n\nstatement: the SQL query to execute.\n\nWhen the transform method of SQLTransformer is invoked on a DataFrame the returned DataFrame is the result of the executed statement query.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider an input DataFrame with two columns: “text” and “id”: create a new DataFrame with a new column, called “numWords”, containing the number of words occurring in column “text”.\nOriginal DataFrame\n\n\n\nid\ntext\n\n\n\n\n\\(1\\)\nThis is Spark\n\n\n\\(2\\)\nSpark\n\n\n\\(3\\)\nAnother sample sentence of words\n\n\n\\(4\\)\nPaolo Rossi\n\n\n\\(5\\)\nGiovanni\n\n\n\nTransformed DataFrame\n\n\n\nid\ntext\nnumWords\n\n\n\n\n\\(1\\)\nThis is Spark\n\\(3\\)\n\n\n\\(2\\)\nSpark\n\\(1\\)\n\n\n\\(3\\)\nAnother sample sentence of words\n\\(5\\)\n\n\n\\(4\\)\nPaolo Rossi\n\\(2\\)\n\n\n\\(5\\)\nGiovanni\n\\(1\\)\n\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import SQLTransformer\n\n#Local Input data\ninputList = [\n    (1,\"ThisisSpark\"),\n    (2,\"Spark\"),\n    (3,\"Anothersamplesentenceofwords\"),\n    (4,\"PaoloRossi\"),\n    (5,\"Giovanni\")\n]\n\n## Create the initial DataFrame\ndfInput = spark.createDataFrame(\n    inputList, \n    [\"id\",\"text\"]\n)\n\n## Define a UDF function that that counts the number of words \n## in an input string\nspark.udf.register(\n    \"countWords\", \n    lambda text: len(text.split(\" \")), \n    IntegerType()\n)\n\n## Define an SQLTranformer to create the columns we are \n## interested in\nsqlTrans = SQLTransformer(\n    statement=\"\"\"\n    SELECT *, countWords(text) AS numLines \n    FROM __THIS__\n    \"\"\"\n)\n\n## Create the new DataFrame by invoking the transform method of \n## the defined SQLTranformer\nnewDF = sqlTrans.transform(dfInput)"
  },
  {
    "objectID": "18b_classification.html",
    "href": "18b_classification.html",
    "title": "20  Classification algorithms",
    "section": "",
    "text": "Spark MLlib provides a (limited) set of classification algorithms\n\nLogistic regression\n\nBinomial logistic regression\nMultinomial logistic regression\n\nDecision tree classifier\nRandom forest classifier\nGradient-boosted tree classifier\nMultilayer perceptron classifier\nLinear Support Vector Machine\n\nAll the available classification algorithms are based on two phases:\n\nModel generation based on a set of training data\nPrediction of the class label of new unlabeled data\n\nAll the classification algorithms available in Spark work only on numerical attributes: categorical values must be mapped to integer values (one distinct value per class) before applying the MLlib classification algorithms.\nAll the Spark classification algorithms are trained on top of an input DataFrame containing (at least) two columns\n\nlabel: the class label, (i.e., the attribute to be predicted by the classification model); it is an integer value (casted to a double)\nfeatures: a vector of doubles containing the values of the predictive attributes of the input records/data points; the data type of this column is pyspark.ml.linalg.Vector, and both dense and sparse vectors can be used\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following classification problem: the goal is to predict if new customers are good customers or not based on their monthly income and number of children.\nThe predictive attributes are\n\nMonthly income\nNumber of children\n\nThe class label (target attribute) is “Customer type”:\n\n“Good customer”, mapped to 1\n“Bad customer”, mapped to 0\n\nExample of input training data\nThe training data is the set of customers for which the value of the class label is known: they are used by the classification algorithm to infer/train a classification model.\n\n\n\nCustomerType\nMonthlyIncome\nNumChildren\n\n\n\n\nGood customer\n\\(1400.0\\)\n\\(2\\)\n\n\nBad customer\n\\(11105.5\\)\n\\(0\\)\n\n\nGood customer\n\\(2150.0\\)\n\\(2\\)\n\n\n\nExample of input training DataFrame\nThe input training DataFrame that must be provided as input to train an MLlib classification algorithm must have the following structure\n\n\n\nlabel\nfeatures\n\n\n\n\n\\(1.0\\)\n\\([1400.0,2.0]\\)\n\n\n\\(0.0\\)\n\\([11105.5,0.0]\\)\n\n\n\\(1.0\\)\n\\([2150.0,2.0]\\)\n\n\n\nNotice that\n\nThe categorical values of “CustomerType” (the class label column) must be mapped to integer data values (then casted to doubles).\nThe values of the predictive attributes are stored in vectors of doubles. One single vector for each input record.\nIn the generated DataFrame the names of the predictive attributes are not preserved.\n\n\n\n\n\nStructured data classification\n\nExample of logistic regression and structured data\nThe following paragraphs show how to\n\nCreate a classification model based on the logistic regression algorithm on structured data: the model is inferred by analyzing the training data, (i.e., the example records/data points for which the value of the class label is known).\nApply the model to new unlabeled data: the inferred model is applied to predict the value of the class label of new unlabeled records/data points.\n\n\nTraining data\nThe input training data is stored in a text file that contains one record/data point per line. The records/data points are structured data with a fixed number of attributes (four)\n\nOne attribute is the class label: it assumed that the first column of each record contains the class label;\nThe other three attributes are the predictive attributes that are used to predict the value of the class label;\n\nThe values are already doubles (no need to convert them), and the input file has the header line.\nConsider the following example input training data file\nlabel,attr1,attr2,attr3\n1.0,0.0,1.1,0.1\n0.0,2.0,1.0,-1.0\n0.0,2.0,1.3,1.0\n1.0,0.0,1.2,-0.5\nIt contains four records/data points. This is a binary classification problem because the class label assumes only two values: 0 and 1.\nThe first operation consists in transforming the content of the input training file into a DataFrame containing two columns\n\nlabel: the double value that is used to specify the label of each training record;\nfeatures: it is a vector of doubles associated with the values of the predictive features.\n\n\n\n\nlabel\nfeatures\n\n\n\n\n\\(1.0\\)\n\\([0.0,1.1,0.1]\\)\n\n\n\\(0.0\\)\n\\([2.0,1.0,-1.0]\\)\n\n\n\\(0.0\\)\n\\([2.0,1.3,1.0]\\)\n\n\n\\(1.0\\)\n\\([0.0,1.2,-0.5]\\)\n\n\n\n\nData type of “label” is double\nData type of “features” is pyspark.ml.linalg.Vector\n\n\n\nUnlabeled data\nThe file containing the unlabeled data has the same format of the training data file, however the first column is empty because the class label is unknown. The goal is to predict the class label value of each unlabeled data by applying the classification model that has been trained on the training data: the predicted class label value of the unlabeled data is stored in a new column, called “prediction”, of the returned DataFrame.\nConsider the following input unlabeled data file\nlabel,attr1,attr2,attr3\n,-1.0,1.5,1.3\n,3.0,2.0,-0.1\n,0.0,2.2,-1.5\nIt contains three unlabeled records/data points. Notice that the first column is empty (the content before the first comma is the empty string).\nAlso the unlabeled data must be stored into a DataFrame containing two columns: “label” and “features”. So, “label” column is required also for unlabeled data, but its value is set to null for all records.\n\n\n\nlabel\nfeatures\n\n\n\n\nnull\n\\([-1.0,1.5,1.3]\\)\n\n\nnull\n\\([3.0,2.0,-0.1]\\)\n\n\nnull\n\\([0.0,2.2,-1.5]\\)\n\n\n\n\n\nPrediction column\nAfter the application of the classification model on the unlabeled data, Spark returns a new DataFrame containing\n\nThe same columns of the input DataFrame\nA new column called prediction, that, for each input unlabeled record, contains the predicted class label value\nTwo columns, associated with the probabilities of the predictions (these columns are not considered in the example)\n\n\n\n\nlabel\nfeatures\nprediction\nrawPrediction\nprobability\n\n\n\n\nnull\n\\([-1.0,1.5,1.3]\\)\n\\(1.0\\)\n…\n…\n\n\nnull\n\\([3.0,2.0,-0.1]\\)\n\\(0.0\\)\n…\n…\n\n\nnull\n\\([0.0,2.2,-1.5]\\)\n\\(1.0\\)\n…\n…\n\n\n\nThe “prediction” column contains the predicted class label values.\n\n\nExample code\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\n\n## input and output folders\ntrainingData = \"ex_data/trainingData.csv\"\nunlabeledData = \"ex_data/unlabeledData.csv\"\noutputPath = \"predictionsLR/\"\n\n## *************************\n## Training step\n## *************************\n\n## Create a DataFrame from trainingData.csv\n## Training data in raw format\ntrainingData = spark.read.load(\n    trainingData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\",\"attr2\",\"attr3\"],\n    outputCol=\"features\"\n)\n\n## Apply the assembler to create column features for the training data\ntrainingDataDF = assembler.transform(trainingData)\n\n## Create a LogisticRegression object.\n## LogisticRegression is an Estimator that is used to\n## create a classification model based on logistic regression.\nlr = LogisticRegression()\n\n## It is possible to set the values of the parameters of the\n## Logistic Regression algorithm using the setter methods.\n## There is one set method for each parameter\n## For example, the number of maximum iterations is set to 10\n## and the regularization parameter is set to 0.01\nlr.setMaxIter(10)\nlr.setRegParam(0.01)\n\n## Train a logistic regression model on the training data\nclassificationModel = lr.fit(trainingDataDF)\n\n## *************************\n## Prediction step\n## *************************\n\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(\n    unlabeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Apply the same assembler we created before also on the unlabeled data\n## to create the features column\nunlabeledDataDF = assembler.transform(unlabeledData)\n\n## Make predictions on the unlabled data using the transform() method of the\n## trained classification model transform uses only the content of 'features'\n## to perform the predictions\npredictionsDF = classificationModel.transform(unlabeledDataDF)\n\n## The returned DataFrame has the following schema (attributes)\n## - attr1\n## - attr2\n## - attr3\n## - features: vector (values of the attributes)\n## - label: double (value of the class label)\n## - rawPrediction: vector (nullable = true)\n## - probability: vector (The i-th cell contains the probability that \n## the current record belongs to the i-th class\n## - prediction: double (the predicted class label)\n\n## Select only the original features (i.e., the value of the original attributes\n## attr1, attr2, attr3) and the predicted class for each record\npredictions = predictionsDF.select(\"attr1\", \"attr2\", \"attr3\", \"prediction\")\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\n\n\nPipelines\nIn the previous solution the same preprocessing steps were applied on both training and unlabeled data (the same assembler on both input data). It is possible to use a pipeline to specify the common phases we apply on both input data sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\ntrainingData = \"ex_data/trainingData.csv\"\nunlabeledData = \"ex_data/unlabeledData.csv\"\noutputPath = \"predictionsLR/\"\n\n## *************************\n## Training step\n## *************************\n## Create a DataFrame from trainingData.csv\n## Training data in raw format\ntrainingData = spark.read.load(\n    trainingData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\",\"attr2\",\"attr3\"],\n    outputCol=\"features\"\n)\n\n## Create a LogisticRegression object\n## LogisticRegression is an Estimator that is used to\n## create a classification model based on logistic regression.\nlr = LogisticRegression()\n\n## Set the values of the parameters of the\n## Logistic Regression algorithm using the setter methods.\n## There is one set method for each parameter\n## For example, we are setting the number of maximum iterations to 10\n## and the regularization parameter to 0.01\nlr.setMaxIter(10)\nlr.setRegParam(0.01)\n\n## Define a pipeline that is used to create the logistic regression\n## model on the training data. The pipeline includes also\n## the preprocessing step\n#pipeline = Pipeline().setStages([assembler, lr]) # <1>\n\n## Execute the pipeline on the training data to build the\n## classification model\nclassificationModel = pipeline.fit(trainingData)\n\n## Now, the classification model can be used to predict the class label\n## of new unlabeled data\n\n## *************************\n## Prediction step\n## *************************\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(\n    unlabeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Make predictions on the unlabled data using the transform() \n## method of the trained classification model transform uses only \n## the content of 'features' to perform the predictions. The model \n## is associated with the pipeline and hence also the assembler is executed\npredictions = classificationModel.transform(unlabeledData)\n\n## The returned DataFrame has the following schema (attributes)\n## - attr1\n## - attr2\n## - attr3\n## - features: vector (values of the attributes)\n## - label: double (value of the class label)\n## - rawPrediction: vector (nullable = true)\n## - probability: vector (The i-th cell contains the probability that the current\n## record belongs to the i-th class\n## - prediction: double (the predicted class label)\n## Select only the original features (i.e., the value of the original attributes\n## attr1, attr2, attr3) and the predicted class for each record\npredictions = predictionsDF.select(\"attr1\",\"attr2\",\"attr3\",\"prediction\")\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\nassembler: the sequence of transformers and estimators to apply on the input data\n\n\n\n\n\n\nDecision trees and structured data\nThe following paragraphs show how to\n\nCreate a classification model based on the decision tree algorithm on structured data: the model is inferred by analyzing the training data, i.e., the example records/data points for which the value of the class label is known;\nApply the model to new unlabeled data: the inferred model is applied to predict the value of the class label of new unlabeled records/data points.\n\nThe same example structured data already used in the running example related to the logistic regression algorithm are used also in this example related to the decision tree algorithm. The main steps are the same of the previous example, the only difference is the definition and configuration of the used classification algorithm.\n\n\n\n\n\n\nExample\n\n\n\n\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\ntrainingData = \"ex_data/trainingData.csv\"\nunlabeledData = \"ex_data/unlabeledData.csv\"\noutputPath = \"predictionsLR/\"\n\n## *************************\n## Training step\n## *************************\n## Create a DataFrame from trainingData.csv\n## Training data in raw format\ntrainingData = spark.read.load(\n    trainingData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\",\"attr2\",\"attr3\"],\n    outputCol=\"features\"\n)\n\n## Create a DecisionTreeClassifier object.\n## DecisionTreeClassifier is an Estimator that is used to\n## create a classification model based on decision trees.\ndt = DecisionTreeClassifier()\n\n## We can set the values of the parameters of the Decision Tree\n## For example we can set the measure that is used to decide if a\n## node must be split. In this case we set gini index\ndt.setImpurity(\"gini\")\n\n## Define a pipeline that is used to create the decision tree\n## model on the training data. The pipeline includes also\n## the preprocessing step\n#pipeline = Pipeline().setStages([assembler, dt]) # <1>\n\n## Execute the pipeline on the training data to build the\n## classification model\nclassificationModel = pipeline.fit(trainingData)\n\n## Now, the classification model can be used to predict the class label\n## of new unlabeled data\n\n## *************************\n## Prediction step\n## *************************\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(unlabeledData,\\\nformat=\"csv\", header=True, inferSchema=True)\n\n## Make predictions on the unlabled data using the transform() method of the\n## trained classification model transform uses only the content of 'features'\n## to perform the predictions. The model is associated with the pipeline and hence\n## also the assembler is executed\npredictions = classificationModel.transform(unlabeledData)\n\n## The returned DataFrame has the following schema (attributes)\n## - attr1\n## - attr2\n## - attr3\n## - features: vector (values of the attributes)\n## - label: double (value of the class label)\n## - rawPrediction: vector (nullable = true)\n## - probability: vector (The i-th cell contains the probability that the current\n## record belongs to the i-th class\n## - prediction: double (the predicted class label)\n## Select only the original features (i.e., the value of the original attributes\n## attr1, attr2, attr3) and the predicted class for each record\npredictions = predictionsDF.select(\"attr1\",\"attr2\",\"attr3\",\"prediction\")\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\nassembler: the sequence of transformers and estimators to apply on the input data. A decision tree algorithm is used in this case.\n\n\n\n\n\n\n\nCategorical class labels\nUsually the class label is a categorical value (i.e., a string). However, as reported before, Spark MLlib works only with numerical values and hence categorical class label values must be mapped to integer (and then double) values: processing and postprocessing steps are used to manage this transformation.\nConsider the following input training data\n\n\n\ncategoricalLabel\nAttr1\nAttr2\nAttr3\n\n\n\n\nPositive\n\\(0.0\\)\n\\(1.1\\)\n\\(0.1\\)\n\n\nNegative\n\\(2.0\\)\n\\(1.0\\)\n\\(-1.0\\)\n\n\nNegative\n\\(2.0\\)\n\\(1.3\\)\n\\(1.0\\)\n\n\n\nA modified input DataFrame must be generated as input for the MLlib classification algorithms\n\n\n\nlabel\nfeatures\n\n\n\n\n\\(1.0\\)\n\\([0.0,1.1,0.1]\\)\n\n\n\\(1.0\\)\n\\([2.0,1.0,-1.0]\\)\n\n\n\\(0.0\\)\n\\([2.0,1.3,1.0]\\)\n\n\n\nNotice that the categorical values of “categoricalLabel” (the class label column) must mapped to integer data values (finally casted to doubles).\n\nStringIndexer and IndexToString\nThe Estimator StringIndexer and the Transformer IndexToString support the transformation of categorical class label into numerical one and vice versa:\n\nStringIndexer maps each categorical value of the class label to an integer (then casted to a double);\nIndexToString is used to perform the opposite operation.\n\nAll in all, these are the main steps\n\nUse StringIndexer to extend the input DataFrame with a new column, called “label”, containing the numerical representation of the class label column;\nCreate a column, called “features”, of type vector containing the predictive features;\nInfer a classification model by using a classification algorithm (e.g., Decision Tree, Logistic regression);\nApply the model on a set of unlabeled data to predict their numerical class label;\nUse IndexToString to convert the predicted numerical class label values to the original categorical values.\n\nNotice that the model is built by considering only the values of features and label. All the other columns are not considered by the classification algorithm during the generation of the prediction model.\n\nTraining data\nGiven the following input training file\ncategoricalLabel,attr1,attr2,attr3\nPositive,0.0,1.1,0.1\nNegative,2.0,1.0,-1.0\nNegative,2.0,1.3,1.0\nThe initial training DataFrame will be\n\n\n\ncategoricalLabel\nfeatures\n\n\n\n\nPositive\n\\([0.0,1.1,0.1]\\)\n\n\nNegative\n\\([2.0,1.0,-1.0]\\)\n\n\nNegative\n\\([2.0,1.3,1.0]\\)\n\n\n\n\nThe type of “categoricalLabel” is String\nThe type of “features” is Vector\n\nAfter applying StringIndexer, the training DataFrame will be\n\n\n\ncategoricalLabel\nfeatures\nlabel!\n\n\n\n\nPositive\n\\([0.0,1.1,0.1]\\)\n\\(1.0\\)\n\n\nNegative\n\\([2.0,1.0,-1.0]\\)\n\\(0.0\\)\n\n\nNegative\n\\([2.0,1.3,1.0]\\)\n\\(0.0\\)\n\n\n\n“label” contains the mapping generated by StringIndexer:\n\n“Positive”: \\(1.0\\)\n“Negative”: \\(0.0\\)\n\n\n\nUnalabeled data\nGiven the input unlabeled data file\ncategoricalLabel,attr1,attr2,attr3\n,-1.0,1.5,1.3\n,3.0,2.0,-0.1\n,0.0,2.2,-1.5\nThe initial unlabeled DataFrame will be\n\n\n\ncategoricalLabel\nfeatures\n\n\n\n\nnull\n\\([-1.0,1.5,1.3]\\)\n\n\nnull\n\\([3.0,2.0,-0.1]\\)\n\n\nnull\n\\([0.0,2.2,-1.5]\\)\n\n\n\nAfter performing the prediction, and applying IndexToString, the output DataFrame will be\n\n\n\ncategoricalLabel\nfeatures\nlabel\nprediction\npredictedLabel\n…\n\n\n\n\n…\n\\([-1.0,1.5,1.3]\\)\n…\n\\(1.0\\)\nPositive\n\n\n\n…\n\\([3.0,2.0,-0.1]\\)\n…\n\\(0.0\\)\nNegative\n\n\n\n…\n\\([0.0,2.2,-1.5]\\)\n…\n\\(1.0\\)\nNegative\n\n\n\n\n\n“prediction” contains the predicted label, expressed as a number\n“predictedLabel” contains the predicted label, expressed as a category (original name)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nIn this example, the input training data is stored in a text file that contains one record/data point per line, and the records/data points are structured data with a fixed number of attributes (four)\n\nOne attribute is the class label (“categoricalLabel”): this is a categorical attribute that can assume two values, “Positive” or “Negative”;\nThe other three attributes (“attr1”, “attr2”, “attr3”) are the predictive attributes that are used to predict the value of the class label.\n\nThe input file has the header line.\nThe file containing the unlabeled data has the same format of the training data file, however the first column is empty because the class label is unknown.\nThe goal is to predict the class label value of each unlabeled data by applying the classification model that has been inferred on the training data.\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import IndexToString\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\ntrainingData = \"ex_dataCategorical/trainingData.csv\"\nunlabeledData = \"ex_dataCategorical/unlabeledData.csv\"\noutputPath = \"predictionsDTCategoricalPipeline/\"\n\n## *************************\n## Training step\n## *************************\n\n## Create a DataFrame from trainingData.csv\n## Training data in raw format\ntrainingData = spark.read.load(trainingData,\\\nformat=\"csv\", header=True, inferSchema=True)\n\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\",\"attr2\",\"attr3\"],\n    outputCol=\"features\"\n)\n\n## The StringIndexer Estimator is used to map each class label\n## value to an integer value (casted to a double).\n## A new attribute called label is generated by applying\n## transforming the content of the categoricalLabel attribute.\n#labelIndexer = StringIndexer( # <1>\n    inputCol=\"categoricalLabel\"\n    outputCol=\"label\",\n    handleInvalid=\"keep\"\n).fit(trainingData) \n\n## Create a DecisionTreeClassifier object.\n## DecisionTreeClassifier is an Estimator that is used to\n## create a classification model based on decision trees.\ndt = DecisionTreeClassifier()\n\n## Set the values of the parameters of the Decision Tree\n## For example set the measure that is used to decide if a\n## node must be split.\n## In this case we set gini index\ndt.setImpurity(\"gini\")\n\n## At the end of the pipeline we must convert indexed labels back\n## to original labels (from numerical to string).\n## The content of the prediction attribute is the index of the predicted class\n## The original name of the predicted class is stored in the predictedLabel\n## attribute.\n## IndexToString creates a new column (called predictedLabel in\n## this example) that is based on the content of the prediction column.\n## prediction is a double while predictedLabel is a string\n#labelConverter = IndexToString( # <2>\n    inputCol=\"prediction\",\n    outputCol=\"predictedLabel\",\n    labels=labelIndexer.labels\n)\n\n## Define a pipeline that is used to create the decision tree\n## model on the training data. The pipeline includes also\n## the preprocessing and postprocessing steps\npipeline = Pipeline() \\ \n    #.setStages([assembler, labelIndexer, dt, labelConverter]) # <3>\n\n## Execute the pipeline on the training data to build the\n## classification model\nclassificationModel = pipeline.fit(trainingData)\n\n## Now, the classification model can be used to predict the class label\n## of new unlabeled data\n\n## *************************\n## Prediction step\n## *************************\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(\n    unlabeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Make predictions on the unlabled data using the transform() method of the\n## trained classification model transform uses only the content of 'features'\n## to perform the predictions. The model is associated with the pipeline and hence\n## also the assembler is executed\npredictions = classificationModel.transform(unlabeledData)\n\n## The returned DataFrame has the following schema (attributes)\n## - attr1: double (nullable = true)\n## - attr2: double (nullable = true)\n## - attr3: double (nullable = true)\n## - features: vector (values of the attributes)\n## - label: double (value of the class label)\n## - rawPrediction: vector (nullable = true)\n## - probability: vector (The i-th cell contains the probability that the\n##   current record belongs to the i-th class\n## - prediction: double (the predicted class label)\n## - predictedLabel: string (nullable = true)\n\n## Select only the original features (i.e., the value of the original attributes\n## attr1, attr2, attr3) and the predicted class for each record\npredictions = predictionsDF \\\n    #.select(\"attr1\", \"attr2\", \"attr3\", \"predictedLabel\") # <4>\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\nThis StringIndexer estimator is used to infer a transformer that maps the categorical values of column “categoricalLabel” to a set of integer values stored in the new column called “label”. The list of valid label values are extracted from the training data.\nThis IndexToString component is used to remap the numerical predictions available in the “prediction” column to the original categorical values that are stored in the new column called “predictedLabel”. The mapping of integer to original string value is the one of “labelIndexer”.\nThis Pipeline is composed of four steps.\nThe “predictedLabel” field is the column containing the predicted categorical class label for the unlabeled data.\n\n\n\n\n\n\n\n\nTextual data management and classification\nThe following paragraphs show how to\n\nCreate a classification model based on the logistic regression algorithm for textual documents: a set of specific preprocessing estimators and transformers are used to preprocess textual data.\nApply the model to new textual documents\n\nThe input training dataset represents a textual document collection, where each line contains one document and its class\n\nThe class label\nA list of words (the text of the document)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the following example training file\nLabel,Text\n1,The Spark system is based on scala\n1,Spark is a new distributed system\n0,Turin is a beautiful city\n0,Turin is in the north of Italy\nIt contains four textual documents, and each line contains two attributes, that are the class label (first attribute) and the text of the document (second attribute).\nThe input data before preprocessing, represented as a DataFrame, is\n\n\n\nLabel\nText\n\n\n\n\n1\nThe Spark system is based on scala\n\n\n1\nSpark is a new distributed system\n\n\n0\nTurin is a beautiful city\n\n\n0\nTurin is in the north of Italy\n\n\n\n\n\n\nA set of preprocessing steps must be applied on the textual attribute before generating a classification model.\n\nSince Spark ML algorithms work only on “Tables” and double values, the textual part of the input data must be translated in a set of attributes to represent the data as a table: usually a table with an attribute for each word is generated.\nMany words are useless (e.g., conjunctions): stopwords are usually removed. In general,\n\nthe words appearing in almost all documents are not characterizing the data, and so they are not very important for the classification problem;\nthe words appearing in few documents allow to distinguish the content of those documents (and hence the class label) with respect to the others, and so they are very important for the classification problem.\n\nTraditionally a weight, based on the TF-IDF measure, is used to assign a difference importance to the words based on their frequency in the collection.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nInput data after the preprocessing transformations (tokenization, stopword removal, TF-IDF computation)\n\n\n\nLabel\nSpark\nsystem\nscala\n…\n\n\n\n\n1\n\\(0.5\\)\n\\(0.3\\)\n\\(0.75\\)\n…\n\n\n1\n\\(0.5\\)\n\\(0.3\\)\n\\(0\\)\n…\n\n\n0\n\\(0\\)\n\\(0\\)\n\\(0\\)\n…\n\n\n0\n\\(0\\)\n\\(0\\)\n\\(0\\)\n…\n\n\n\n\n\n\nThe DataFrame associated with the input data after the preprocessing transformations must contain, as usual, the columns\n\nlabel: class label value\nfeatures: the preprocessed version of the input text\n\nThere are also some other intermediate columns, related to applied transformations, but they are not considered by the classification algorithm.\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe DataFrame associated with the input data after the preprocessing transformations\n\n\n\nlabel\nfeatures\ntext\n…\n…\n\n\n\n\n1\n\\([0.5,0.3,0.75,...]\\)\nThe Spark system is based on scala\n…\n…\n\n\n1\n\\([0.5,0.3,0,...]\\)\nSpark is a new distributed system\n…\n…\n\n\n0\n\\([0,0,0,...]\\)\nTurin is a beautiful city\n…\n…\n\n\n0\n\\([0,0,0,...]\\)\nTurin is in the north of Italy\n…\n…\n\n\n\nOnly “label” and “features” are considered by the classification algorithm.\n\n\n\nIn the following solution we will use a set of new Transformers to prepare input data\n\nTokenizer: to split the input text in words;\nStopWordsRemover: to remove stopwords;\nHashingTF: to compute the (approximate) term frequency of each input term;\nIDF: to compute the inverse document frequency of each input word.\n\nThe input data (training and unlabeled data) are stored in input csv files. Each line contains two attributes:\n\nThe class label (label)\nThe text of the document (text)\n\nWe infer a linear regression model on the training data and apply the model on the unlabeled data.\n\n\n\n\n\n\nExample\n\n\n\n\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Tokenizer\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.feature import HashingTF\nfrom pyspark.ml.feature import IDF\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\ntrainingData = \"ex_dataText/trainingData.csv\"\nunlabeledData = \"ex_dataText/unlabeledData.csv\"\noutputPath = \"predictionsLRPipelineText/\"\n\n## *************************\n## Training step\n## *************************\n## Create a DataFrame from trainingData.csv\n## Training data in raw format\ntrainingData = spark.read.load(\n    trainingData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Configure an ML pipeline, which consists of five stages:\n## tokenizer -> split sentences in set of words\n## remover -> remove stopwords\n## hashingTF -> map set of words to a fixed-length feature vectors (each\n## word becomes a feature and the value of the feature is the frequency of\n## the word in the sentence)\n## idf -> compute the idf component of the TF-IDF measure\n## lr -> logistic regression classification algorithm\n## The Tokenizer splits each sentence in a set of words.\n## It analyzes the content of column \"text\" and adds the\n## new column \"words\" in the returned DataFrame\ntokenizer = Tokenizer() \\\n    .setInputCol(\"text\") \\\n    .setOutputCol(\"words\")\n\n## Remove stopwords.\n## The StopWordsRemover component returns a new DataFrame with\n## a new column called \"filteredWords\". \"filteredWords\" is generated\n## by removing the stopwords from the content of column \"words\"\nremover = StopWordsRemover() \\\n    .setInputCol(\"words\") \\\n    .setOutputCol(\"filteredWords\")\n\n## Map words to a features\n## Each word in filteredWords must become a feature in a Vector object\n## The HashingTF Transformer can be used to perform this operation.\n## This operations is based on a hash function and can potentially\n## map two different words to the same \"feature\". The number of conflicts\n## in influenced by the value of the numFeatures parameter.\n## The \"feature\" version of the words is stored in Column \"rawFeatures\".\n## Each feature, for a document, contains the number of occurrences\n## of that feature in the document (TF component of the TF-IDF measure)\nhashingTF = HashingTF() \\\n    .setNumFeatures(1000) \\\n    .setInputCol(\"filteredWords\") \\\n    .setOutputCol(\"rawFeatures\")\n\n## Apply the IDF transformation/computation.\n## Update the weight associated with each feature by considering also the\n## inverse document frequency component. The returned new column\n## is called \"features\", that is the standard name for the column that\n## contains the predictive features used to create a classification model\nidf = IDF() \\\n    .setInputCol(\"rawFeatures\") \\\n    .setOutputCol(\"features\")\n\n## Create a classification model based on the logistic regression algorithm\n## We can set the values of the parameters of the\n## Logistic Regression algorithm using the setter methods.\nlr = LogisticRegression() \\\n    .setMaxIter(10) \\\n    .setRegParam(0.01)\n\n## Define the pipeline that is used to create the logistic regression\n## model on the training data.\n## In this case the pipeline is composed of five steps\n## - text tokenizer\n## - stopword removal\n## - TF-IDF computation (performed in two steps)\n## - Logistic regression model generation\npipeline = Pipeline()\\\n    .setStages([tokenizer, remover, hashingTF, idf, lr])\n\n## Execute the pipeline on the training data to build the\n## classification model\nclassificationModel = pipeline.fit(trainingData)\n## Now, the classification model can be used to predict the class label\n## of new unlabeled data\n\n## *************************\n## Prediction step\n## *************************\n## Read unlabeled data\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(\n    unlabeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Make predictions on unlabeled documents by using the\n## Transformer.transform() method.\n## The transform will only use the 'features' columns\npredictionsDF = classificationModel.transform(unlabeledData)\n\n## The returned DataFrame has the following schema (attributes)\n## |-- label: string (nullable = true)\n## |-- text: string (nullable = true)\n## |-- words: array (nullable = true)\n## | |-- element: string (containsNull = true)\n## |-- filteredWords: array (nullable = true)\n## | |-- element: string (containsNull = true)\n## |-- rawFeatures: vector (nullable = true)\n## |-- features: vector (nullable = true)\n## |-- rawPrediction: vector (nullable = true)\n## |-- probability: vector (nullable = true)\n## |-- prediction: double (nullable = false)\n\n## Select only the original features (i.e., the value of the original text attribute) and\n## the predicted class for each record\npredictions = predictionsDF.select(\"text\", \"prediction\")\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\n\n\n\n\nPerformance evaluation\nIn order to test the goodness of algorithms there are some evaluators. The Evaluator can be\n\na BinaryClassificationEvaluator for binary data\na MulticlassClassificationEvaluator for multiclass problems\n\nProvided metrics are:\n\nAccuracy\nPrecision\nRecall\nF-measure\n\nUse the MulticlassClassificationEvaluator estimator from pyspark.ml.evaluator on a DataFrame. The instantiated estimator has the method .evaluate() that is applied on a DataFrame: it compares the predictions with the true label values, and the output is the double value of the computed performance metric.\nThe parameters of MulticlassClassificationEvaluator are\n\nmetricName: type of metric to compute. It can assume the following values\n\n\"accuracy\"\n\"f1\"\n\"weightedPrecision\"\n\"weightedRecall\"\n\nlabelCol: input column with the true label/class value\npredictionCol: input column with the predicted class/label value\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nIn this example, the set of labeled data is read from a text file that contains one record/data point per line, and the records/data points are structured data with a fixed number of attributes (four)\n\nOne attribute is the class label (“label”);\nThe other three attributes (“attr1”, “attr2”, “attr3”) are the predictive attributes that are used to predict the value of the class label.\n\nAll attributes are already double attributes, and the input file has the header line.\nConsider the following example input labeled data file\nlabel,attr1,attr2,attr3\n1,0.0,1.1,0.1\n0,2.0,1.0,-1.0\n0,2.0,1.3,1.0\n1,0.0,1.2,-0.5\nFollow these steps\n\nSplit the labeled data set in two subsets\n\nTraining set: \\(75\\%\\) of the labeled data\nTest set: \\(25\\%\\) of the labeled data\n\nInfer/train a logistic regression model on the training set\nEvaluate the prediction quality of the inferred model on both the test set and the training set\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\nlabeledData = \"ex_dataValidation/labeledData.csv\"\noutputPath = \"predictionsLRPipelineValidation/\"\n\n## Create a DataFrame from labeledData.csv\n## Training data in raw format\nlabeledDataDF = spark.read.load(\n    labeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Split labeled data in training and test set\n## training data : 75%\n## test data: 25%\n#trainDF, testDF = labeledDataDF.randomSplit([0.75, 0.25], seed=10) # <1>\n\n## *************************\n## Training step\n## *************************\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\", \"attr2\", \"attr3\"],\n    outputCol=\"features\"\n)\n\n## Create a LogisticRegression object.\n## LogisticRegression is an Estimator that is used to\n## create a classification model based on logistic regression.\nlr = LogisticRegression()\n\n## Set the values of the parameters of the\n## Logistic Regression algorithm using the setter methods.\n## There is one set method for each parameter\n## For example, we are setting the number of maximum iterations to 10\n## and the regularization parameter to 0.01\nlr.setMaxIter(10)\nlr.setRegParam(0.01)\n\n## Define a pipeline that is used to create the logistic regression\n## model on the training data. The pipeline includes also\n## the preprocessing step\npipeline = Pipeline().setStages([assembler, lr])\n\n## Execute the pipeline on the training data to build the\n## classification model\nclassificationModel = pipeline.fit(trainDF)\n## Now, the classification model can be used to predict the class label\n## of new unlabeled data\n\n## Make predictions on the test data using the transform() method of the\n## trained classification model transform uses only the content of 'features'\n## to perform the predictions. The model is associated with the pipeline and hence\n## also the assembler is executed\npredictionsDF = classificationModel.transform(testDF)\n\n## The predicted value is column prediction\n## The actual label is column label\n## Define a set of evaluators\nmyEvaluatorAcc = MulticlassClassificationEvaluator(\n    labelCol=\"label\",\n    predictionCol=\"prediction\",\n    metricName='accuracy'\n)\n\nmyEvaluatorF1 = MulticlassClassificationEvaluator(\n    labelCol=\"label\",\n    predictionCol=\"prediction\",\n    metricName='f1'\n)\n\nmyEvaluatorWeightedPrecision = MulticlassClassificationEvaluator(\n    labelCol=\"label\",\n    predictionCol=\"prediction\",\n    metricName='weightedPrecision'\n)\n\nmyEvaluatorWeightedRecall = MulticlassClassificationEvaluator(\n    labelCol=\"label\",\n    predictionCol=\"prediction\",\n    metricName='weightedRecall'\n)\n\n## Apply the evaluators on the predictions associated with the test data\n## Print the results on the standard output\nprint(\n    \"Accuracy on test data \", \n    myEvaluatorAcc.evaluate(predictionsDF)\n)\n\nprint(\n    \"F1 on test data \", \n    myEvaluatorF1.evaluate(predictionsDF)\n)\n\nprint(\n    \"Weighted recall on test data \",\n    myEvaluatorWeightedRecall.evaluate(predictionsDF)\n)\n\nprint(\n    \"Weighted precision on test data \",\n    myEvaluatorWeightedPrecision.evaluate(predictionsDF)\n)\n\n## Compute the prediction quality also for the training data.\n## To check if the model is overfitted on the training data\n\n## Make predictions on the training data using the transform() method of the\n## trained classification model transform uses only the content of 'features'\n## to perform the predictions. The model is associated with the pipeline and hence\n## also the assembler is executed\npredictionsTrainingDF = classificationModel.transform(trainDF)\n\n## Apply the evaluators on the predictions associated with the test data\n## Print the results on the standard output\n\nprint(\n    \"Accuracy on training data \",\n    myEvaluatorAcc.evaluate(predictionsTrainingDF)\n)\n\nprint(\n    \"F1 on training data \",\n    myEvaluatorF1.evaluate(predictionsTrainingDF)\n)\n\nprint(\n    \"Weighted recall on training data \",\n    myEvaluatorWeightedRecall.evaluate(predictionsTrainingDF)\n)\n\nprint(\n    \"Weighted precision on training data \",\n    myEvaluatorWeightedPrecision.evaluate(predictionsTrainingDF)\n)\n\nrandomSplit can be used to split the content of an input DataFrame in subsets\n\n\n\n\n\n\nHyperparameter tuning\nThe setting of the parameters of an algorithm is always a difficult task. A brute force approach can be used to find the setting optimizing a quality index, by splitting the training data in two subsets:\n\nThe first set is used to build a model\nThe second one is used to evaluate the quality of the model\n\nThe setting that maximizes a quality index (e.g., the prediction accuracy) is used to build the final model on the whole training dataset.\nUsing one single split of the training set usually leads to biased results, so the cross-validation approach is normally used\n\nCreate \\(k\\) splits and \\(k\\) models\nThe parameter setting that achieves, on the average, the best result on the \\(k\\) models is selected as final setting of the algorithm parameters\n\nSpark supports a brute-force grid-based approach to evaluate a set of possible parameter settings on a pipeline\n\nInput\n\nAn MLlib pipeline\nA set of values to be evaluated for each input parameter of the pipeline: all the possible combinations of the specified parameter values are considered and the related models are automatically generated and evaluated by Spark\nA quality evaluation metric to evaluate the result of the input pipeline\n\nOutput: the model associated with the best parameter setting, in term of quality evaluation metric\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis example shows how a grid-based approach can be used to tune a logistic regression classifier on a structured dataset: the pipeline that is repeated multiple times is based on the cross validation component. The input data set is the same structured dataset used for the example of the evaluators.\nThe following parameters of the logistic regression algorithm are considered in the brute-force search/parameter tuning\n\nMaximum iteration: \\([10, 100, 1000]\\)\nRegulation parameter: \\([0.1, 0.01]\\)\n\nIn total, 6 parameter configurations are evaluated (\\(3*2\\)).\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\nlabeledData = \"ex_dataValidation/labeledData.csv\"\nunlabeledData = \"ex_dataValidation/unlabeledData.csv\"\noutputPath = \"predictionsLRPipelineTuning/\"\n\n## Create a DataFrame from labeledData.csv\n## Training data in raw format\nlabeledDataDF = spark.read.load(\n    labeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## *************************\n## Training step\n## *************************\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\", \"attr2\", \"attr3\"],\n    outputCol=\"features\"\n)\n\n## Create a LogisticRegression object.\n## LogisticRegression is an Estimator that is used to\n## create a classification model based on logistic regression.\nlr = LogisticRegression()\n\n## Define a pipeline that is used to create the logistic regression\n## model on the training data. The pipeline includes also the preprocessing step\npipeline = Pipeline().setStages([assembler, lr])\n\n## We use a ParamGridBuilder to construct a grid of parameter values to\n## search over.\n## We set 3 values for lr.setMaxIter and 2 values for lr.regParam.\n## This grid will evaluate 3 x 2 = 6 parameter settings for\n## the input pipeline.\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.maxIter, [10,100,1000]) \\\n    .addGrid(lr.regParam, [0.1,0.01]) \\\n    #.build() # <1>\n\n## We now treat the Pipeline as an Estimator, wrapping it in a\n## CrossValidator instance. This allows us to jointly choose parameters\n## for all Pipeline stages.\n## CrossValidator requires\n## - an Estimator\n## - a set of Estimator ParamMaps\n## - an Evaluator.\ncv = CrossValidator() \\\n    .setEstimator(pipeline) \\\n    .setEstimatorParamMaps(paramGrid) \\\n    .setEvaluator(BinaryClassificationEvaluator()) \\\n    #.setNumFolds(3) # <2>\n\n## Run cross-validation. The result is the logistic regression model\n## based on the best set of parameters (based on the results of the\n## cross-validation operation).\n#tunedLRmodel = cv.fit(labeledDataDF) # <3>\n\n## Now, the tuned classification model can be used to predict the class label\n## of new unlabeled data\n\n## *************************\n## Prediction step\n## *************************\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(\n    unlabeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Make predictions on the unlabled data using the transform() method of the\n## trained tuned classification model transform uses only the content of 'features'\n## to perform the predictions. The model is associated with the pipeline and hence\n## also the assembler is executed\npredictionsDF = tunedLRmodel.transform(unlabeledData)\n\n## The returned DataFrame has the following schema (attributes)\n## - features: vector (values of the attributes)\n## - label: double (value of the class label)\n## - rawPrediction: vector (nullable = true)\n## - probability: vector (The i-th cell contains the probability that the current\n## record belongs to the i-th class\n## - prediction: double (the predicted class label)\n## Select only the original features (i.e., the value of the original attributes\n## attr1, attr2, attr3) and the predicted class for each record\npredictions = predictionsDF.select(\"attr1\", \"attr2\", \"attr3\", \"prediction\")\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\nThere is one call to the addGrid method for each parameter that we want to set: each call to the addGrid method is characterized by the parameter we want to consider, and the list of values to test/to consider.\nHere the characteristics of the cross validation are set: the pipeline to be evaluated, the set of parameters to be considered, the evaluator (i.e., the object that is used to evaluate the quality of the model), and the number of folds to consider (i.e., the number of repetitions).\nThe returned model is the one associated with the best parameter setting, based on the result of the cross-validation test\n\n\n\n\n\n\nSparse labeled data\nFrequently the training data are sparse (e.g., textual data are sparse: each document contains only a subset of the possible words), so sparse vectors are frequently used. MLlib supports reading training examples stored in the LIBSVM format: this is a commonly used textual format that is used to represent sparse documents/data points.\nThe LIBSVM format is a textual format in which each line represents an input record/data point by using a sparse feature vector: each line has the format\nlabel index1:value1 index2:value2 ...\nwhere\n\nlabel is an integer associated with the class label. It is the first value of each line.\nindex# are integer values representing the features\nvalue# are the (double) values of the features\n\nConsider the following two records/data points characterized by 4 predictive features and a class label\n\n\n\n\\(\\textbf{Features} = [5.8, 1.7, 0 , 0 ]\\)\n\\(\\textbf{Label} = 1\\)\n\n\n\\(\\textbf{Features} = [4.1, 0 , 2.5, 1.2]\\)\n\\(\\textbf{Label} = 0\\)\n\n\n\nTheir LIBSVM format-based representation is the following\n1 1:5.8 2:1.7\n0 1:4.1 3:2.5 4:1.2\nLIBSVM files can be loaded into DataFrames by combining the following methods\n\nread()\nformat(\"libsvm\")\nload(inputpath)\n\nThe returned DataFrame has two columns:\n\nlabel: the double value associated with the label\nfeatures: the sparse vector associated with the predictive features\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nspark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")"
  },
  {
    "objectID": "18c_clustering.html",
    "href": "18c_clustering.html",
    "title": "21  Clustering algorithms",
    "section": "",
    "text": "Spark MLlib provides a (limited) set of clustering algorithms\n\nK-means\nBisecting k-means\nGaussian Mixture Model (GMM)\n\nEach clustering algorithm has its own parameters, however all the provided algorithms identify a set of groups of objects/clusters and assign each input object to one single cluster. All the clustering algorithms available in Spark work only with numerical data: categorical values must be mapped to integer values (i.e., numerical values).\nThe input of the MLlib clustering algorithms is a DataFrame containing a column called features of type Vector. The clustering algorithm clusters the input records by considering only the content of features (the other columns, if any, are not considered).\n:::{.callout-note collapse=“true”} ### Example The goal is to group customers in groups based on their characteristics.\nConsider the following input data: a set of customer profiles.\n\n\n\nMonthlyIncome\nNumChildren\n\n\n\n\n\\(1400.0\\)\n\\(2\\)\n\n\n\\(11105.5\\)\n\\(0\\)\n\n\n\\(2150.0\\)\n\\(2\\)\n\n\n\nThe following input DataFrame that must be generated as input for the MLlib clustering algorithms\n\n\n\nfeatures\n\n\n\n\n\\(1400.0,2.0\\)\n\n\n\\(11105.5,0.0\\)\n\n\n\\(2150.0,2.0\\)\n\n\n\nThe values of all input attributes are stored in a vector of doubles (one vector for each input record). The generated DataFrame contains a column called features containing the vectors associated with the input records.\n\nMain steps\nThe steps for clustering with Mllib are the following\n\nCreate a DataFrame with the features column.\nDefine the clustering pipeline and run the .fit() method on the input data to infer the clustering model (e.g., the centroids of the k-means algorithm). This step returns a clustering model.\nInvoke the .transform() method of the inferred clustering model on the input data to assign each input record to a cluster. This step returns a new DataFrame with the new column “prediction” in which the cluster identifier is stored for each input record.\n\n\n\nK-means clustering algorithm\nK-means is one of the most popular clustering algorithms, characterized by one important parameter: the number of clusters \\(K\\) (the choice of \\(K\\) is a complex operation). Notice that this method is able to identify only spherical shaped clusters.\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe following paragraphs show how to apply the K-means algorithm provided by MLlib. The input dataset is a structured dataset with a fixed number of attributes, and all the attributes are numerical attributes.\nExample of input file\nattr1,attr2,attr3\n0.5,0.9,1.0\n0.6,0.6,0.7\nIn this example code it is assumed that the input data is already normalized (i.e., all values are already in the range \\([0,1]\\)). Scalers/Normalizers can be used to normalized data if it is needed.\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\ninputData = \"ex_datakmeans/dataClusteering.csv\"\noutputPath = \"clusterskmeans/\"\n\n## Create a DataFrame from dataClusteering.csv\n## Training data in raw format\ninputDataDF = spark.read.load(\n    inputData,format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\", \"attr2\", \"attr3\"],\n    outputCol=\"features\"\n)\n\n## Create a k-means object.\n## k-means is an Estimator that is used to\n## create a k-means algorithm\nkm = KMeans()\n\n## Set the value of k ( = number of clusters)\nkm.setK(2)\n\n## Define the pipeline that is used to cluster\n## the input data\npipeline = Pipeline().setStages([assembler, km])\n\n## Execute the pipeline on the data to build the\n## clustering model\nkmeansModel = pipeline.fit(inputDataDF)\n\n## Now the clustering model can be applied on the input data\n## to assign them to a cluster (i.e., assign a cluster id)\n## The returned DataFrame has the following schema (attributes)\n## - features: vector (values of the attributes)\n## - prediction: double (the predicted cluster id)\n## - original attributes attr1, attr2, attr3\n#clusteredDataDF = kmeansModel.transform(inputDataDF) # <1>\n\n## Select only the original columns and the clusterID (prediction) one\n## I rename prediction to clusterID\nclusteredData = clusteredDataDF \\\n    .select(\"attr1\", \"attr2\", \"attr3\", \"prediction\") \\\n    .withColumnRenamed(\"prediction\", \"clusterID\")\n\n## Save the result in an HDFS output folder\nclusteredData.write.csv(outputPath, header=\"true\")\n\nThe returned DataFrame has a new column called “prediction” in which the predicted cluster identifier (an integer) is stored for each input record."
  },
  {
    "objectID": "18d_regression.html",
    "href": "18d_regression.html",
    "title": "22  Regression algorithms",
    "section": "",
    "text": "Spark MLlib provides a set of regression algorithms\n\nLinear regression\nDecision tree regression\nRandom forest regression\nSurvival regression\nIsotonic regression\n\nA regression algorithm is used to predict the value of a continuous attribute (the target attribute) by applying a model on the predictive attributes. The model is trained on a set of training data (i.e., a set of data for which the value of the target attribute is know), and it is applied on new data to predict the target attribute.\nThe regression algorithms available in Spark work only on numerical data. They work similarly to classification algorithms, but they predict continuous numerical values (the target attribute is a continuous numerical attribute).\nThe input data must be transformed in a DataFrame having the following attributes:\n\nlabel: the continuous numerical double value to be predicted\nfeatures: the double vector with the predictive features.\n\nThe main steps used to infer a regression model with MLlib are the same we use to infer a classification model, and the difference is only given by the type of the target attribute to predict.\n\nLinear regression\nLinear regression (LR) is a popular, effective and efficient regression algorithm. The following paragraphs show how to instantiate a linear regression algorithm in Spark and apply it on new data.\n\nLR with structured data\nThe input dataset is a structured dataset with a fixed number of attributes\n\nOne attribute is the target attribute (the label): it is assumed that the first column contains the target attribute;\nThe others are predictive attributes that are used to predict the value of the target attribute.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider the following example file\nlabel,attr1,attr2,attr3\n2.0,0.0,1.1,0.1\n5.0,2.0,1.0,-1.0\n5.0,2.0,1.3,1.0\n2.0,0.0,1.2,-0.5\nEach record has three predictive attributes and the target attribute\n\nThe first attribute (“label”) is the target attribute;\nThe other attributes (“attr1”, “attr2”, “attr3”) are the predictive attributes.\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\n## input and output folders\ntrainingData = \"ex_dataregression/trainingData.csv\"\nunlabeledData = \"ex_dataregression/unlabeledData.csv\"\noutputPath = \"predictionsLinearRegressionPipeline/\"\n\n## *************************\n## Training step\n## *************************\n## Create a DataFrame from trainingData.csv\n## Training data in raw format\ntrainingData = spark.read.load(\n    trainingData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Define an assembler to create a column (features) of type Vector\n## containing the double values associated with columns attr1, attr2, attr3\nassembler = VectorAssembler(\n    inputCols=[\"attr1\", \"attr2\", \"attr3\"],\n    outputCol=\"features\"\n)\n\n## Create a LinearRegression object.\n## LinearRegression is an Estimator that is used to\n## create a regression model based on linear regression\nlr = LinearRegression()\n\n## Set the values of the parameters of the\n## Linear Regression algorithm using the setter methods.\n## There is one set method for each parameter\n## For example, we are setting the number of maximum iterations to 10\n## and the regularization parameter to 0.01\nlr.setMaxIter(10)\nlr.setRegParam(0.01)\n\n## Define a pipeline that is used to create the linear regression\n## model on the training data. The pipeline includes also\n## the preprocessing step\npipeline = Pipeline().setStages([assembler, lr])\n\n## Execute the pipeline on the training data to build the\n## regression model\nregressionModel = pipeline.fit(trainingData)\n## Now, the regression model can be used to predict the target attribute value\n## of new unlabeled data\n\n## Create a DataFrame from unlabeledData.csv\n## Unlabeled data in raw format\nunlabeledData = spark.read.load(\n    unlabeledData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Make predictions on the unlabled data using the transform() method of the\n## trained regression model transform uses only the content of 'features'\n## to perform the predictions. The model is associated with the pipeline and hence\n## also the assembler is executed\npredictionsDF = regressionModel.transform(unlabeledData)\n\n## The returned DataFrame has the following schema (attributes)\n## - attr1\n## - attr2\n## - attr3\n## - original attributes\n## - features: vector (values of the attributes)\n## - label: double (actual value of the target variable)\n## - prediction: double (the predicted continuous value of the target variable)\n## Select only the original features (i.e., the value of the original attributes\n## attr1, attr2, attr3) and the predicted value of the target variable for each record\npredictions = predictionsDF.select(\"attr1\", \"attr2\", \"attr3\", \"prediction\")\n\n## Save the result in an HDFS output folder\npredictions.write.csv(outputPath, header=\"true\")\n\n\n\n\n\nLR with textual data\nThe linear regression algorithms can be used also when the input dataset is a collection of documents/texts. Also in this case the text must be mapped to a set of continuous attributes.\n\n\nParameter setting\nThe tuning approach that we used for the classification problem can also be used to optimize the regression problem, the only difference is given by the used evaluator: in this case the difference between the actual value and the predicted one must be computed."
  },
  {
    "objectID": "18e_mining.html",
    "href": "18e_mining.html",
    "title": "23  Itemset and Association rule mining",
    "section": "",
    "text": "Spark MLlib provides\n\nAn itemset mining algorithm based on the FP-growth algorithm, that extracts all the sets of items (of any length) with a minimum frequency;\nA rule mining algorithm, that extracts the association rules with a minimum frequency and a minimum confidence; notice that only the rules with one single item in the consequent of the rules are extracted.\n\nThe input dataset in this case is a set of transactions, where each transaction is defined as a set of items\nA transactional dataset example\nABCD\nAB\nBC\nADE\nIt contains 4 transactions, and the distinct items are A, B, C, D, E.\n\nThe FP-Growth algorithm and Association rule mining\nFP-growth is one of the most popular and efficient itemset mining algorithms. It is characterized by one single parameter: the minimum support threshold (minsup), that is the minimum frequency of the itemset in the input transational dataset; it can assume a real value in the range \\((0,1]\\). The minsup threshold is used to limit the number of mined itemsets.\nThe input dataset is a transactional dataset.\nGiven a set of frequent itemsets, the frequent association rules can be mined. An association rule is mined if\n\nIts frequency is greater than the minimum support threshold minsup (i.e., a minimum frequency). The minsup value is specified during the itemset mining step and not during the association rule mining step.\nIts confidence is greater than the minimum confidence threshold minconf (i.e., a minimum correlation). It is a real value in the range \\([0,1]\\).\n\nThe MLlib implementation of FP-growth is based on DataFrames, but differently from the other algorithms, the FP-growth algorithm is not invoked by using pipelines.\n\nSteps for itemset and association rule mining in Spark\n\nInstantiate an FP-Growth object\nInvoke the fit(input data) method on the FP-Growth object\nRetrieve the sets of frequent itemset and association rules by invoking the following methods of on the FP-Growth object\n\nfreqItemsets()\nassociationRules()\n\n\n\n\nInput\nThe input of the MLlib itemset and rule mining algorithm is a DataFrame containing a column called items, whose data type is array of values. Each record of the input DataFrame contains one transaction (i.e., a set of items).\n\n\n\n\n\n\nExample\n\n\n\n\n\nExample of input data\ntransactions\nABCD\nAB\nBC\nADE\nThe column items must be created before invoking FP-growth\n\n\n\nitems\n\n\n\n\n\\([A,B,C,D]\\)\n\n\n\\([A,B]\\)\n\n\n\\([B,C]\\)\n\n\n\\([A,D,E]\\)\n\n\n\nEach input line is stored in an array of strings. The generated DataFrame contains a column called items, which is an ArrayType, containing the lists of items associated with the input transactions.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis example shows how to extract the set of frequent itemsets from a transactional dataset and the association rules from the extracted frequent itemsets.\nThe input dataset is a transactional dataset: each line of the input file contains a transaction (i.e., a set of items)\ntransactions\nABCD\nAB\nBC\nADE\nfrom pyspark.ml.fpm import FPGrowth\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\nfrom pyspark.sql.functions import col, split\n\n## input and output folders\ntransactionsData = \"ex_dataitemsets/transactions.csv\"\noutputPathItemsets = \"Itemsets/\"\noutputPathRules = \"Rules/\"\n\n## Create a DataFrame from transactions.csv\ntransactionsDataDF = spark.read.load(\n    transactionsData,\n    format=\"csv\",\n    header=True,\n    inferSchema=True\n)\n\n## Transform Column transactions into an ArrayType\ntrsDataDF = transactionsDataDF \\\n    .selectExpr('split(transactions, \" \")') \\\n    #.withColumnRenamed(\"split(transactions, )\", \"items\") # <1>\n\n## Transform Column transactions into an ArrayType\ntrsDataDF = transactionsDataDF\\\n.selectExpr('split(transactions, \" \")')\\\n.withColumnRenamed(\"split(transactions, )\", \"items\")\n\n## Create an FP-growth Estimator\nfpGrowth = FPGrowth(\n    itemsCol=\"items\",\n    minSupport=0.5,\n    minConfidence=0.6\n)\n\n## Extract itemsets and rules\nmodel = fpGrowth.fit(trsDataDF)\n\n## Retrieve the DataFrame associated with the frequent itemsets\ndfItemsets = model.freqItemsets\n\n## Retrieve the DataFrame associated with the frequent rules\ndfRules = model.associationRules\n\n## Save the result in an HDFS output folder\n#dfItemsets.write.json(outputPathItemsets) # <2>\n\n## Save the result in an HDFS output folder\ndfRules.write.json(outputPathRules)\n\n'split(transactions, \" \")' is the pyspark.sql.functions.split() function. It returns a SQL ArrayType.\nThe result is stored in a JSON file because itemsets and rules are stored in columns associated with the data type Array. Hence, CSV files cannot be used to store the result."
  },
  {
    "objectID": "19_graph_analytics_1.html",
    "href": "19_graph_analytics_1.html",
    "title": "24  Graph analytics in Spark",
    "section": "",
    "text": "Introduction\nGraphs are data structures composed of nodes and edges\n\nnodes/vertexes are denoted as \\(V=\\{v_1,v_2,...,v_n\\}\\)\nedges are denoted as \\(E=\\{e_1,e_2,...,e_n\\}\\)\n\nGraph analytics is the process of analyzing relationships between vertexes and edges.\n\n\nExample of graph\n\n\n\nGraphs are called undirected if edges do not have a direction, otherwise they are called directed graphs. Vertexes and edges can have data associated with them\n\nweights are associated to edges (e.g., they may represent the strength of the relationship);\nlabels are associated to vertexes (e.g., they may be the string associated with the name of the vertex).\n\n\n\nGraph with labels and weights\n\n\n\n\n\n\n\n\n\nWhy graph analytics?\n\n\n\nGraphs are natural way of describing relationships. Some practical example of analytics over graphs\n\nRanking web pages (Google PageRank)\n\n\n\nPages in the web\n\n\n\n\nDetecting group of friends\n\n\n\nSocial networks\n\n\n\n\n\nMovies watched by users\n\n\n\n\nDetermine importance of infrastructure in electrical networks\n…\n\n\n\n\n\nSpark GraphX and GraphFrames\nGraphX is the Spark RDD-based library for performing graph processing. It is a core part of Spark.\n\n\nSpark core libraries\n\n\n\nGraphX\n\nis low level interface with RDD\nis very powerful: many application and libraries built on top of it\nis not easy to use or optimize\nhas no Python version of the APIs\n\nGraphFrames is a library DataFrame-based for performing graph processing. It is a Spark external package built on top of GraphX.\n\n\nGraphFrame structure\n\n\n\n\n\nBuilding and querying graphs with GraphFrames\n\nBuilding a Graph\nDefine vertexes and edges of the graph: vertexes and edges are represented by means of records inside DataFrames with specifically named columns\n\nOne DataFrame for the definition of the vertexes of the graph. The DataFrames that are used to represent nodes/vertexes\n\nContain one record per vertex\nMust contain a column named “id” that stores unique vertex IDs\nCan contain other columns that are used to characterize vertexes\n\nOne DataFrame for the definition of the edges of the graph. The DataFrames that are used to represent edges\n\nContain one record per edge\nMust contain two columns “src” and “dst” storing source vertex IDs and destination vertex IDs of edges\nCan contain other columns that are used to characterize edges\n\n\nCreate a graph of type graphframes.graphframe.GraphFrame by invoking the constructor GraphFrame(v,e)\n\nv: the DataFrame containing the definition of the vertexes\ne: the DataFrame containing the definition of the edges\n\nGraphs in graphframes are directed graphs.\n\n\nBuilding a graph example\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven this Vertex DataFrame\n\n\n\nid\nname\nage\n\n\n\n\nu1\nAlice\n34\n\n\nu2\nBob\n36\n\n\nu3\nCharlie\n30\n\n\nu4\nDavid\n29\n\n\nu5\nEsther\n32\n\n\nu6\nFanny\n36\n\n\nu7\nGabby\n60\n\n\n\nAnd this Edge DataFrame\n\n\n\nsrc\ndst\nrelationship\n\n\n\n\nu1\nu2\nfriend\n\n\nu2\nu3\nfollow\n\n\nu3\nu2\nfollow\n\n\nu6\nu3\nfollow\n\n\nu5\nu6\nfollow\n\n\nu5\nu4\nfriend\n\n\nu4\nu1\nfriend\n\n\nu1\nu5\nfriend\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n\n\n\n\nDirected vs undirected edges\nIn undirected graphs the edges indicate a two-way relationship (each edge can be traversed in both directions). In GraphX it is possible to use to_undirected() to create an undirected copy of the Graph. Unfortunately GraphFrames does not support it, but it is possible to convert a graph by applying a flatMap function over the edges of the directed graph that creates symmetric edges and then create a new GraphFrame.\n\n\nCache graphs\nAs with RDD and DataFrame, it is possible to cache graphs in GraphFrame: it is convenient if the same (complex) graph result of (multiple) transformations is used multiple times in the same application. To do it, simply invoke cache() on the GraphFrame to cache, so that it persists the DataFrame-based representation of vertexes and edges of the graph.\n\n\nQuerying the graph\nSome specific methods are provided to execute queries on graphs\n\nfilterVertices(condition)\nfilterEdges(condition)\ndropIsolatedVertices()\n\nThe returned result is the filtered version of the input graph.\n\nfilterVertices(condition)\nfilterVertices(condition) selects only the vertexes for which the specified condition is satisfied and returns a new graph with only the subset of selected vertexes.\ncondition contains an SQL-like condition on the values of the attributes of the vertexes (e.g., “age>35”).\n\n\nfilterEdges(condition)\nfilterEdges(condition) selects only the edges for which the specified condition is satisfied and returns a new graph with only the subset of selected edges.\ncondition contains an SQL-like condition on the values of the attributes of the edges (e.g., “relationship=‘friend’”).\n\n\ndropIsolatedVertices()\ndropIsolatedVertices() drops the vertexes that are not connected with any other node and returns a new graph without the dropped nodes.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the input graph, create a new subgraph\n\n\nInput graph\n\n\n\n\nInclude only the vertexes associated with users characterized by age between 29 and 50\n\n\n\nFilter vertexes\n\n\n\n\nInclude only the edges representing the friend relationship\n\n\n\nFilter edges\n\n\n\n\nDrop isolated vertexes\n\n\n\nDrop isolated vertices\n\n\n\n\n\nOutput graph\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\nselectedUsersandFriendRelGraph = g \\\n    .filterVertices(\"age>=29 AND age<=50\") \\\n    .filterEdges(\"relationship='friend'\") \\\n    .dropIsolatedVertices()\n\n\n\nGiven a GraphFrame, it is possible to access its vertexes and edges\n\ng.vertices returns the DataFrame associated with the vertexes of the input graph\ng.edges returns the DataFrame associated with the edges of the input graph\n\nAll the standard DataFrame transformations/actions are available also for the DataFrames that are used to store vertexes and edges. For example, the number of vertexes and the number of edges can be computed by invoking the count() action on the DataFrames vertices and edges, respectively.\n\n\n\n\n\n\nExample\n\n\n\n\n\nGiven the input graph\n\nCount how many vertexes and edges has the graph\nFind the smallest value of age (i.e., the age of the youngest user in the graph)\nCount the number of edges of type “follow” in the graph\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Count how many vertexes and edges has the graph\nprint(\"Number of vertexes: \",g.vertices.count())\nprint(\"Number of edges: \",g.edges.count())\n\n## Print on the standard output the smallest value of age\n## (i.e., the age of the youngest user in the graph)\ng.vertices.agg({\"age\":\"min\"}).show()\n\n## Print on the standard output\n## the number of \"follow\" edges in the graph.\nnumFollows = g.edges.filter(\"relationship = 'follow' \").count()\n\nprint(numFollows)\n\n\n\n\n\n\nMotif finding\nMotif finding refers to searching for structural patterns in graphs. A simple Domain-Specific Language (DSL) is used to specify the structure of the interesting patterns: the paths/subgraphs in the graph matching the specified structural pattern are selected.\n\nDSL for Motif finding\nThe basic unit of a pattern is a connection between vertexes\n\\[\n(v1) – [e1] \\rightarrow (v2)\n\\]\nmeans: an arbitrary edge [e1] from an arbitrary vertex (v1) to another arbitrary vertex (v2)\n\nEdges are denoted by square brackets: \\([e1]\\)\nVertexes are expressed by round brackets: \\((v1)\\), \\((v2)\\)\n\n\n\nBasic unit\n\n\n\nPatterns are chains of basic units\n\\[\n(v1) – [e1] \\rightarrow (v2);\\quad (v2) – [e2] \\rightarrow (v3)\n\\]\nmeans: an arbitrary edge from an arbitrary vertex \\(v1\\) to another arbitrary vertex \\(v2\\) and another arbitrary edge from \\(v2\\) to another arbitrary vertex \\(v3\\). Notice that \\(v3\\) and \\(v1\\) can be the same vertex.\n\n\nBasic unit chaining\n\n\n\nThe same vertex name is used in a pattern to have a reference to the same vertex\n\\[\n(v1) – [e1] \\rightarrow (v2);\\quad (v2) – [e2] \\rightarrow (v1)\n\\]\nmeans: an arbitrary edge from an arbitrary vertex \\(v1\\) to another arbitrary vertex \\(v2\\) and vice-versa.\n\n\nBasic unit self-chaining\n\n\n\nIt is acceptable to omit names for vertices or edges in patterns when not needed\n\\[\n(v1)-[\\quad]\\rightarrow(v2)\n\\]\nmeans: an arbitrary edge between two arbitrary vertexes \\(v1\\), \\(v2\\), but does not assign a name to the edge. These are called anonymous vertexes and edges.\n\n\nAnonymous vertexes and edges\n\n\n\nA basic unit (an edge between two vertexes) can be negated to indicate that the edge should not be present in the graph\n\\[\n(v1)-[\\quad]\\rightarrow(v2);\\quad !(v2)-[\\quad]\\rightarrow(v1)\n\\]\nmeans: edges from \\(v1\\) to \\(v2\\) but no edges from \\(v2\\) to \\(v1\\).\n\n\nNegating edges\n\n\n\nThe find(motif) method of GraphFrame is used to select motifs\n\nmotif is a DSL representation of the structural pattern\n\nfind() returns a DataFrame of all the paths matching the structural motif/pattern, one path per record. The returned DataFrame will have a column for each of the named elements (vertexes and edges) in the structural pattern/motif: Each column is a struct, and the fields of each struct are the labels/features of the associated vertex or edge. It can return duplicate rows/records, if there are many paths connecting the same nodes.\nMore complex queries on the structure and content of the patterns can be expressed by applying filters to the result DataFrame (i.e., more complex queries can be applied by combing find() and filter()).\n\n\n\n\n\n\nExample 1\n\n\n\n\n\nGiven the following graph\n\n\nExample graph\n\n\n\nFind the paths/subgraphs matching the pattern\n\\[\n(v1) – [e1] \\rightarrow (v2);\\quad (v2) – [e2] \\rightarrow (v1)\n\\]\nStore the result in a DataFrame\n\n\nResult\n\n\n\nPay attention that two paths are returned:\n\n\\(u2 \\rightarrow \\text{follow} \\rightarrow u3 \\rightarrow \\text{follow} \\rightarrow u2\\)\n\\(u3 \\rightarrow \\text{follow} \\rightarrow u2 \\rightarrow \\text{follow} \\rightarrow u3\\)\n\nThe content of the returned Dataframe is the following\n\n\n\n\n\n\n\n\n\n\\(v1\\)\n\\(e1\\)\n\\(v2\\)\n\\(e2\\)\n\n\n\n\n\\([u2, \\text{Bob}, 36]\\)\n\\([u2, u3, \\text{follow}]\\)\n\\([u3, \\text[Charlie], 30]\\)\n\\([u3, u2, \\text{follow}]\\)\n\n\n\\([u3, \\text[Charlie], 30]\\)\n\\([u3, u2, \\text{follow}]\\)\n\\([u2, \\text{Bob}, 36]\\)\n\\([u2, u3, \\text{follow}]\\)\n\n\n\n\nThere is one column for each (distinct) named vertex and edge of the structural pattern;\nThe records are associated with the vertexes and edges of the selected paths;\nAll columns are associated with the data type “struct”. Each struct has the same “schema/features” of the associated vertex or edge.\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Retrieve the motifs associated with the pattern\n## vertex -> edge -> vertex -> edge ->vertex\nmotifs = g.find(\"(v1)-[e1]->(v2); (v2)-[e2]->(v1)\")\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\nGiven the following graph\n\n\nExample graph\n\n\n\nFind the paths/subgraphs matching the pattern\n\\[\n(v1) - [\\text{friend}] \\rightarrow (v2);\\quad (v2) - [\\text{follow}] \\rightarrow (v3)\n\\]\nStore the result in a DataFrame\n\n\nFirst selected path\n\n\n\n\n\nSecond selected path\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Retrieve the motifs associated with the pattern\n## vertex -> edge -> vertex -> edge ->vertex\nmotifs = g.find(\"(v1)-[friend]->(v2); (v2)-[follow]->(v3)\")\n\n## Filter the motifs (the content of the motifs DataFrame)\n## Select only the ones matching the pattern\n## vertex -> friend-> vertex -> follow ->vertex\nmotifsFriendFollow = motifs \\\n    #.filter(\"friend.relationship='friend' AND follow.relationship='follow' \") # <1>\n\nColumns friend and follow are structs with three fields/attributes: “src”, “dst”, “relationship”. To access a field of a struct column use the syntax columnName.field (e.g., friend.relationship)\n\n\n\n\n\n\n\n\nBasic statistics\nSome specific properties are provided to compute basic statistics on the degrees of the vertexes\n\ndegrees\ninDegrees\noutDegrees\n\nThe returned result of each of this property is a DataFrame with id and (in/out)Degree value.\n\ndegrees\ndegrees returns the degree of each vertex (i.e., the number of edges associated with each vertex). The result is stored in a DataFrame with Columns (vertex) “id” and “degree”, with one record per vertex. Only the vertexes with \\(\\text{degree} \\geq 1\\) are stored in the returned DataFrame.\n\n\ninDegrees\ninDegrees returns the in-degree of each vertex (i.e., the number of in-edges associated with each vertex). The result is stored in a DataFrame with Columns (vertex) “id” and “inDegree”, with one record per vertex. Only the vertexes with \\(\\text{in-degree} \\geq 1\\) are stored in the returned DataFrame.\n\n\noutDegrees\noutDegrees returns the out-degree of each vertex (i.e., the number of out-edges associated with each vertex). The result is stored in a DataFrame with Columns (vertex) “id” and “outDegree”, with one record per vertex. Only the vertexes with \\(\\text{out-degree} \\geq 1\\) are stored in the returned DataFrame.\n\n\n\n\n\n\nExample 1\n\n\n\n\n\nGiven the input graph, compute\n\nDegree of each vertex\ninDegree of each vertex\noutDegree of each vertex\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Retrieve the DataFrame with the information about the degree of\n## each vertex\nvertexesDegreesDF = g.degrees\n\n## Retrieve the DataFrame with the information about the in-degree of\n## each vertex\nvertexesInDegreesDF = g.inDegrees\n\n## Retrieve the DataFrame with the information about the out-degree of\n## each vertex\nvertexesOutDegreesDF = g.outDegrees\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nGiven the input graph, select only the ids of the vertexes with at least 2 in-edges.\n\n\nExample graph\n\n\n\n\n\nFirst selected vertex\n\n\n\n\n\nSecond selected vertex\n\n\n\n\n\nResulting vertexes\n\n\n\nThe selected IDs are \\(u2\\) and \\(u3\\)\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Retrieve the DataFrame with the information about the in-degree of\n## each vertex\nvertexesInDegreesDF = g.inDegrees\n\n## Select only the vertexes with and in-degree value >=2\nselectedVertexesDF = vertexesInDegreesDF.filter(\"inDegree>=2\")\n\n## Select only the content of Column id\nselectedVertexesIDsDF = selectedVertexesDF.select(\"id\")"
  },
  {
    "objectID": "20_graph_analytics_2.html",
    "href": "20_graph_analytics_2.html",
    "title": "25  Graph Analytics in Spark",
    "section": "",
    "text": "Algorithms over graphs\nGraphFrame provides the parallel implementation of a set of state of the art algorithms for graph analytics\n\nBreadth first search\nShortest paths\nConnected components\nStrongly connected component\nLabel propagation\nPageRank\n…\n\nAlso custom algorithms can be designed and implemented.\n\nCheckpoint directory\nTo run some expensive algorithms, set a checkpoint directory that will store the state of the job at every iteration. This allows to continue where left off if the job crashes. Create such a folder to set the checkpoint directory with:\nsc.setCheckpointDir(graphframes_ckpts_dir)\n\ngraphframes_ckpts_dir is the new checkpoint folder directory\nsc is the SparkContext object (retrieve it from a SparkSession by using spark.sparkContext)\n\n\n\nBreadth first search\nBreadth-first search (BFS) is an algorithm for traversing/searching graph data structures: it finds the shortest path(s) from one vertex (or a set of vertexes)to another vertex (or a set of vertexes). It is used in many other algorithms\n\nLength of shortest paths\nConnected components\n…\n\n\nImplementation\nbfs(fromExpr, toExpr, edgeFilter=None maxPathLength=10)\nThe bfs() method of the GraphFrame class returns the shortest path(s) from the vertexes matching expression fromExpr expression to vertexes matching expression toExpr. If there are many vertexes matching fromExpr and toExpr, only the couple(s) with the shortest length is returned.\n\nfromExpr: Spark SQL expression specifying valid starting vertexes for the execution of the BFS algorithm (e.g., to start from a specific vertex: “id = [start vertex id]”);\ntoExpr: Spark SQL expression specifying valid target vertexes for the BFS algorithm;\nmaxPathLength: Limit on the length of paths (default = 10);\nedgeFilter: Spark SQL expression specifying edges that may be used in the search (default None).\n\nbfs() returns a DataFrame containing the selected shortest path(s). Notice that ff multiple paths are valid and their length is equal to the shortest length, the returned DataFrame will contain one Row for each path. The number of columns of the returned DataFrame is equal to \\((\\text{length of the shortest path}*2).+1\\).\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nFind the shortest path from Esther to Charlie\nStore the result in a DataFrame\n\n\n\nExample graph\n\n\n\n\n\nResulting graph\n\n\n\nThe content of the returned DataFrame is the following\n\n\n\n\n\n\n\n\n\n\nfrom\n\\(e0\\)\n\\(v1\\)\n\\(e1\\)\nto\n\n\n\n\n\\([u5, \\text{Esther}, 32]\\)\n\\([u5, u6, \\text{follow}]\\)\n\\([u6, \\text{Fanny}, 36]\\)\n\\([u6, u3, \\text{follow}]\\)\n\\([u3, \\text{Charlie}, 30]\\)\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Search from vertex with name = \"Esther\" to vertex with name = \"Charlie\"\nshortestPaths = g.bfs(\"name = 'Esther' \", \"name = 'Charlie' \")\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nFind the shortest path from Alice to a user who is 30 years old\nStore the result in a DataFrame\n\n\n\nExample graph\n\n\n\n\n\nResulting graph\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Find the shortest path from Alice to a user who is 30 years old\nshortestPaths = g.bfs(\"name = 'Alice' \", \"age= 30\")\n\n\n\n\n\n\n\n\n\nExample 3\n\n\n\n\n\n\nFind the shortest path from any user who is less than 31 years old to any user who is more than 30 years old\nStore the result in a DataFrame\n\n\n\nExample graph\n\n\n\n\n\nResulting graph\n\n\n\nNotice that two paths are selected in this case\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Find the shortest path from any user who is less than 31 years old\n## to any user who is more than 30 years old\nshortestPaths = g.bfs(\"age<31\", \"age>30\")\n\n\n\n\n\n\n\n\n\nExample 4\n\n\n\n\n\n\nFind the shortest path from Alice to any user who is less than 31 years old without using follow edges\nStore the result in a DataFrame\n\n\n\nExample graph\n\n\n\n\n\nResulting graph\n\n\n\nNotice that two paths are selected in this case\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Find the shortest path from Alice to any user who is less\n## than 31 years old without using “follow” edges\nshortestPaths = g.bfs(\"name = 'Alice' \", \"age<31\", \"relationship<> 'follow' \")\n\n\n\n\n\n\nShortest path\nThe shortest path method selects the length of the shortest path(s) from each vertex to a given set of landmark vertexes. It uses the BFS algorithm for computing the shortest paths.\n\nImplementation\nshortestPaths(landmarks)\nThe shortestPaths(landmarks) method of the GraphFrame class returns the length of the shortest path(s) from each vertex to a given set of landmarks vertexes. For each vertex, one shortest path for each landmark vertex is computed and its length is returned.\n\nlandmarks: list of IDs of landmark vertexes (e.g., \\([u1, u4]\\))\n\nshortestPaths() returns a DataFrame that contains one record/row for each distinct vertex of the input graph (also for the non-connected ones). This method is characterized by the following columns\n\none column for each attribute of the vertexes\ndistances (type map): for each landmark lm it contains one pair (ID lm: length shortest path from the vertex of the current record to lm)\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nFind for each user the length of the shortest path to user \\(u1\\) (i.e., Alice)\nStore the result in a DataFrame\n\n\n\nExample graph\n\n\n\n\n\n\nVertex\nDistance to \\(u1\\)\n\n\n\n\n\\(u1\\)\n0\n\n\n\\(u2\\)\n-\n\n\n\\(u3\\)\n-\n\n\n\\(u4\\)\n1\n\n\n\\(u5\\)\n2\n\n\n\\(u6\\)\n-\n\n\n\\(u7\\)\n-\n\n\n\nThe content of the returned DataFrame is the following\n\n\n\nid\nname\nage\ndistances\n\n\n\n\n\\(u1\\)\nAlice\n34\n\\([u1 \\rightarrow 0]\\)\n\n\n\\(u2\\)\nBob\n36\n\\([\\quad]\\)\n\n\n\\(u3\\)\nCharlie\n30\n\\([\\quad]\\)\n\n\n\\(u4\\)\nDavid\n29\n\\([u1 \\rightarrow 1]\\)\n\n\n\\(u5\\)\nEsther\n32\n\\([u1 \\rightarrow 2]\\)\n\n\n\\(u6\\)\nFanny\n36\n\\([\\quad]\\)\n\n\n\\(u7\\)\nGabby\n60\n\\([\\quad]\\)\n\n\n\n\n\\([u1 \\rightarrow 0]\\): data type is map. It stores a set of pairs (Key: Value)\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Find for each user the length of the shortest path to user u1\nshortestPaths = g.shortestPaths([\"u1\"])\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\nFind for each user the length of the shortest path to users \\(u1\\) (Alice) and \\(u4\\) (David)\nStore the result in a DataFrame\n\n\n\nExample graph\n\n\n\n\n\n\nVertex\nDistance to \\(u1\\)\nDistance to \\(u1\\)\n\n\n\n\n\\(u1\\)\n0\n2\n\n\n\\(u2\\)\n-\n-\n\n\n\\(u3\\)\n-\n-\n\n\n\\(u4\\)\n1\n0\n\n\n\\(u5\\)\n2\n1\n\n\n\\(u6\\)\n-\n-\n\n\n\\(u7\\)\n-\n-\n\n\n\nThe content of the returned DataFrame is the following\n\n\n\nid\nname\nage\ndistances\n\n\n\n\n\\(u1\\)\nAlice\n34\n\\([u1 \\rightarrow 0, u4 \\rightarrow 2]\\)\n\n\n\\(u2\\)\nBob\n36\n\\([\\quad]\\)\n\n\n\\(u3\\)\nCharlie\n30\n\\([\\quad]\\)\n\n\n\\(u4\\)\nDavid\n29\n\\([u1 \\rightarrow 1, u4 \\rightarrow 0]\\)\n\n\n\\(u5\\)\nEsther\n32\n\\([u1 \\rightarrow 2, u4 \\rightarrow 1]\\)\n\n\n\\(u6\\)\nFanny\n36\n\\([\\quad]\\)\n\n\n\\(u7\\)\nGabby\n60\n\\([\\quad]\\)\n\n\n\n\n\\([u1 \\rightarrow 0]\\): data type is map. It stores a set of pairs (Key: Value)\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Find for each user the length of the shortest paths to users u1 and u4\nshortestPaths = g.shortestPaths([\"u1\", \"u4\"])\n\n\n\n\n\n\nConnected components\nA connected component of a graph is a subgraph \\(sg\\) such that\n\nAny two vertexes in \\(sg\\) are connected to each other by at least one path\nThe set of vertexes in \\(sg\\) is not connected to any additional vertexes in the original graph\n\nThe direction of edges is not considered.\n\n\nTwo connected components\n\n\n\n\n\nThree connected components\n\n\n\nThe connectedComponents() method of the GraphFrame class returns the connected components of the input graph. This is an expensive algorithm, and requires setting a Spark checkpoint directory.\n\nImplementation\nconnectedComponents()\nThe connectedComponents() method returns a DataFrame that contains one record/row for each distinct vertex of the input graph. It is characterized by the following columns\n\none column for each attribute of the vertexes\ncomponent (type long). It is the identifier of the connected component to which the current vertex has been assigned.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nPrint on the stdout the number of connected components of the following graph\n\n\nExample graph\n\n\n\n\n\nResult\n\n\n\nNotice that The are two connected components on this graph.\nThis is the content of the DataFrame used to store the two identified connected components\n\n\n\nid\nname\nage\ncomponent\n\n\n\n\n\\(u6\\)\nFanny\n36\n146028888064\n\n\n\\(u1\\)\nAlice\n34\n146028888064\n\n\n\\(u3\\)\nCharlie\n30\n146028888064\n\n\n\\(u5\\)\nEsther\n32\n146028888064\n\n\n\\(u2\\)\nBob\n36\n146028888064\n\n\n\\(u4\\)\nDavid\n29\n146028888064\n\n\n\\(u7\\)\nGabby\n60\n1546188226560\n\n\n\nNotice the “component” field\n\n“146028888064”: vertexes of the first component\n“1546188226560”: vertexes of the second component\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Set checkpoint folder\nsc.setCheckpointDir(\"tmp_ckpts\")\n\n## Run the algorithm\nconnComp=g.connectedComponents()\n\n## Count the number of components\nnComp=connComp.select(\"component\").distinct().count()\n\nprint(\"Number of connected components: \", nComp)\n\n\n\n\n\n\nStrongly connected components\nA directed subgraph \\(sg\\) is called strongly connected if every vertex in \\(sg\\) is reachable from every other vertex in \\(sg\\). For undirected graph, connected and strongly connected components are the same.\n\n\nThree strongly connected subgraphs/components\n\n\n\n\nImplementation\nstronglyConnectedComponents()\nThe stronglyConnectedComponents() method of the GraphFrame class returns the strongly connected components of the input graph. It is an expensive algorithm (better to run it on a cluster with yarn scheduler even with small graphs), and it requires setting a Spark checkpoint directory.\nstronglyConnectedComponents() returns a DataFrame that contains one record/row for each distinct vertex of the input graph. It is characterized by the following columns\n\none column for each attribute of the vertexes\ncomponent (type long). It is the identifier of the strongly connected component to which the current vertex has been assigned.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nPrint on the stdout the number of strongly connected components of the input graph.\n\n\nExample graph\n\n\n\n\n\nResulting graph\n\n\n\nNotice that there are four connected components on this graph.\nThis is the content of the DataFrame used to store the identified strongly connected components\n\n\n\nid\nname\nage\ncomponent\n\n\n\n\n\\(u3\\)\nCharlie\n30\n146028888064\n\n\n\\(u2\\)\nBob\n36\n146028888064\n\n\n\\(u1\\)\nAlice\n34\n498216206336\n\n\n\\(u5\\)\nEsther\n32\n498216206336\n\n\n\\(u4\\)\nDavid\n29\n498216206336\n\n\n\\(u6\\)\nFanny\n36\n1090921693184\n\n\n\\(u7\\)\nGabby\n60\n1546188226560\n\n\n\nNotice the “component” field\n\n“146028888064”: vertexes of the first strongly connected component\n“498216206336”: vertexes of the second strongly connected component\n“1090921693184”: vertexes of the third strongly connected component\n“1546188226560”: vertexes of the fourth strongly connected component\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Set checkpoint folder\nsc.setCheckpointDir(\"tmp_ckpts\")\n\n## Run the algorithm\nstrongConnComp = g.stronglyConnectedComponents(maxIter=10)\n\n## Count the number of strongly connected components\nnComp=strongConnComp.select(\"component\").distinct().count()\n\nprint(\"Number of strongly connected components: \", nComp)\n\n\n\n\n\n\nLabel propagation\nLabel Propagation is an algorithm for detecting communities in graphs. It is similar to clustering, but exploits connectivity. Convergence is not guaranteed, and also it is possible to end up with trivial solutions.\n\nThe Label Propagation algorithm\nEach vertex in the network is initially assigned to its own community: at every step, vertexes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages.\n\n\nImplementation\nlabelPropagation(maxIter)\nThe labelPropagation(maxIter) method of the GraphFrame class runs and returns the result of the label propagation algorithm.\n\nmaxIter: number of iterations to run\n\nlabelPropagation() returns a DataFrame that contains one record/Row for each distinct vertex of the input graph. It is characterized by the following columns\n\none column for each attribute of the vertexes\nlabel (type long). It is the identifier of the community to which the current vertex has been assigned.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSplit in groups the vertexes of the graph by using the label propagation algorithm.\n\n\nExample graph\n\n\n\nNotice that the result returned by one run of the algorithm. Pay attention that convergence is not guarantee, and different results may come out from different runs.\n\n\nResults\n\n\n\nThis is the content of the DataFrame used to store the identified communities\n\n\n\nid\nname\nage\nlabel\n\n\n\n\n\\(u3\\)\nCharlie\n30\n146028888064\n\n\n\\(u4\\)\nDavid\n29\n498216206336\n\n\n\\(u1\\)\nAlice\n34\n498216206336\n\n\n\\(u5\\)\nEsther\n32\n498216206337\n\n\n\\(u7\\)\nGabby\n60\n1546188226560\n\n\n\\(u2\\)\nBob\n36\n1606317768704\n\n\n\\(u6\\)\nFanny\n36\n1606317768704\n\n\n\n\n“146028888064”: vertexes of the first community\n“498216206336”: vertexes of the second community\n“1546188226560”: vertexes of the third community\n“1606317768704”: vertexes of the fourth community\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Run the label propagation algorithm\nlabelComm = g.labelPropagation(10)\n\n\n\n\n\n\nPageRank\nPageRank is the original famous algorithm used by the Google Search engine to rank vertexes (web pages) in a graph by order of importance. For the Google search engine vertexes are web pages in the World Wide Web, and edges are hyperlinks among web pages. This algorithm assigns a numerical weighting (importance) to each node.\nIt computes a likelihood that a person randomly clicking on links will arrive at any particular web page. For having a high PageRank, it is important to\n\nHave many in-links\nBe liked by relevant pages (pages characterized by a high PageRank)\n\nThe basic idea is that each link vote is proportional to the importance of its source page \\(p\\): if page \\(p\\) with importance \\(\\text{PageRank}(p)\\) has \\(n\\) out-links, each out-link gets \\(\\frac{\\text{PageRank}(p)}{n}\\) votes; the importance of page \\(p\\) is the sum of the votes on its in-links.\n\nSimple recursive formulation\n\nInitialize each page rank to \\(1.0\\): for each \\(p\\) in pages set \\(\\textbf{PageRank}(p)\\) to \\(1.0\\)\nIterate for \\(max\\) iterations\n\nPage \\(p\\) sends a contribution \\(\\frac{\\textbf{PageRank}(p)}{\\textbf{numOutLinks}(p)}\\) to its neighbors (the pages it links);\nUpdate each page rank \\(\\textbf{PageRank}(p)\\) with the sum of the received contributions.\n\n\n\n\nRandom jumps formulation\nThe PageRank algorithm simulates the “random walk” of a user on the web. Indeed, at each step of the random walk, the random surfer has two options:\n\nwith probability \\(1-\\alpha\\), follow a link at random among the ones in the current page;\nwith probability \\(\\alpha\\), jump to a random page.\nInitialize each page rank to \\(1.0\\): for each \\(p\\) in pages set \\(\\textbf{PageRank}(p)\\) to \\(1.0\\)\nIterate for max iterations\n\nPage \\(p\\) sends a contribution \\(\\frac{\\textbf{PageRank}(p)}{\\textbf{numOutLinks}(p)}\\) to its neighbors (the pages it links);\nUpdate each page rank \\(\\textbf{PageRank}(p)\\) to \\(\\alpha + (1 - \\alpha)\\) times the sum of the received contributions.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\\(\\alpha=0.15\\)\nInitialization: \\(\\forall{p}, \\textbf{PageRank}(p) = 1.0\\)\n\n\n\nInitialization\n\n\n\n\nFigure 25.1: Iterations\n\n\n\n\n\nIteration #1\n\n\n\n\n\n\n\nIteration #2\n\n\n\n\n\n\n\n\n\nIteration #3\n\n\n\n\n\n\n\nIteration #4\n\n\n\n\n\n\n\n\n\nIteration #5\n\n\n\n\n\n\n\nIteration #50\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation\npageRank(resetProbability, maxIter, tol, sourceId)\nThe pageRank() method of the GraphFrame class runs the PageRank algorithm on the input graph.\n\nresetProbability: probability of resetting to a random vertex (probability \\(\\alpha\\) associated with random jumps);\nmaxIter: if set, the algorithm is run for a fixed number of iterations; this may not be set if the tol parameter is set;\ntol: if set, the algorithm is run until the given tolerance; this may not be set if the numIter parameter is set;\nsourceId: the source vertex for a personalized PageRank (optional parameter).\n\npageRank() returns a new GraphFrame that contains the same vertexes and edges of the input graph\n\nthe vertexes of the new graph are characterized by one new attribute, called “pagerank”, that stores the PageRank of the vertexes;\nthe edges of the new graph are characterized by one new attribute, called “weight”, that stores the weight (PageRank contribution) propagated through that edge.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nApply the PageRank algorithm on the following graph and select the user associated with the highest PageRank value.\n\n\nExample graph\n\n\n\n\n\nResulting graph\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## Run the PageRank algorithm\npageRanks = g.pageRank(maxIter=30)\n\n## Select the maximum value of PageRank\nmaxPageRank = pageRanks.vertices \\\n    .agg({\"pagerank\":\"max\"}) \\\n    .first()[\"max(pagerank)\"]\n\n## Select the user with the maximum PageRank\npageRanks.vertices \\\n    .filter(pageRanks.vertices.pagerank==maxPageRank) \\\n    .show()\n\n\n\n\n\n\nCustom graph algorithms\nGraphFrames provides primitives for developing yourself other graph algorithms. It is based on message passing approach: the two key components are:\n\naggregateMessages: it sends messages between vertexes, and aggregate messages for each vertex\njoins: it joins message aggregates with the original graph\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor each user, compute the sum of the ages of adjacent users (count many times the same adjacent user if there are many links).\nThe resulting table is\n\n\n\nVertex\nSumAges\n\n\n\n\n\\(u1\\)\n97\n\n\n\\(u2\\)\n94\n\n\n\\(u3\\)\n108\n\n\n\\(u4\\)\n66\n\n\n\\(u5\\)\n99\n\n\n\\(u6\\)\n62\n\n\n\nfrom graphframes import GraphFrame\n\n## Vertex DataFrame\nv = spark.createDataFrame(\n    [\n        (\"u1\", \"Alice\", 34),\n        (\"u2\", \"Bob\", 36),\n        (\"u3\", \"Charlie\", 30),\n        (\"u4\", \"David\", 29),\n        (\"u5\", \"Esther\", 32),\n        (\"u6\", \"Fanny\", 36),\n        (\"u7\", \"Gabby\", 60)\n    ],\n    [\"id\", \"name\", \"age\"]\n)\n\n## Edge DataFrame\ne = spark.createDataFrame(\n    [\n        (\"u1\", \"u2\", \"friend\"),\n        (\"u2\", \"u3\", \"follow\"),\n        (\"u3\", \"u2\", \"follow\"),\n        (\"u6\", \"u3\", \"follow\"),\n        (\"u5\", \"u6\", \"follow\"),\n        (\"u5\", \"u4\", \"friend\"),\n        (\"u4\", \"u1\", \"friend\"),\n        (\"u1\", \"u5\", \"friend\")\n    ],\n    [\"src\", \"dst\", \"relationship\"]\n)\n\n## Create the graph\ng = GraphFrame(v, e)\n\n## For each user, sum the ages of the adjacent users\n## Send the age of each destination of an edge to its source\nmsgToSrc = AggregateMessages.dst[\"age\"]\n\n## Send the age of each source of an edge to its destination\nmsgToDst = AggregateMessages.src[\"age\"]\n\n## Aggregate messages\naggAge = g.aggregateMessages(\n    sum(AggregateMessages.msg),\n    sendToSrc=msgToSrc,\n    sendToDst=msgToDst\n)\n\n#Show result\naggAge.show()"
  },
  {
    "objectID": "21_streaming_analytics.html",
    "href": "21_streaming_analytics.html",
    "title": "26  Streaming data analytics frameworks",
    "section": "",
    "text": "Introduction\n\nWhat is streaming processing?\nStreaming processing is the act of continuously incorporating new data to compute a result. Input data is unbounded (i.e., it has no beginning and no end). Series of events that arrive at the stream processing system, and the application will output multiple versions of the results as it runs or put them in a storage.\nMany important applications must process large streams of live data and provide results in near-real-time\n\nSocial network trends\nWebsite statistics\nIntrusion detection systems\n…\n\nThe main advantages of stream processing are:\n\nVastly higher throughput in data processing\nLow latency: application respond quickly (e.g., in seconds). It can keep states in memory\nMore efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation\n\nSome requirements and challenges are:\n\nScalable to large clusters\nResponding to events at low latency\nSimple programming model\nProcessing each event exactly once despite machine failures - Efficient fault-tolerance in stateful computations\nProcessing out-of-order data based on application timestamps (also called event time)\nMaintaining large amounts of state\nHandling load imbalance and stragglers\nUpdating your application’s business logic at runtime\n\n\n\nStream processing frameworks for big streaming data analytics\nSeveral frameworks have been proposed to process in real-time or in near real-time data streams\n\nApache Spark (Streaming component)\nApache Storm\nApache Flink\nApache Samza\nApache Apex\nApache Flume\nAmazon Kinesis Streams\n…\n\nAll these frameworks use a cluster of servers to scale horizontally with respect to the (big) amount of data to be analyzed.\n\nMain solutions\nThere are two main solutions\n\nContinuous computation of data streams. In this case, data are processed as soon as they arrive: every time a new record arrives from the input stream, it is immediately processed and a result is emitted as soon as possible. This is real-time processing.\nMicro-batch stream processing. Input data are collected in micro-batches, where each micro-batch contains all the data received in a time window (typically less than a few seconds of data). One micro-batch a time is processed: every time a micro-batch of data is ready, its entire content is processed and a result is emitted. This is near real-time processing.\n\n\n\nContinuous computation: one record at a time\n\n\n\n\n\nMicro-batch computation: one micro-batch at a time\n\n\n\n\n\n\nInput data processing and result guarantees\n\nAt-most-once\n\nEvery input element of a stream is processed once or less\nIt is also called no guarantee\nThe result can be wrong/approximated\n\nAt-least-once\n\nEvery input element of a stream is processed once or more\nInput elements are replayed when there are failures\nThe result can be wrong/approximated\n\nExactly-once\n\nEvery input element of a stream is processed exactly once\nInput elements are replayed when there are failures\nIf elements have been already processed they are not reprocessed\nThe result is always correct\nSlower than the other processing approaches\n\n\n\n\n\nSpark Streaming\n\nWhat is Spark Streaming\nSpark Streaming is a framework for large scale stream processing\n\nScales to 100s of nodes\nCan achieve second scale latencies\nProvides a simple batch-like API for implementing complex algorithm\nMicro-batch streaming processing\nExactly-once guarantees\nCan absorb live data streams from Kafka, Flume, ZeroMQ, Twitter, …\n\n\n\nSpark Streaming components\n\n\n\nMany important applications must process large streams of live data and provide results in near-real-time\n\nSocial network trends\nWebsite statistics\nIntrusion detection systems\n…\n\nThe requirements are\n\nScalable to large clusters\nSecond-scale latencies\nSimple programming model\nEfficient fault-tolerance in stateful computations\n\n\n\nSpark discretized stream processing\nSpark streaming runs a streaming computation as a series of very small, deterministic batch jobs. It splits each input stream in portions and processes one portion at a time (in the incoming order): the same computation is applied on each portion (called batch) of the stream.\nSo, Spark streaming\n\nSplits the live stream into batches of X seconds\nTreats each batch of data as RDDs and processes them using RDD operations\nFinally, the processed results of the RDD operations are returned in batches\n\n\n\nDiscretization in batches\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nWord count implementation using Spark streaming. Problem specification:\n\nThe input is a stream of sentences\nSplit the input stream in batches of 10 seconds each and print on the standard output, for each batch, the occurrences of each word appearing in the batch (i.e., execute the word count application one time for each batch of 10 seconds)\n\n\n\nInput and output\n\n\n\n\n\n\n\n\n\n\n\n\nKey concepts\n\n\n\n\nDSream\n\nSequence of RDDs representing a discretized version of the input stream of data (Twitter, HDFS, Kafka, Flume, ZeroMQ, Akka Actor, TCP sockets, …)\nOne RDD for each batch of the input stream\n\nTransformations\n\nModify data from one DStream to another\nStandard RDD operations (map, countByValue, reduce, join, …)\nWindow and Stateful operations (window, countByValueAndWindow, …)\n\nOutput Operations/Actions\n\nSend data to external entity (saveAsHadoopFiles, saveAsTextFile, …)\n\n\n\n\n\n\nWord count using DStreams\nA DStream is represented by a continuous series of RDDs. Each RDD in a DStream contains data from a certain batch/interval.\n\n\nRDDs composing a DStreams\n\n\n\nAny operation applied on a DStream translates to operations on the underlying RDDs. These underlying RDD transformations are computed by the Spark engine.\n\n\nOperations in a DStreams\n\n\n\n\n\nFault-tolerance\nDStreams remember the sequence of operations that created them from the original fault-tolerant input data. Batches of input data are replicated in memory of multiple worker nodes, therefore fault-tolerant: data lost due to worker failure, can be recomputed from input data.\n\n\n\nSpark streaming programs\n\nBasic structure of a Spark streaming program\n\nDefine a Spark Streaming Context object. Define the size of the batches (in seconds) associated with the Streaming context.\nSpecify the input stream and define a DStream based on it\nSpecify the operations to execute for each batch of data\nUse transformations and actions similar to the ones available for standard RDDs\nInvoke the start method, to start processing the input stream\nWait until the application is killed or the timeout specified in the application expires: if the timeout is not set and the application is not killed the application will run forever\n\n\n\nSpark streaming context\nThe Spark Streaming Context is defined by using the StreamingContext(SparkConf sparkC, Duration batchDuration) constructor of the class pyspark.streaming.StreamingContext. The batchDuration parameter specifies the size of the batches in seconds\n\n\n\n\n\n\nExample\n\n\n\n\n\nfrom pyspark.streaming import StreamingContext\nssc = StreamingContext(sc, 10)\nThe input streams associated with this context will be split in batches of 10 seconds.\n\n\n\nAfter a context is defined, the next steps are\n\nDefine the input sources by creating input Dstreams\nDefine the streaming computations by applying transformation and output operations to DStreams\n\n\n\nInput streams\nThe input Streams can be generated from different sources\n\nTCP socket, Kafka, Flume, Kinesis, Twitter.\nAlso a HDFS folder can be used as input stream. This option is usually used during the application development to perform a set of initial tests.\n\n\nInput: TCP socket\nA DStream can be associated with the content emitted by a TCP socket: socketTextStream(String hostname, int port_number) is used to create a DStream based on the textual content emitted by a TCP socket.\n\n\n\n\n\n\nExample\n\n\n\n\n\nlines = ssc.socketTextStream(\"localhost\", 9999)\nIt stores the content emitted by localhost:9999 in the lines DStream.\n\n\n\n\n\nInput: (HDFS) folder\nA DStream can be associated with the content of an input (HDFS) folder: every time a new file is inserted in the folder, the content of the file is stored in the associated DStream and processed. Pay attention that updating the content of a file does not trigger/change the content of the DStream. textFileStream(String folder) is used to create a DStream based on the content of the input folder.\n\n\n\n\n\n\nExample\n\n\n\n\n\nlines = textFileStream(inputFolder)\nStore the content of the files inserted in the input folder in the lines Dstream: every time new files are inserted in the folder their content is stored in the current batch of the stream.\n\n\n\n\n\nInput: other sources\nUsually DStream objects are defined on top of streams emitted by specific applications that emit real-time streaming data (e.g., Apache Kafka, Apache Flume, Kinesis, Twitter). It is also possible to write custom applications for generating streams of data, however Kafka, Flume and similar tools are usually a more reliable and effective solutions for generating streaming data.\n\n\n\nTransformations\nAnalogously to standard RDDs, also DStreams are characterized by a set of transformations that, when applied to DStream objects, return a new DStream Object. The transformation is applied on one batch (RDD) of the input DStream at a time and returns a batch (RDD) of the new DStream (i.e., each batch (RDD) of the input DStream is associated with exactly one batch (RDD) of the returned DStream). Many of the available transformations are the same transformations available for standard RDDs.\n\nBasic transformations\n\n\n\n\n\n\n\nTransformation\nEffect\n\n\n\n\nmap(func)\nIt returns a new DStream by passing each element of the source DStream through a function func.\n\n\nflatMap(func)\neach input item can be mapped to 0 or more output items. Returns a new DStream.\n\n\nfilter(func)\nIt returns a new DStream by selecting only the records of the source DStream on which func returns true.\n\n\nreduce(func)\nIt returns a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func. The function must be associative and commutative so that it can be computed in parallel. Note that the reduce method of DStreams is a transformation.\n\n\nreduceByKey(func)\nWhen called on a DStream of \\((K, V)\\) pairs, returns a new DStream of \\((K, V)\\) pairs where the values for each key are aggregated using the given reduce function.\n\n\ncombineByKey(...)\nwhen called on a DStream of \\((K, V)\\) pairs, returns a new DStream of \\((K, W)\\) pairs where the values for each key are aggregated using the given combine functions. The parameters are: createCombiner, mergeValue, and mergeCombiners\n\n\ngroupByKey()\nwhen called on a DStream of \\((K, V)\\) pairs, returns a new DStream of \\((K, \\text{Iterable<V>})\\) pairs where the values for each key is the concatenation of all the values associated with key \\(K\\) (i.e., It returns a new DStream by applying groupByKey on one batch (one RDD) of the input stream at a time).\n\n\ncountByValue()\nwhen called on a DStream of elements of type \\(K\\), returns a new DStream of \\((K, \\text{Long})\\) pairs where the value of each key is its frequency in each batch of the source Dstream. Note that the countByValue method of DStreams is a transformation.\n\n\ncount()\nIt returns a new DStream of single-element RDDs by counting the number of elements in each batch (RDD) of the source Dstream (i.e., it counts the number of elements in each input batch (RDD)). Note that the count method of DStreams is a transformation.\n\n\nunion(otherStream)\nIt returns a new DStream that contains the union of the elements in the source DStream and otherDStream.\n\n\njoin(otherStream)\nwhen called on two DStreams of \\((K, V)\\) and \\((K, W)\\) pairs, return a new DStream of \\((K, (V, W))\\) pairs with all pairs of elements for each key.\n\n\ncogroup(otherStream)\nwhen called on a DStream of \\((K, V)\\) and \\((K, W)\\) pairs, return a new DStream of \\((K, \\text{Seq}[V], \\text{Seq}[W])\\) tuples.\n\n\n\n\n\nBasic actions\nAction | Effect |\npprint() | It prints the first 10 elements of every batch of data in a DStream on the standard output of the driver node running the streaming application. It is useful for development and debugging |\nsaveAsTextFiles(prefix, [suffix]) | It saves the content of the DStream on which it is invoked as text files: one folder for each batch, and the folder name at each batch interval is generated based on prefix, time of the batch (and suffix): “prefix-TIME_IN_MS[.suffix]” (e.g., Counts.saveAsTextFiles(outputPathPrefix, \"\")). |\n\n\n\nStart and run the computations\nThe streamingContext.start() method is used to start the application on the input stream(s). The awaitTerminationOrTimeout(long millisecons) method is used to specify how long the application will run.\nThe awaitTermination() method is used to run the application forever\n\nUntil the application is explicitly killed\nThe processing can be manually stopped using streamingContext.stop()\n\n\nPoints to remember\n\nOnce a context has been started, no new streaming computations can be set up or added to it\nOnce a context has been stopped, it cannot be restarted\nOnly one StreamingContext per application can be active at the same time\nstop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to False\n\n\n\n\n\n\n\nExample: Spark Streaming version of word count\n\n\n\n\n\nProblem specification\n\nInput: a stream of sentences retrieved from localhost:9999\nTask:\n\nSplit the input stream in batches of 5 seconds each and print on the standard output, for each batch, the occurrences of each word appearing in the batch (i.e., execute the word count problem for each batch of 5 seconds)\nStore the results also in an HDFS folder\n\n\nfrom pyspark.streaming import StreamingContext\n\n## Set prefix of the output folders\noutputPathPrefix=\"resSparkStreamingExamples\"\n\n#Create a configuration object and#set the name of the applicationconf\nSparkConf().setAppName(\"Streaming word count\")\n\n## Create a Spark Context object\nsc = SparkContext(conf=conf)\n\n## Create a Spark Streaming Context object\nssc = StreamingContext(sc, 5)\n\n## Create a (Receiver) DStream that will connect to localhost:9999\nlines = ssc.socketTextStream(\"localhost\", 9999)\n\n## Apply a chain of transformations to perform the word count task\n## The returned RDDs are DStream RDDs\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwordsOnes = words.map(lambda word: (word, 1))\nwordsCounts = wordsOnes.reduceByKey(lambda v1, v2: v1+v2)\n\n## Print the result on the standard output\nwordsCounts.pprint()\n\n## Store the result in HDFS\nwordsCounts.saveAsTextFiles(outputPathPrefix, \"\")\n\n#Start the computation\nssc.start()\n\n## Run this application for 90 seconds\nssc.awaitTerminationOrTimeout(90)\nssc.stop(stopSparkContext=False)\n\n\n\n\n\n\n\nWindowed computation\nSpark Streaming also provides windowed computations, allowing to apply transformations over a sliding window of data: each window contains a set of batches of the inputstream, and windows can be overlapped (i.e., the same batch can be included in many consecutive windows).\nEvery time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream.\n\n\nGraphical example\n\n\n\nIn the example, the operationis applied over the last 3 time units of data (i.e., the last 3 batches of the input DStream), and each window contains the data of 3 batches. It slides by 2 time units.\n\nParameters\nAny window operation needs to specify two parameters\n\nWindow length: the duration of the window (3 in the example)\nSliding interval: the interval at which the window operation is performed (2 in the example)\n\nThese two parameters must be multiples of the batch interval of the source DStream.\n\n\n\n\n\n\nExample: word count and window\n\n\n\n\n\nProblem specification\n\nInput: a stream of sentences\nSplit the input stream in batches of 10 seconds\nDefine widows with the following characteristics\n\nWindow length: 20 seconds (i.e., 2 batches)\nSliding interval: 10 seconds (i.e., 1 batch)\n\nPrint on the standard output, for each window, the occurrences of each word appearing in the window (i.e., execute the word count problem for each window)\n\n\n\n\n\n\nStep one\n\n\n\n\n\n\n\n\n\nStep two\n\n\n\n\n\n\n\nWord count and windows\n\n\n\n\n\n\n\n\nBasic window transformations\n\n\n\n\n\n\n\n\n\nWindow transformation\nEffect\n\n\n\n\nwindow(windowLength, slideInterval)\nIt returns a new DStream which is computed based on windowed batches of the source DStream.\n\n\ncountByWindow(windowLength, slideInterval)\nIt returns a new single-element stream containing the number of elements of each window. The returned object is a Dstream of Long objects. However, it contains only one value for each window (the number of elements of the last analyzed window).\n\n\nreduceByWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration)\nIt returns a new single-element stream, created by aggregating elements in the stream over a sliding interval using func. The function must be associative and commutative so that it can be computed correctly in parallel. If invReduceFunc is not None, the reduction is done incrementally using the old window’s reduced value.\n\n\ncountByValueAndWindow(windowDuration , slideDuration)\nWhen called on a DStream of elements of type \\(K\\), it returns a new DStream of \\((K, \\text{Long})\\) pairs where the value of each key \\(K\\) is its frequency in each window of the source DStream.\n\n\nreduceByKeyAndWindow(func, invFunc, windowDuration, slideDuration=None, numPartitions=None)\nWhen called on a DStream of \\((K, V)\\) pairs, it returns a new DStream of \\((K, V)\\) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window. The window duration (length) is specified as a parameter of this invocation (windowDuration). Notice that, if slideDuration is None, the batchDuration of the StreamingContext object is used (i.e., 1 batch sliding window); if invFunc is provideved (is not None), the reduction is done incrementally using the old window’s reduced values (i.e., invFunc is used to apply an inverse reduce operation by considering the old values that left the window, e.g., subtracting old counts).\n\n\n\n\n\n\n\nCheckpoint\nA streaming application must operate 24/7 and hence must be resilient to failures unrelated to the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible, Spark Streaming needs to checkpoint enough information to a fault- tolerant storage system such that it can recover from failures, and this result is achieved by means of checkpoints, which are operations that store the data and metadata needed to restart the computation if failures happen. Checkpointing is necessary even for some window transformations and stateful transformations.\nCheckpointing is enabled by using the checkpoint(String folder) method of SparkStreamingContext: the parameter is the folder that is used to store temporary data. This is similar as for processing graphs with GraphFrames library, however, with GraphFrames, the checkpoint was the one of SparkContext.\n\n\n\n\n\n\nExample: word count and windows\n\n\n\n\n\nProblem specification\n\nInput: a stream of sentences retrieved from localhost:9999\nSplit the input stream in batches of 5 seconds\nDefine widows with the following characteristics\n\nWindow length: 15 seconds (i.e., 3 batches)\nSliding interval: 5 seconds (i.e., 1 batch)\n\nPrint on the standard output, for each window, the occurrences of each word appearing in the window (i.e., execute the word count problem for each window)\nStore the results also in an HDFS folder\n\nFirst solution\nfrom pyspark.streaming import StreamingContext\n\n## Set prefix of the output folders\noutputPathPrefix=\"resSparkStreamingExamples\"\n\n#Create a configuration object and#set the name of the applicationconf\nSparkConf().setAppName(\"Streaming word count\")\n\n## Create a Spark Context object\nsc = SparkContext(conf=conf)\n\n## Create a Spark Streaming Context object\nssc = StreamingContext(sc, 5)\n\n## Set the checkpoint folder (it is needed by some window transformations)\nssc.checkpoint(\"checkpointfolder\")\n\n## Create a (Receiver) DStream that will connect to localhost:9999\nlines = ssc.socketTextStream(\"localhost\", 9999)\n\n## Apply a chain of transformations to perform the word count task\n## The returned RDDs are DStream RDDs\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwordsOnes = words.map(lambda word: (word, 1))\n\n## reduceByKeyAndWindow is used instead of reduceByKey\n## The durantion of the window is also specified\nwordsCounts = wordsOnes \\\n    .reduceByKeyAndWindow(lambda v1, v2: v1+v2, None, 15)\n\n## Print the num. of occurrences of each word of the current window\n## (only 10 of them)\nwordsCounts.pprint()\n\n## Store the output of the computation in the folders with prefix\n## outputPathPrefix\nwordsCounts.saveAsTextFiles(outputPathPrefix, \"\")\n\n#Start the computation\nssc.start()\nssc.awaitTermination ()\nSecond solution\nfrom pyspark.streaming import StreamingContext\n\n## Set prefix of the output folders\noutputPathPrefix=\"resSparkStreamingExamples\"\n\n#Create a configuration object and#set the name of the applicationconf\nSparkConf().setAppName(\"Streaming word count\")\n\n## Create a Spark Context object\nsc = SparkContext(conf=conf)\n\n## Create a Spark Streaming Context object\nssc = StreamingContext(sc, 5)\n\n## Set the checkpoint folder (it is needed by some window transformations)\nssc.checkpoint(\"checkpointfolder\")\n\n## Create a (Receiver) DStream that will connect to localhost:9999\nlines = ssc.socketTextStream(\"localhost\", 9999)\n\n## Apply a chain of transformations to perform the word count task\n## The returned RDDs are DStream RDDs\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwordsOnes = words.map(lambda word: (word, 1))\n\n## reduceByKeyAndWindow is used instead of reduceByKey\n## The durantion of the window is also specified\nwordsCounts = wordsOnes \\\n    .reduceByKeyAndWindow(\n        lambda v1, v2: v1+v2, \n        #lambda vnow, # <1>\n        vold: vnow-vold, 15\n    )\n\n## Print the num. of occurrences of each word of the current window\n## (only 10 of them)\nwordsCounts.pprint()\n\n## Store the output of the computation in the folders with prefix\n## outputPathPrefix\nwordsCounts.saveAsTextFiles(outputPathPrefix, \"\")\n\n#Start the computation\nssc.start()\n\n## Run this application for 90 seconds\nssc.awaitTerminationOrTimeout(90)\nssc.stop(stopSparkContext=False)\n\nIn this solution the inverse function is also specified in order to compute the result incrementally\n\n\n\n\n\n\n\nStateful computation\n\nupdateStateByKey transformation\nThe updateStateByKey transformation allows maintaining a state for each key. The value of the state of each key is continuously updated every time a new batch is analyzed.\nThe use of updateStateByKey is based on two steps\n\nDefine the state: the data type of the state associated with the keys can be an arbitrary data type\nDefine the state update function: specify with a function how to update the state of a key using the previous state and the new values from an input stream associated with that key\n\nIn every batch, Spark will apply the state update function for all existing keys. For each key, the update function is used to update the value associated with a key by combining the former value and the new values associated with that key; in other words, for each key, the call method of the function is invoked on the list of new values and the former state value and returns the new aggregated value for the considered key.\n\n\n\n\n\n\nExample: word count and updateStateByKey transformation\n\n\n\n\n\nBy using the updateStateByKey, the application can continuously update the number of occurrences of each word. The number of occurrences stored in the DStream returned by this transformation is computed over the union of all the batches (from the first one to the current one). For efficiency reasons, the new value for each key is computed by combining the last value for that key with the values of the current batch for the same key.\nProblem specification:\n\nInput: a stream of sentences retrieved from localhost:9999\nSplit the input stream in batches of 5 seconds\nPrint on the standard output, every 5 seconds, the occurrences of each word appearing in the stream (from time 0 to the current time) (i.e., execute the word count problem from the beginning of the stream to current time)\nStore the results also in an HDFS folder\n\nfrom pyspark.streaming import StreamingContext\n\n## Set prefix of the output folders\noutputPathPrefix=\"resSparkStreamingExamples\"\n\n#Create a configuration object and#set the name of the applicationconf\nSparkConf().setAppName(\"Streaming word count\")\n\n## Create a Spark Context object\nsc = SparkContext(conf=conf)\n\n## Create a Spark Streaming Context object\nssc = StreamingContext(sc, 5)\n\n## Set the checkpoint folder (it is needed by some window transformations)\nssc.checkpoint(\"checkpointfolder\")\n\n## Create a (Receiver) DStream that will connect to localhost:9999\nlines = ssc.socketTextStream(\"localhost\", 9999)\n\n## Apply a chain of transformations to perform the word count task\n## The returned RDDs are DStream RDDs\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwordsOnes = words.map(lambda word: (word, 1))\n\n## Define the function that is used to update the state of a key at a time\n#def updateFunction(newValues, currentCount): # <1>\n    if currentCount is None:\n        currentCount = 0\n\n    ## Sum the new values to the previous state for the current key\n    #return sum(newValues, currentCount) # <2>\n\n## DStream made of cumulative counts for each key that get updated \n## in every batch\n#totalWordsCounts = wordsOnes.updateStateByKey(updateFunction) # <3>\n\n## Print the num. of occurrences of each word of the current window\n## (only 10 of them)\ntotalWordsCounts.pprint()\n\n## Store the output of the computation in the folders with prefix\n## outputPathPrefix\ntotalWordsCounts.saveAsTextFiles(outputPathPrefix, \"\")\n\n#Start the computation\nssc.start()\n\n## Run this application for 90 seconds\nssc.awaitTerminationOrTimeout(90)\nssc.stop(stopSparkContext=False)\n\ncurrentCount: current state/value for the current key\nnewValues: list of new integer values for the current key\nsum(newValues, currentCount): Combine current state and new values\nupdateFunction: this function is invoked one time for each key\n\n\n\n\n\n\n\nTransform transformation\nSome types of transformations are not available for DStreams (e.g., sortBy(), sortByKey(), distinct()), moreover, sometimes it is needed to combine DStreams and RDDs. For example, the functionality of joining every batch in a data stream with another dataset (a standard RDD) is not directly exposed in the DStream API. The transform() transformation can be used in these situations.\n\n\n\n\n\n\n\nTransformation\nEffect\n\n\n\n\ntransform(func)\nIt is a specific transformation of DStreams that returns a new DStream by applying an RDD-to-RDD function to every RDD of the source Dstream. This can be used to apply arbitrary RDD operations on the DStream\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nProblem specification\n\nInput: a stream of sentences retrieved from localhost:9999\nSplit the input stream in batches of 5 seconds each and print on the standard output, for each batch, the occurrences of each word appearing in the batch. The pairs must be returned/displayed sorted by decreasing number of occurrences (per batch)\nStore the results also in an HDFS folder\n\nfrom pyspark.streaming import StreamingContext\n\n## Set prefix of the output folders\noutputPathPrefix=\"resSparkStreamingExamples\"\n\n#Create a configuration object and#set the name of the applicationconf\nSparkConf().setAppName(\"Streaming word count\")\n\n## Create a Spark Context object\nsc = SparkContext(conf=conf)\n\n## Create a Spark Streaming Context object\nssc = StreamingContext(sc, 5)\n\n## Create a (Receiver) DStream that will connect to localhost:9999\nlines = ssc.socketTextStream(\"localhost\", 9999)\n\n## Apply a chain of transformations to perform the word count task\n## The returned RDDs are DStream RDDs\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwordsOnes = words.map(lambda word: (word, 1))\nwordsCounts = wordsOnes.reduceByKey(lambda v1, v2: v1+v2)\n\n## Sort the content/the pairs by decreasing value (# of occurrences)\nwordsCountsSortByKey = wordsCounts \\\n    .transform(lambda batchRDD: batchRDD.sortBy(lambda pair: -1*pair[1]))\n\n## Print the result on the standard output\nwordsCountsSortByKey.pprint()\n\n## Store the result in HDFS\nwordsCountsSortByKey.saveAsTextFiles(outputPathPrefix, \"\")\n\n#Start the computation\nssc.start()\n\n## Run this application for 90 seconds\nssc.awaitTerminationOrTimeout(90)\nssc.stop(stopSparkContext=False)"
  },
  {
    "objectID": "22_structured_streaming.html",
    "href": "22_structured_streaming.html",
    "title": "27  Spark structured streaming",
    "section": "",
    "text": "What is Spark structured streaming?\nStructured Streaming is a scalable and fault-tolerant stream processing engine that is built on the Spark SQL engine, and input data are represented by means of (streaming) DataFrames. Structured Streaming uses the existing Spark SQL APIs to query data streams (the same methods used for analyzing static DataFrames).\nA set of specific methods are used to define\n\nInput and output streams\nWindows\n\n\nInput data model\nEach input data stream is modeled as a table that is being continuously appended: every time new data arrive they are appended at the end of the table (i.e., each data stream is considered an unbounded input table).\nNew input data in the stream are new rows appended to an unbounded table\n\n\nInput stream\n\n\n\n\n\nQueries\nThe expressed queries are incremental queries that are run incrementally on the unbounded input tables.\n\nThe arrive of new data triggers the execution of the incremental queries\nThe result of a query at a specific timestamp is the one obtained by running the query on all the data arrived until that timestamp (i.e., stateful queries are executed).\nAggregation queries combine new data with the previous results to optimize the computation of the new results.\n\nThe queries can be executed\n\nAs micro-batch queries with a fixed batch interval: this is the standard behavior, with exactly-once fault-tolerance guarantees\nAs continuous queries: this is experimental behavior, with at-least-once fault-tolerance guarantees\n\nIn this example the (micro-batch) query is executed every 1 second\n\n\nQuery execution\n\n\n\nNote that every time the query is executed, all data received so far are considered.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nInput\n\nA stream of records retrieved from localhost:9999\nEach input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format stationId,# free slots,#used slots,timestamp\n\nFor each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0\n\nPrint the requested information when new data are received by using the micro-batch processing mode\nSuppose the batch-duration is set to 2 seconds\n\n\n\n\nExample execution\n\n\n\n\n\n\n\n\n\nKey concepts\n\nInput sources\nTransformations\nOutputs\n\nExternal destinations/sinks\nOutput Modes\n\nQuery run/execution\nTriggers\n\n\nInput sources\n\nFile source\n\nReads files written in a directory as a stream of data\nEach line of the input file is an input record\nSupported file formats are text, csv, json, orc, parquet, …\n\nKafka source\n\nReads data from Kafka\nEach Kafka message is one input record\n\nSocket source (for debugging purposes)\n\nReads UTF8 text data from a socket connection\nThis type of source does not provide end-to-end fault-tolerance guarantees\n\nRate source (for debugging purposes)\n\nGenerates data at the specified number of rows per second\nEach generated row contains a timestamp and value of type long\n\n\nThe readStream property of the SparkSession class is used to create DataStreamReaders: the methods format() and option() of the DataStreamReader class are used to specify the input streams (e.g., type, location). The method load() of the DataStreamReader class is used to return DataFrames associated with the input data streams.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn this example the (streaming) DataFrame recordsDF is created and associated with the input stream of type socket\n\nAddress: localhost\nInput port: 9999\n\nrecordsDF = spark.readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load() # <1>\n\nTest\n\n\n\n\n\n\nTransformations\nTransformations are the same of DataFrames, however there are restrictions on some types of queries/transformations that cannot be executed incrementally.\nUnsupported operations:\n\nMultiple streaming aggregations (i.e. a chain of aggregations on a streaming DataFrame)\nLimit and take first N rows\nDistinct operations\nSorting operations are supported on streaming DataFrames only after an aggregation and in complete output mode\nFew types of outer joins on streaming DataFrames are not supported\n\n\n\nOutputs\n\nSinks: they are instances of the class DataStreamWriter and are used to specify the external destinations and store the results in the external destinations\nFile sink: it stores the output to a directory; supported file formats are text, csv, json, orc, parquet, …\nKafka sink: it stores the output to one or more topics in Kafka\nForeach sink: it runs arbitrary computation on the output records\nConsole sink (for debugging purposes): it prints the computed output to the console every time a new batch of records has been analyzed; this should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after each computation\nMemory sink (for debugging purposes): the output is stored in memory as an in-memory table; this should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory\n\n\nOutput mods\nWe must define how we want Spark to write output data in the external destinations. The supported output modes depend on the query type, and the possible output mods are the following\n\nAppend\nThis is the default mode. Only the new rows added to the computed result since the last trigger (computation) will be outputted. This mode is supported for only those queries where rows added to the result is never going to change: this mode guarantees that each row will be output only once. So, the supported queries are only select, filter, map, flatMap, filter, join, etc.\n\n\nComplete\nThe whole computed result will be outputted to the sink after every trigger (computation). This mode is supported for aggregation queries.\n\n\nUpdate\nOnly the rows in the computed result that were updated since the last trigger (computation) will be outputted.\nThe complete list of supported output modes for each query type is available in the Apache Spark documentation.\n\n\n\nCode\nThe writeStream property of the SparkSession class is used to create DataStreamWriters. The methods outputMode(), format(), and option() of the DataStreamWriter class are used to specify the output destination (data format, location, output mode, etc.).\n\n\n\n\n\n\nNote\n\n\n\n\n\nExample The DataStreamWriter “streamWriterRes” is created and associated with the console. The output mode is set to append.\nstreamWriterRes = stationIdTimestampDF \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\")\n\n\n\n\n\n\nQuery run/execution\nTo start executing the defined queries/structured streaming applications you must explicitly invoke the start() action on the defined sinks (DataStreamWriter objects associated with the external destinations in which the results will be stored). It is possible to start several queries in the same application, and structured streaming queries run forever (they must be explicitly stop/kill).\n\n\nTriggers\nFor each Spark structured streaming query it is possible to specify when new input data must be processed, and whether the query is going to be executed as a micro-batch query with a fixed batch interval or as a continuous processing query (experimental). The trigger type for each query is specified by means of the trigger() method of the DataStreamWriter class.\n\nTrigger types\nNo trigger type is explicitly specified, by default the query will be executed in micro-batch mode, where each micro-batch is generated and processed as soon as the previous micro-batch has been processed.\n\nFixed interval micro-batches\nThe query will be executed in micro-batch mode. Micro-batches will be processed at the user-specified intervals: the parameter processingTime of the trigger method() is used to specify the micro-batch size, and, if the previous micro-batch completes within its interval, then the engine will wait until the interval is over before processing the next micro-batch; if the previous micro-batch takes longer than the interval to complete (i.e. if an interval boundary is missed), then the next micro-batch will start as soon as the previous one completes.\n\n\nOne-time micro-batch\nThe query will be executed in micro-batch mode, but the query will be executed only one time on one single micro-batch containing all the available data of the input stream; after the single execution the query stops on its own. This trigger type is useful when the goal is to periodically spin up a cluster, process everything that is available since the last period, and then shutdown the cluster. In some case, this may lead to significant cost savings.\n\n\nContinuous with fixed checkpoint interval (experimental)\nThe query will be executed in the new low-latency, continuous processing mode. It offers at-least-once fault-tolerance guarantees.\n\n\n\n\nSpark structured streaming examples\n\nExample 1\n\nInput\n\nA stream of records retrieved from localhost:9999\nEach input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”\n\nOutput\n\nFor each input reading with a number of free slots equal to 0 print on the standard output the value of stationId and timestamp\nUse the standard micro-batch processing mode\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import split\n\n## Create a \"receiver\" DataFrame that will connect to localhost:9999\nrecordsDF = spark.readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n\n## The input records are characterized by one single column called value\n## of type string\n## Example of an input record: s1,0,3,2016-03-11 09:00:04\n## Define four more columns by splitting the input column value\n## New columns:\n## - stationId\n## - freeslots\n## - usedslots\n## - timestamp\nreadingsDF = recordsDF \\\n    .withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\")) \\ \n    .withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\")) \\\n    .withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\")) \\\n    #.withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\")) # <1>\n\n## Filter data\n## Use the standard filter transformation\n#fullReadingsDF = readingsDF.filter(\"freeslots=0\") # <2>\n\n## Select stationid and timestamp\n## Use the standard select transformation\n#stationIdTimestampDF = fullReadingsDF.select(\"stationId\", \"timestamp\") # <2>\n\n## The result of the structured streaming query will be stored/printed on\n## the console \"sink“.\n## append output mode\nqueryFilterStreamWriter = stationIdTimestampDF \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\")\n\n## Start the execution of the query (it will be executed until it is explicitly stopped)\nqueryFilter = queryFilterStreamWriter.start()\n\nwithColumn() is used to add new columns (it is a standard DataFrame method). It returns a DataFrame with the same columns of the input DataFrame and the new defined column. For each new column it is possible to specify name (e.g. “stationId”) and the SQL function that is used to define its value in each record. The cast() method is used to specify the data type of each defined column.\nfilter and select are standard DataFrame transformations\n\n\n\nExample 2\n\nInput\n\nA stream of records retrieved from localhost:9999\nEach input record is a reading about the status of a station of\nbike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”\n\nOutput\n\nFor each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0\nPrint the requested information when new data are received by using the standard micro-batch processing mode\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import split\n\n## Create a \"receiver\" DataFrame that will connect to localhost:9999\nrecordsDF = spark.readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n\n## The input records are characterized by one single column called value\n## of type string\n## Example of an input record: s1,0,3,2016-03-11 09:00:04\n## Define four more columns by splitting the input column value\n## New columns:\n## - stationId\n## - freeslots\n## - usedslots\n## - timestamp\nreadingsDF = recordsDF \\\n    .withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\")) \\\n    .withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\")) \\\n    .withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\")) \\\n    .withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\"))\n\n## Filter data\n## Use the standard filter transformation\nfullReadingsDF = readingsDF.filter(\"freeslots=0\")\n\n## Count the number of readings with a number of free slots equal to 0\n## for each stationId\n## The standard groupBy method is used\ncountsDF = fullReadingsDF \\\n    .groupBy(\"stationId\") \\\n    #.agg({\"*\":\"count\"}) # <1>\n\n## The result of the structured streaming query will be stored/printed on\n## the console \"sink\"\n## complete output mode\n## (append mode cannot be used for aggregation queries)\nqueryCountStreamWriter = countsDF \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\")\n\n## Start the execution of the query (it will be executed until it is explicitly stopped)\nqueryCount = queryCountStreamWriter.start()\n\ngroupBy and agg are standard DataFrame transformations\n\n\n\n\n\nEvent time and window operations\nInput streaming records are usually characterized by a time information: it is usually called event-time, and it is the time when the data was generated. For many applications, you want to operate by taking into consideration the event-time and windows containing data associated with the same event-time range.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCompute the number of events generated by each monitored IoT device every minute based on the event-time. For each window associated with one distinct minute, consider only the data with an event-time inside that minute/window and compute the number of events for each IoT device: one computation for each minute/window. You want to use the time when the data was generated (i.e., the event-time) rather than the time Spark receives them.\n\n\n\nSpark allows defining windows based on the time-event input column, and then apply aggregation functions over each window.\nFor each structured streaming query on which you want to apply a window computation you must specify\n\nthe name of the time-event column in the input (streaming) DataFrame\nthe characteristics of the (sliding) windows\n\nwindowDuration\nslideDuration\n\n\nDo not set it if you want non-overlapped windows, (i.e., if you want to a slideDuration equal to windowDuration). You can set different window characteristics for each query of your application.\nThe window(timeColumn, windowDuration, slideDuration=None) function is used inside the standard groupBy() one to specify the characteristics of the windows. Notice that windows can be used only with queries that are applying aggregation functions.\n\nEvent time and window operations: example 1\n\nInput\n\nA stream of records retrieved from localhost:9999\nEach input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”\ntimestamp is the event-time column\n\nOutput\n\nFor each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window\nThe query is executed for each window\nSet windowDuration to 2 seconds and no slideDuration (i.e., non-overlapped windows)\n\n\n\n\nStep 1\n\n\n\nThe returned result has a column called window. It contains the time slot associated with the window \\([\\text{from timestamp}, \\text{to timestamp})\\)\n\n\nStep 2\n\n\n\n\n\nStep 3\n\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import window\n\n## Create a \"receiver\" DataFrame that will connect to localhost:9999\nrecordsDF = spark.readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n\n## The input records are characterized by one single column called value\n## of type string\n## Example of an input record: s1,0,3,2016-03-11 09:00:04\n## Define four more columns by splitting the input column value\n## New columns:\n## - stationId\n## - freeslots\n## - usedslots\n## - timestamp\nreadingsDF = recordsDF \\\n    .withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\")) \\\n    .withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\")) \\\n    .withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\")) \\\n    .withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\"))\n\n## Filter data\n## Use the standard filter transformation\nfullReadingsDF = readingsDF.filter(\"freeslots=0\")\n\n## Count the number of readings with a number of free slots equal to 0\n## for each stationId in each window.\n## windowDuration = 2 seconds\n## no overlapping windows\ncountsDF = fullReadingsDF \\\n    .groupBy(window(fullReadingsDF.timestamp, \"2 seconds\"), \"stationId\") \\\n    .agg({\"*\":\"count\"}) \\\n    .sort(\"window\")\n\n## The result of the structured streaming query will be stored/printed on\n## the console \"sink\"\n## complete output mode\n## (append mode cannot be used for aggregation queries)\nqueryCountWindowStreamWriter = countsDF \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .option(\"truncate\", \"false\")\n\n## Start the execution of the query (it will be executed until it is explicitly stopped)\nqueryCountWindow = queryCountWindowStreamWriter.start()\n\n\nLate data\nSparks handles data that have arrived later than expected based on its event-time; these are called late data. Spark has full control over updating old aggregates when there are late data: every time new data are processed the result is computed by combining old aggregate values and the new data by considering the event-time column instead of the time Spark receives the data.\n\nLate data example\n\nInput\n\nA stream of records retrieved from localhost:9999\nEach input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”\ntimestamp is the event-time column\n\nOutput\n\nFor each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window\nThe query is executed for each window\nSet windowDuration to 2 seconds and no slideDuration (i.e., non-overlapped windows)\n\n\n\n\nStep 1\n\n\n\n\n\nStep 2\n\n\n\n\n\nStep 3\n\n\n\nNotice that late data that was generated at 2016-03-11 09:00:06, but arrived at 2016-03-11 09:00:08: the result consider also late data and assign them to the right window by considering the event-time information.\nThe code is the same of the previous example (Event time and window operations: example 1): late data are automatically handled by Spark.\n\n\n\nEvent time and window operations: example 2\n\nInput\n\nA stream of records retrieved from localhost:9999\nEach input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”\ntimestamp is the event-time column\n\nOutput\n\nFor each window, print on the standard output the total number of received input readings with a number of free slots equal to 0\nThe query is executed for each window\nSet windowDuration to 2 seconds and no slideDuration (i.e., non-overlapped windows)\n\n\n\n\nStep 1\n\n\n\n\n\nStep 2\n\n\n\n\n\nStep 3\n\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import window\n\n## Create a \"receiver\" DataFrame that will connect to localhost:9999\nrecordsDF = spark.readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n\n## The input records are characterized by one single column called value\n## of type string\n## Example of an input record: s1,0,3,2016-03-11 09:00:04\n## Define four more columns by splitting the input column value\n## New columns:\n## - stationId\n## - freeslots\n## - usedslots\n## - timestamp\nreadingsDF = recordsDF \\\n    .withColumn(\"stationId\", split(recordsDF.value, ',')[0].cast(\"string\")) \\\n    .withColumn(\"freeslots\", split(recordsDF.value, ',')[1].cast(\"integer\")) \\\n    .withColumn(\"usedslots\", split(recordsDF.value, ',')[2].cast(\"integer\")) \\\n    .withColumn(\"timestamp\", split(recordsDF.value, ',')[3].cast(\"timestamp\"))\n\n## Filter data\n## Use the standard filter transformation\nfullReadingsDF = readingsDF.filter(\"freeslots=0\")\n\n## Count the number of readings with a number of free slots equal to 0\n## for in each window.\n## windowDuration = 2 seconds\n## no overlapping windows\ncountsDF = fullReadingsDF \\\n    .groupBy(window(fullReadingsDF.timestamp, \"2 seconds\")) \\\n    .agg({\"*\":\"count\"}) \\\n    .sort(\"window\")\n\n## The result of the structured streaming query will be stored/printed on\n## the console \"sink\"\n## complete output mode\n## (append mode cannot be used for aggregation queries)\nqueryCountWindowStreamWriter = countsDF \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .option(\"truncate\", \"false\")\n\n## Start the execution of the query (it will be executed until it is \n## explicitly stopped)\nqueryCountWindow = queryCountWindowStreamWriter.start()\n\n\nWatermarking\nWatermarking is a feature of Spark that allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state. Results related to old event-times are not needed in many real streaming applications: they can be dropped to improve the efficiency of the application, since keeping the state of old results is resource expensive; in this way every time new data are processed only recent records are considered.\nSpecifically, to run windowed queries for days, it is necessary for the system to bound the amount of intermediate in-memory state it accumulates. This means the system needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more; to enable this, in Spark 2.1, watermarking has been introduced.\nWatermarking lets the Spark Structured Streaming engine automatically track the current event time in the data and attempt to clean up old state accordingly. It allows to define the watermark of a query by specifying the event time column and the threshold on how late the data is expected to be in terms of event time: for a specific window ending at time \\(T\\), the engine will maintain state and allow late data to update the state/the result until max event time seen by the engine \\(< T + \\text{late threshold}\\). In other words, late data within the threshold will be aggregated, but data later than \\({T + \\text{threshold}}\\) will be dropped.\n\n\n\nJoin operations\nSpark Structured Streaming manages also join operations\n\nBetween two streaming DataFrames\nBetween a streaming DataFrame and a static DataFrame\n\nThe result of the streaming join is generated incrementally.\nWhen joining between two streaming DataFrames, for both input streams, past input streaming data must be buffered/recorded in order to be able to match every future input record with past input data and accordingly generate joined results. Too many resources are needed for storing all the input data, hence, old data must be discarded. Watermark thresholds must be defined on both input streams such that the engine knows how delayed the input can be and drop old data.\nThe methods join() and withWatermark() are used to join streaming DataFrames: the join method is similar to the one available for static DataFrame.\nfrom pyspark.sql.functions import expr\nimpressions = spark.readStream. ...\nclicks = spark.readStream. ...\n\n## Apply watermarks on event-time columns\nimpressionsWithWatermark = impressions \\\n    .withWatermark(\"impressionTime\", \"2 hours\")\n    \nclicksWithWatermark = clicks \\\n    .withWatermark(\"clickTime\", \"3 hours\")\n\n## Join with event-time constraints\nimpressionsWithWatermark.join(\n    clicksWithWatermark,\n    expr(\n        \"\"\"\n        clickAdId = impressionAdId AND \n        clickTime >= impressionTime AND\n        clickTime <= impressionTime + interval 1 hour\"\"\"\n    ) \n)"
  },
  {
    "objectID": "23_streaming_frameworks.html",
    "href": "23_streaming_frameworks.html",
    "title": "28  Streaming data analytics frameworks",
    "section": "",
    "text": "Stream processing frameworks for (big) streaming data analytics\nSeveral frameworks have been proposed to process in real-time or in near real-time data streams:\n\nApache Spark (Streaming component)\nApache Storm\nApache Flink\nApache Samza\nApache Apex\nApache Flume\nAmazon Kinesis Streams\n…\n\nAll these frameworks use a cluster of servers to scale horizontally with respect to the (big) amount of data to be analyzed.\n\nComparison among state of the art streaming frameworks\nApache Spark Streaming\n\nMicro-batch applications\nProcesses each record exactly once\n\nApache Storm\n\nContinuous/real-time computation: very low latency\nProcesses each record at least once in real-time: each record could be processed multiple times, hence may update mutable state twice\nApache Storm Trident API: it is a running modality of Apache Storm that processes each record exactly once (micro-batch); slower than the Apache Storm version\n\nApache Flink\n\nContinuous/real-time stateful computations over data streams: low latency\nProcesses each record exactly once\n\n\n\n\nIntroduction to Apache Storm\nApache Storm™ is a distributed framework that is used for real-time processing of data streams (e.g., Tweets analysis, Log processing). Currently, it is an open source project of the Apache Software Foundation. It is implemented in Clojure and Java (12 core committers, plus about 70 contributors).\nStorm was first developed by Nathan Marz at BackType, a company that provided social search applications. Later (2011), BackType was acquired by Twitter, and now it is a critical part of their infrastructure. Currently, Storm is a project of the Apache Software Foundation (since 2013).\n\nData processing\n\nContinuous computation: Storm can do continuous computation on data streams in real time; it can process each message as it comes (an example of continuous computation is streaming trending topics detection on Twitter)\nReal-time analytics: Storm can analyze and extract insights or complex knowledge from data that come from several real-time data streams\n\n\n\nFeatures of Storm\nStorm is\n\nDistributed: Storm is a distributed system than can run on a cluster of commodity servers.\nHorizontally scalable: Storm allows adding more servers (nodes) to your Storm cluster and increase the processing capacity of your application. It is linearly scalable with respect to the number of nodes, which means that you can double the processing capacity by doubling the nodes.\nFast: Storm has been reported to process up to 1 million tuples per second per node.\nFault tolerant: Units of work are executed by worker processes in a Storm cluster. When a worker dies, Storm will restart that worker (on the same node or on to another node).\nReliable - Guaranteed data processing: Storm provides guarantees that each message (tuple) will be processed at least once; in case of failures, Storm will replay the lost tuples, and it can be configured to process each tuple only once.\nEasy to operate: Storm is simple to deploy and manage. Once the cluster is deployed, it requires little maintenance.\nProgramming language agnostic: Even though the Storm platform runs on Java Virtual Machine, the applications that run over it can be written in any programming language that can read and write to standard input and output streams.\n\n\n\n\nStorm core concepts\nStorm can be considered a distributed Function Programming-like processing of data streams. It applies a set of functions, in a specific order, on the elements of the input data streams and emits new data streams, however, each function can store its state by means of variables, and so it is not pure functional programming.\n\nMain concepts\n\nTuple\nData Stream\nSpout\nBolt\nTopology\n\n\n\nData model\nThe basic unit of data that can be processed by a Storm application is called a tuple: each tuple is a predefined list of fields. The data type of each field can be common data types, (e.g., byte, char, string, integer), or your own data types, which can be serialized as fields in a tuple. Each field of a tuple has a name.\nA tuple is dynamically typed, that is, you just need to define the names of the fields in a tuple and not their data type.\nStorm processes streams of tuples. Each stream\n\nis an unbounded sequence of tuples\nhas a name\nis composed of homogenous tuples (i.e., tuples with the same structure)\n\nHowever, each applications can process multiple, heterogonous, data streams.\n\n\n\n\n\n\nExample\n\n\n\n\n\nTuple\n(1.1.1.1,\"foo.com\")\nStream of tuples\n(1.1.1.1,\"foo.com\")\n(2.2.2.2,\"bar.net\")\n(3.3.3.3,\"foo.com\")\n...\n\n\n\n\n\nSpout\nSpout is the component generating/handling the input data stream. Spouts read or listen to data from external sources and publish them (emit in Storm terminology) into streams.\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nA spout can be used to connect to the Twitter API and emit a stream of tweets\nA spout can be used to read a log file and emit a stream of composed of the its lines\n\n\n\n\nEach spout can emit multiple streams, with different schemas; for example, it is possible to implement a spout that reads 10-field records from a log file and emits them as two different streams of 7-tuples and 4-tuples, respectively.\nSpouts can be\n\nunreliable (fire-and-forget)\nreliable (can replay failed tuples)\n\n\n\nBolt\nBolt is the component that is used to apply a function over each tuple of a stream. Bolts consume one or more streams, emitted by spouts or other bolts, and potentially produce new streams.\nBolts can be used to\n\nfilter or transform the content of the input streams and emit new data streams that will be processed by other bolts\nprocess the data streams and store/persist the result of the computation in some of “storage” (files, Databases, ..)\n\nEach bolt can emit multiple streams, with different schemas.\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nA bolt can be used to extract one field from each tuple of its input stream\nA bolt can be used to join two streams, based on a common field\nA bolt can be used to count the occurrences of a set of URLs\n\n\n\n\nThe input streams of a Storm cluster are handled by spouts\n\nEach spout passes the data streams to bolts, which transform them in some way\nEach bolt either persists the data in some sort of storage or passes it to some other bolts\n\nA Storm program is a chain of bolts making some computations/transformations on the data exposed by spouts and bolts.\n\n\nTopology\nA Storm topology is an abstraction that defines the graph of the computation: it specifies which spouts and bolts are used and how they are connected. A topology can be represented by a direct acyclic graph (DAG), where each node does some kind of processing and eventually forwards it to the next node(s) in the flow (i.e., a topology in Storm wires data and functions via a DAG).\n\n\nTopology example\n\n\n\n\n\nFunctional programming\n\n\n\n\n\nTopology example\n\n\n\n\nNotice that there are two input data streams in this topology\nThere are also two output data streams\n\n\nTopology execution\nThe topology is executed on the servers of the cluster running Storm. The system automatically decides which parts of the topology are executed by each server of the cluster\n\nEach topology runs until its is explicitly killed\nEach cluster can runs multiple topologies at the same time\n\n\nWorker processes\n\nEach node in the cluster can run one or more JVMs called worker processes that are responsible for processing a part of the topology.\nEach topology executes across one or more worker processes\nEach worker process is bound to one of the topologies and can execute multiple components (spouts and/or bolts) of that topology; hence, even if multiple topologies are run at the same time, none of them will share any of the workers\n\n\n\nExecutor\n\nWithin each worker process, there can be multiple threads that execute parts of the topology. Each of these threads is called an executor\nAn executor can execute only one of the components of the topology, that is, any one spout or bolt in the topology, but it may run one or more tasks for the same component\nEach spout or bolt can be associated with many executors and hence executed in parallel\n\n\n\nTasks\nA task is the most granular unit of task execution in Storm: each task is an instance of a spout or bolt, and it performs the actual data processing\n\nEach spout or bolt that you implement in your code executes as many tasks across the cluster\nEach task can be executed alone or with another task of the same type (in the same executor)\n\nThe number of tasks for a component is always the same throughout the lifetime of a topology (it is set when the topology is submitted), but the number of executors (threads) for a component can change over time (i.e., it is possible to add/remove executors for each component).\nThe parallelism of the topology is given by the number of executors (i.e., number of threads). For each spout/bolt the application can specify\n\nThe number of executors: this value can be changed at runtime\nThe number of tasks: this value is set before submitting the topology and cannot be change at runtime"
  }
]