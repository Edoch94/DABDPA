[
  {
    "objectID": "00_01_intro.html#who-generates-big-data",
    "href": "00_01_intro.html#who-generates-big-data",
    "title": "2  Introduction to Big data",
    "section": "Who generates Big Data",
    "text": "Who generates Big Data\n\nUser generated content: social networks (web and mobile)\nHealth and scientific computing\nLog files: web server log files, machine system log files\nInternet of Things (IoT): sensor networks, RFIDs, smart meters\n\n\nExample of Big Data at work\n\n\nBigdata example"
  },
  {
    "objectID": "00_01_intro.html#the-five-vs-of-big-data",
    "href": "00_01_intro.html#the-five-vs-of-big-data",
    "title": "2  Introduction to Big data",
    "section": "The five Vs of Big Data",
    "text": "The five Vs of Big Data\n\nVolume: scale of data\nVariety: different forms of data\nVelocity: analysis of streaming data\nVeracity: uncertainty of data\nValue: exploit information provided by data"
  },
  {
    "objectID": "00_01_intro.html#the-bottleneck-and-the-solution",
    "href": "00_01_intro.html#the-bottleneck-and-the-solution",
    "title": "2  Introduction to Big data",
    "section": "The bottleneck and the solution",
    "text": "The bottleneck and the solution\n\nBottleneck\n\nProcessors process data\nHard drives store data\nWe need to transfer data from the disk to the processor\n\n\n\nSolution\n\nTransfer the processing power to the data\nMultiple distributed disks: each one holding a portion of a large dataset\nProcess in parallel different file portions from different disks"
  },
  {
    "objectID": "02_architectures.html#definition-of-big-data-architecture",
    "href": "02_architectures.html#definition-of-big-data-architecture",
    "title": "3  Big data architectures",
    "section": "Definition of Big data architecture",
    "text": "Definition of Big data architecture\n\n\n\n\n\n\nFrom Data Architecture Guide in Microsoft Learn\n\n\n\nA big data architecture is designed to handle the ingestion, processing, and analysis of data that is too large or complex for traditional database systems…\nBig data solutions typically involve one or more of the following types of workload:\n\nBatch processing of big data sources at rest\nReal-time processing of big data in motion\nInteractive exploration of big data\nPredictive analytics and machine learning\n\nConsider big data architectures when you need to:\n\nStore and process data in volumes too large for a traditional database\nTransform unstructured data for analysis and reporting\nCapture, process, and analyze unbounded streams of data in real time, or with low latency"
  },
  {
    "objectID": "02_architectures.html#lambda-architecture",
    "href": "02_architectures.html#lambda-architecture",
    "title": "3  Big data architectures",
    "section": "Lambda architecture",
    "text": "Lambda architecture\nThe most frequently used big data architecture is the Lambda Architecture. The lambda architecture was proposed by Nathan Marz in 2011.\n\nDefinitions\n\n\n\n\n\n\nFrom Nathan Marz\n\n\n\nThe past decade has seen a huge amount of innovation in scalable data systems. These include large-scale computation systems like Hadoop and databases such as Cassandra and Riak. These systems can handle very large amounts of data, but with serious trade-offs.Hadoop, for example, can parallelize large-scale batch computations on very large amounts of data, but the computations have high latency. You don’t use Hadoop for anything where you need low-latency results.\nNoSQL databases like Cassandra achieve their scalability by offering you a much more limited data model than you’re used to with something like SQL. Squeezing your application into these limited data models can be very complex. And because the databases are mutable, they’re not human-fault tolerant.\nThese tools on their own are not a panacea. But when intelligently used in conjunction with one another, you can produce scalable systems for arbitrary data problems with human-fault tolerance and a minimum of complexity. This is the Lambda Architecture you’ll learn throughout the book.\n\n\n\n\n\n\n\n\nFrom What is Lambda Architecture? article in Databricks website\n\n\n\nLambda architecture is a way of processing massive quantities of data (i.e. “Big Data”) that provides access to batch-processing and stream-processing methods with a hybrid approach.\nLambda architecture is used to solve the problem of computing arbitrary functions.\n\n\n\n\n\n\n\n\nFrom Lambda architecture article in Wikipedia\n\n\n\nLambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.\nThis approach to architecture attempts to balance latency, throughput, and fault tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation.\nLambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.\n\n\n\n\nRequirements\nFault-tolerant against both hardware failures and human errors Support variety of use cases that include low latency querying as well as updates Linear scale-out capabilities Extensible, so that the system is manageable and can accommodate newer features easily\n\n\nQueries\n\\[\n\\textbf{query} = \\textbf{function}(\\textbf{all data})\n\\]\nSome query properties\n\nLatency: the time it takes to run a query\nTimeliness: how up to date the query results are (freshness and consistency)\nAccuracy: tradeoff between performance and scalability (approximations)\n\nIt is based on two data paths:\n\nCold path (batch layer)\n\nIt stores all of the incoming data in its raw form and performs batch processing on the data\nThe result of this processing is stored as batch views\n\nHot path (speed layer)\n\nIt analyzes data in real time\nThis path is designed for low latency, at the expense of accuracy\n\n\n\n\nBasic structure\n\n\nGeneral Lambda architecture\n\n\n\n\nAll data entering the system is dispatched to both the batch layer and the speed layer for processing\nThe batch layer has two functions:\n\nmanaging the master dataset(an immutable, append-only set of raw data), and\nto pre-compute the batch views\n\nThe serving layer indexes the batch views so that they can be queried in low-latency, ad-hoc way\nThe speed layer compensates for the high latency of updates to the serving layer and deals with recent data only\nAny incoming query can be answered by merging results from batch views and real-time views (e.g., the query looks at the serving layer for days until today, and looks at the speed layer for today’s data).\n\n\n\nDetailed view\n\n\nMore detailed of Lambda architecture\n\n\n\nStructure similar to the one described before\n\nData stream\nBatch layer\n\nimmutable data\nprecompute views\n\nReal-time layer\n\nprocess stream\nincrement views\n\nServing layer\n\n\nPossible instances\n\n\nMore detailed of Lambda architecture\n\n\n\nIn general, the technologies used are\n\nData stream: Kafka\nBatch layer:\n\nimmutable data: Hadoop HDFS\nprecompute views: Hadoop, Spark\nviews: Hive (it is a distributed relational database; SQL-like query language can be used)\n\nReal-time layer:\n\nprocess stream and increment views:\n\nSpark (it has a module available for managing stream data)\nApache Storm (pros: true real-time; cons: sometimes it approximates)\nFlink (used stream data analysis)\n\nviews: HBase, Cassandra, MongoDB\n\nServing layer\n\nIn general: choose the most suitable technology, but also be able to adapt on what’s available."
  },
  {
    "objectID": "03b_HDFS_clc.html#hdfs",
    "href": "03b_HDFS_clc.html#hdfs",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "HDFS",
    "text": "HDFS\nThe content of a HDFS file can be accessed by means of\n\nCommand line commands\nA basic web interface provided by Apache Hadoop. The HDFS content can only be browsed and its files downloaded from HDFS to the local file system, while uploading functionalities are not available.\nVendor-specific web interfaces providing a full set of functionalities (upload, download, rename, delete, …) (e.g., the HUE web application of Cloudera).\n\n\nUser folder\nEach user of the Hadoop cluster has a personal folder in the HDFS file system. The default folder of a user is /user/username\n\n\nCommand line\nThe hdfs command can be executed in a Linux shell to read/write/modify/delete the content of the distributed file system. The parameters/arguments of hdfs command are used to specify the operation to execute.\n\nContent of a folder\nTo list the content of a folder of the HDFS file system, use hdfs dfs -ls folder\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -ls /user/garza shows the content (list of files and folders) of the /user/garza folder.\nThe command hdfs dfs -ls . shows the content of the current folder (i.e., the content of /user/current_username).\nNotice that the mapping between the local linux user and the user of the cluster is based on\n\nA Kerberos ticket if Kerberos is active\nOtherwise the local linux user is considered\n\n\n\n\n\n\nContent of a file\nTo show the content of a file in the HDFS file system, use hdfs dfs -cat file_name\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -cat /user/garza/document.txt shows the content of the /user/garza/document.txt file stored in HDFS.\n\n\n\n\n\nCopy a file from local to HDFS\nTo copy a file from the local file system to the HDFS file system, use hdfs dfs -put local_file HDFS_path\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -put /data/document.txt /user/garza/ copies the local file /data/document.txt in the folder /user/garza in HDFS.\n\n\n\n\n\nCopy a file from HDFS to local\nTo copy a file from the HDFS file system to the local file system, use hdfs dfs -get HDFS_path local_file\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -get /user/garza/document.txt /data/ copies the HDFS file /user/garza/document.txt in the local file system folder /data/.\n\n\n\n\n\nDelete a file\nTo delete a file from the HDFS file system, use hdfs dfs -rm HDFS_path\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe command hdfs dfs -rm /user/garza/document.txt deletes from HDFS the file /user/garza/document.txt\n\n\n\n\n\nOther commands\nThere are many other linux-like commands, for example\n\nrmdir\ndu\ntail\n\nSee the HDFS commands guide for a complete list."
  },
  {
    "objectID": "03b_HDFS_clc.html#hadoop",
    "href": "03b_HDFS_clc.html#hadoop",
    "title": "4  HDFS and Hadoop: command line commands",
    "section": "Hadoop",
    "text": "Hadoop\nThe Hadoop programs are executed (submitted to the cluster) by using the hadoop command. It is a command line program, characterized by a set of parameters, such as\n\nthe name of the jar file containing all the classes of the MapReduce application we want to execute\nthe name of the Driver class\nthe parameters/arguments of the MapReduce application\n\n\nExample\nThe following command executes/submits a MapReduce application\nhadoop jar MyApplication.jar \\\nit.polito.bigdata.hadoop.DriverMyApplication \\\n1 inputdatafolder/ outputdatafolder/\n\nIt executes/submits the application contained in MyApplication.jar\nThe Driver Class is it.polito.bigdata.hadoop.DriverMyApplication\nThe application has three arguments\n\nNumber of reducers (1)\nInput data folder (inputdatafolder/)\nOutput data folder (outputdatafolder/)"
  }
]