<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Distributed architectures for big data processing and analytics - 27&nbsp; Spark structured streaming</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./23_streaming_frameworks.html" rel="next">
<link href="./21_streaming_analytics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./22_structured_streaming.html"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Spark structured streaming</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Distributed architectures for big data processing and analytics</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">About this Book</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Big data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Big data architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03b_HDFS_clc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">HDFS and Hadoop: command line commands</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_intro_hadoop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Hadoop and MapReduce</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hadoop_implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">How to write MapReduce programs in Hadoop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_mapreduce_patterns_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">MapReduce patterns - 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_mapreduce_advanced_topics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">MapReduce and Hadoop Advanced Topics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_mapreduce_patterns_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">MapReduce patterns - 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_sql_operators_mapreduce.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Relational Algebra Operations and MapReduce</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10b_spark_submit_execute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">How to submit/execute a Spark application</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_intro_spark.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_rdd_based_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">RDD based programming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_rdd_keyvalue_pairs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">RDDs and key-value pairs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_rdd_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RDD of numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_cache_accumulators_broadcast.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cache, Accumulators, Broadcast Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15b_pagerank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to PageRank</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_sparksql_dataframes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Spark SQL and DataFrames</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18a_spark_mllib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Spark MLlib</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18b_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Classification algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18c_clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Clustering algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18d_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Regression algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18e_mining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Itemset and Association rule mining</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_graph_analytics_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Graph analytics in Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_graph_analytics_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Graph Analytics in Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21_streaming_analytics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Streaming data analytics frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22_structured_streaming.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Spark structured streaming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23_streaming_frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Streaming data analytics frameworks</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-spark-structured-streaming" id="toc-what-is-spark-structured-streaming" class="nav-link active" data-scroll-target="#what-is-spark-structured-streaming">What is Spark structured streaming?</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key concepts</a></li>
  <li><a href="#event-time-and-window-operations" id="toc-event-time-and-window-operations" class="nav-link" data-scroll-target="#event-time-and-window-operations">Event time and window operations</a></li>
  <li><a href="#join-operations" id="toc-join-operations" class="nav-link" data-scroll-target="#join-operations">Join operations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Spark structured streaming</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="what-is-spark-structured-streaming" class="level3">
<h3 class="anchored" data-anchor-id="what-is-spark-structured-streaming">What is Spark structured streaming?</h3>
<p>Structured Streaming is a scalable and fault-tolerant stream processing engine that is built on the Spark SQL engine, and input data are represented by means of (streaming) DataFrames. Structured Streaming uses the existing Spark SQL APIs to query data streams (the same methods used for analyzing static DataFrames).</p>
<p>A set of specific methods are used to define</p>
<ul>
<li>Input and output streams</li>
<li>Windows</li>
</ul>
<section id="input-data-model" class="level4">
<h4 class="anchored" data-anchor-id="input-data-model">Input data model</h4>
<p>Each input data stream is modeled as a table that is being continuously appended: every time new data arrive they are appended at the end of the table (i.e., each data stream is considered an unbounded input table).</p>
<p>New input data in the stream are new rows appended to an unbounded table</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Input stream</figcaption>
<p><img src="images/22_structured_streaming/input_data_model.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="queries" class="level4">
<h4 class="anchored" data-anchor-id="queries">Queries</h4>
<p>The expressed queries are incremental queries that are run incrementally on the unbounded input tables.</p>
<ul>
<li>The arrive of new data triggers the execution of the incremental queries</li>
<li>The result of a query at a specific timestamp is the one obtained by running the query on all the data arrived until that timestamp (i.e., stateful queries are executed).</li>
<li>Aggregation queries combine new data with the previous results to optimize the computation of the new results.</li>
</ul>
<p>The queries can be executed</p>
<ul>
<li>As micro-batch queries with a fixed batch interval: this is the standard behavior, with exactly-once fault-tolerance guarantees</li>
<li>As continuous queries: this is experimental behavior, with at-least-once fault-tolerance guarantees</li>
</ul>
<p>In this example the (micro-batch) query is executed every 1 second</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Query execution</figcaption>
<p><img src="images/22_structured_streaming/query_execution.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Note that every time the query is executed, all data received so far are considered.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Input
<ul>
<li>A stream of records retrieved from localhost:9999</li>
<li>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format <code>stationId,# free slots,#used slots,timestamp</code></li>
</ul></li>
<li>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0
<ul>
<li>Print the requested information when new data are received by using the micro-batch processing mode</li>
<li>Suppose the batch-duration is set to 2 seconds</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Example execution</figcaption>
<p><img src="images/22_structured_streaming/stream_example.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="key-concepts" class="level3">
<h3 class="anchored" data-anchor-id="key-concepts">Key concepts</h3>
<ul>
<li>Input sources</li>
<li>Transformations</li>
<li>Outputs
<ul>
<li>External destinations/sinks</li>
<li>Output Modes</li>
</ul></li>
<li>Query run/execution</li>
<li>Triggers</li>
</ul>
<section id="input-sources" class="level4">
<h4 class="anchored" data-anchor-id="input-sources">Input sources</h4>
<ul>
<li>File source
<ul>
<li>Reads files written in a directory as a stream of data</li>
<li>Each line of the input file is an input record</li>
<li>Supported file formats are text, csv, json, orc, parquet, …</li>
</ul></li>
<li>Kafka source
<ul>
<li>Reads data from Kafka</li>
<li>Each Kafka message is one input record</li>
</ul></li>
<li>Socket source (for debugging purposes)
<ul>
<li>Reads UTF8 text data from a socket connection</li>
<li>This type of source does not provide end-to-end fault-tolerance guarantees</li>
</ul></li>
<li>Rate source (for debugging purposes)
<ul>
<li>Generates data at the specified number of rows per second</li>
<li>Each generated row contains a timestamp and value of type long</li>
</ul></li>
</ul>
<p>The <code>readStream</code> property of the <code>SparkSession</code> class is used to create <code>DataStreamReaders</code>: the methods <code>format()</code> and <code>option()</code> of the <code>DataStreamReader</code> class are used to specify the input streams (e.g., type, location). The method <code>load()</code> of the <code>DataStreamReader</code> class is used to return DataFrames associated with the input data streams.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this example the (streaming) DataFrame recordsDF is created and associated with the input stream of type socket</p>
<ul>
<li>Address: localhost</li>
<li>Input port: 9999</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>    .load()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="transformations" class="level4">
<h4 class="anchored" data-anchor-id="transformations">Transformations</h4>
<p>Transformations are the same of DataFrames, however there are restrictions on some types of queries/transformations that cannot be executed incrementally.</p>
<p>Unsupported operations:</p>
<ul>
<li>Multiple streaming aggregations (i.e.&nbsp;a chain of aggregations on a streaming DataFrame)</li>
<li>Limit and take first N rows</li>
<li>Distinct operations</li>
<li>Sorting operations are supported on streaming DataFrames only after an aggregation and in complete output mode</li>
<li>Few types of outer joins on streaming DataFrames are not supported</li>
</ul>
</section>
<section id="outputs" class="level4">
<h4 class="anchored" data-anchor-id="outputs">Outputs</h4>
<ul>
<li>Sinks: they are instances of the class DataStreamWriter and are used to specify the external destinations and store the results in the external destinations</li>
<li>File sink: it stores the output to a directory; supported file formats are text, csv, json, orc, parquet, …</li>
<li>Kafka sink: it stores the output to one or more topics in Kafka</li>
<li>Foreach sink: it runs arbitrary computation on the output records</li>
<li>Console sink (for debugging purposes): it prints the computed output to the console every time a new batch of records has been analyzed; this should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after each computation</li>
<li>Memory sink (for debugging purposes): the output is stored in memory as an in-memory table; this should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory</li>
</ul>
<section id="output-mods" class="level5">
<h5 class="anchored" data-anchor-id="output-mods">Output mods</h5>
<p>We must define how we want Spark to write output data in the external destinations. The supported output modes depend on the query type, and the possible output mods are the following</p>
<section id="append" class="level6">
<h6 class="anchored" data-anchor-id="append">Append</h6>
<p>This is the default mode. Only the new rows added to the computed result since the last trigger (computation) will be outputted. This mode is supported for only those queries where rows added to the result is never going to change: this mode guarantees that each row will be output only once. So, the supported queries are only select, filter, map, flatMap, filter, join, etc.</p>
</section>
<section id="complete" class="level6">
<h6 class="anchored" data-anchor-id="complete">Complete</h6>
<p>The whole computed result will be outputted to the sink after every trigger (computation). This mode is supported for aggregation queries.</p>
</section>
<section id="update" class="level6">
<h6 class="anchored" data-anchor-id="update">Update</h6>
<p>Only the rows in the computed result that were updated since the last trigger (computation) will be outputted.</p>
<p>The complete list of supported output modes for each query type is available in the <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Apache Spark documentation</a>.</p>
</section>
</section>
<section id="code" class="level5">
<h5 class="anchored" data-anchor-id="code">Code</h5>
<p>The <code>writeStream</code> property of the <code>SparkSession</code> class is used to create <code>DataStreamWriters</code>. The methods <code>outputMode()</code>, <code>format()</code>, and <code>option()</code> of the <code>DataStreamWriter</code> class are used to specify the output destination (data format, location, output mode, etc.).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Example The <code>DataStreamWriter</code> “streamWriterRes” is created and associated with the console. The output mode is set to append.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>streamWriterRes <span class="op">=</span> stationIdTimestampDF <span class="op">\</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>    .writeStream <span class="op">\</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    .outputMode(<span class="st">"append"</span>) <span class="op">\</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="query-runexecution" class="level4">
<h4 class="anchored" data-anchor-id="query-runexecution">Query run/execution</h4>
<p>To start executing the defined queries/structured streaming applications you must explicitly invoke the <code>start()</code> action on the defined sinks (<code>DataStreamWriter</code> objects associated with the external destinations in which the results will be stored). It is possible to start several queries in the same application, and structured streaming queries run forever (they must be explicitly stop/kill).</p>
</section>
<section id="triggers" class="level4">
<h4 class="anchored" data-anchor-id="triggers">Triggers</h4>
<p>For each Spark structured streaming query it is possible to specify when new input data must be processed, and whether the query is going to be executed as a micro-batch query with a fixed batch interval or as a continuous processing query (experimental). The trigger type for each query is specified by means of the <code>trigger()</code> method of the <code>DataStreamWriter</code> class.</p>
<section id="trigger-types" class="level5">
<h5 class="anchored" data-anchor-id="trigger-types">Trigger types</h5>
<p>No trigger type is explicitly specified, by default the query will be executed in micro-batch mode, where each micro-batch is generated and processed as soon as the previous micro-batch has been processed.</p>
<section id="fixed-interval-micro-batches" class="level6">
<h6 class="anchored" data-anchor-id="fixed-interval-micro-batches">Fixed interval micro-batches</h6>
<p>The query will be executed in micro-batch mode. Micro-batches will be processed at the user-specified intervals: the parameter <code>processingTime</code> of the trigger <code>method()</code> is used to specify the micro-batch size, and, if the previous micro-batch completes within its interval, then the engine will wait until the interval is over before processing the next micro-batch; if the previous micro-batch takes longer than the interval to complete (i.e.&nbsp;if an interval boundary is missed), then the next micro-batch will start as soon as the previous one completes.</p>
</section>
<section id="one-time-micro-batch" class="level6">
<h6 class="anchored" data-anchor-id="one-time-micro-batch">One-time micro-batch</h6>
<p>The query will be executed in micro-batch mode, but the query will be executed only one time on one single micro-batch containing all the available data of the input stream; after the single execution the query stops on its own. This trigger type is useful when the goal is to periodically spin up a cluster, process everything that is available since the last period, and then shutdown the cluster. In some case, this may lead to significant cost savings.</p>
</section>
<section id="continuous-with-fixed-checkpoint-interval-experimental" class="level6">
<h6 class="anchored" data-anchor-id="continuous-with-fixed-checkpoint-interval-experimental">Continuous with fixed checkpoint interval (experimental)</h6>
<p>The query will be executed in the new low-latency, continuous processing mode. It offers at-least-once fault-tolerance guarantees.</p>
</section>
</section>
</section>
<section id="spark-structured-streaming-examples" class="level4">
<h4 class="anchored" data-anchor-id="spark-structured-streaming-examples">Spark structured streaming examples</h4>
<section id="example-1" class="level5">
<h5 class="anchored" data-anchor-id="example-1">Example 1</h5>
<ul>
<li>Input
<ul>
<li>A stream of records retrieved from localhost:9999</li>
<li>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”</li>
</ul></li>
<li>Output
<ul>
<li>For each input reading with a number of free slots equal to 0 print on the standard output the value of stationId and timestamp</li>
<li>Use the standard micro-batch processing mode</li>
</ul></li>
</ul>
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3"></a></span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a>    .load()</span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a></span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a><span class="co">## of type string</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="annotated-cell-1-14"><a href="#annotated-cell-1-14"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="annotated-cell-1-15"><a href="#annotated-cell-1-15"></a><span class="co">## New columns:</span></span>
<span id="annotated-cell-1-16"><a href="#annotated-cell-1-16"></a><span class="co">## - stationId</span></span>
<span id="annotated-cell-1-17"><a href="#annotated-cell-1-17"></a><span class="co">## - freeslots</span></span>
<span id="annotated-cell-1-18"><a href="#annotated-cell-1-18"></a><span class="co">## - usedslots</span></span>
<span id="annotated-cell-1-19"><a href="#annotated-cell-1-19"></a><span class="co">## - timestamp</span></span>
<span id="annotated-cell-1-20"><a href="#annotated-cell-1-20"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="annotated-cell-1-21"><a href="#annotated-cell-1-21"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) \ </span>
<span id="annotated-cell-1-22"><a href="#annotated-cell-1-22"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="annotated-cell-1-23"><a href="#annotated-cell-1-23"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-24" class="code-annotation-target"><a href="#annotated-cell-1-24"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="annotated-cell-1-25"><a href="#annotated-cell-1-25"></a></span>
<span id="annotated-cell-1-26"><a href="#annotated-cell-1-26"></a><span class="co">## Filter data</span></span>
<span id="annotated-cell-1-27"><a href="#annotated-cell-1-27"></a><span class="co">## Use the standard filter transformation</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2">2</button><span id="annotated-cell-1-28" class="code-annotation-target"><a href="#annotated-cell-1-28"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="annotated-cell-1-29"><a href="#annotated-cell-1-29"></a></span>
<span id="annotated-cell-1-30"><a href="#annotated-cell-1-30"></a><span class="co">## Select stationid and timestamp</span></span>
<span id="annotated-cell-1-31"><a href="#annotated-cell-1-31"></a><span class="co">## Use the standard select transformation</span></span>
<span id="annotated-cell-1-32"><a href="#annotated-cell-1-32"></a>stationIdTimestampDF <span class="op">=</span> fullReadingsDF.select(<span class="st">"stationId"</span>, <span class="st">"timestamp"</span>)</span>
<span id="annotated-cell-1-33"><a href="#annotated-cell-1-33"></a></span>
<span id="annotated-cell-1-34"><a href="#annotated-cell-1-34"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="annotated-cell-1-35"><a href="#annotated-cell-1-35"></a><span class="co">## the console "sink“.</span></span>
<span id="annotated-cell-1-36"><a href="#annotated-cell-1-36"></a><span class="co">## append output mode</span></span>
<span id="annotated-cell-1-37"><a href="#annotated-cell-1-37"></a>queryFilterStreamWriter <span class="op">=</span> stationIdTimestampDF <span class="op">\</span></span>
<span id="annotated-cell-1-38"><a href="#annotated-cell-1-38"></a>    .writeStream <span class="op">\</span></span>
<span id="annotated-cell-1-39"><a href="#annotated-cell-1-39"></a>    .outputMode(<span class="st">"append"</span>) <span class="op">\</span></span>
<span id="annotated-cell-1-40"><a href="#annotated-cell-1-40"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="annotated-cell-1-41"><a href="#annotated-cell-1-41"></a></span>
<span id="annotated-cell-1-42"><a href="#annotated-cell-1-42"></a><span class="co">## Start the execution of the query (it will be executed until it is explicitly stopped)</span></span>
<span id="annotated-cell-1-43"><a href="#annotated-cell-1-43"></a>queryFilter <span class="op">=</span> queryFilterStreamWriter.start()</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-annotation="1" data-code-cell="annotated-cell-1" data-code-lines="24"><code>withColumn()</code> is used to add new columns (it is a standard DataFrame method). It returns a DataFrame with the same columns of the input DataFrame and the new defined column. For each new column it is possible to specify name (e.g.&nbsp;“stationId”) and the SQL function that is used to define its value in each record. The <code>cast()</code> method is used to specify the data type of each defined column.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-annotation="2" data-code-cell="annotated-cell-1" data-code-lines="28,32"><code>filter</code> and <code>select</code> are standard DataFrame transformations</span>
</dd>
</dl>
</section>
<section id="example-2" class="level5">
<h5 class="anchored" data-anchor-id="example-2">Example 2</h5>
<ul>
<li>Input
<ul>
<li>A stream of records retrieved from localhost:9999</li>
<li>Each input record is a reading about the status of a station of</li>
<li>bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”</li>
</ul></li>
<li>Output
<ul>
<li>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0</li>
<li>Print the requested information when new data are received by using the standard micro-batch processing mode</li>
</ul></li>
</ul>
<div class="sourceCode" id="annotated-cell-2"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="annotated-cell-2-2"><a href="#annotated-cell-2-2"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="annotated-cell-2-3"><a href="#annotated-cell-2-3"></a></span>
<span id="annotated-cell-2-4"><a href="#annotated-cell-2-4"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="annotated-cell-2-5"><a href="#annotated-cell-2-5"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="annotated-cell-2-6"><a href="#annotated-cell-2-6"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="annotated-cell-2-7"><a href="#annotated-cell-2-7"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="annotated-cell-2-8"><a href="#annotated-cell-2-8"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="annotated-cell-2-9"><a href="#annotated-cell-2-9"></a>    .load()</span>
<span id="annotated-cell-2-10"><a href="#annotated-cell-2-10"></a></span>
<span id="annotated-cell-2-11"><a href="#annotated-cell-2-11"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="annotated-cell-2-12"><a href="#annotated-cell-2-12"></a><span class="co">## of type string</span></span>
<span id="annotated-cell-2-13"><a href="#annotated-cell-2-13"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="annotated-cell-2-14"><a href="#annotated-cell-2-14"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="annotated-cell-2-15"><a href="#annotated-cell-2-15"></a><span class="co">## New columns:</span></span>
<span id="annotated-cell-2-16"><a href="#annotated-cell-2-16"></a><span class="co">## - stationId</span></span>
<span id="annotated-cell-2-17"><a href="#annotated-cell-2-17"></a><span class="co">## - freeslots</span></span>
<span id="annotated-cell-2-18"><a href="#annotated-cell-2-18"></a><span class="co">## - usedslots</span></span>
<span id="annotated-cell-2-19"><a href="#annotated-cell-2-19"></a><span class="co">## - timestamp</span></span>
<span id="annotated-cell-2-20"><a href="#annotated-cell-2-20"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="annotated-cell-2-21"><a href="#annotated-cell-2-21"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) <span class="op">\</span></span>
<span id="annotated-cell-2-22"><a href="#annotated-cell-2-22"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="annotated-cell-2-23"><a href="#annotated-cell-2-23"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="annotated-cell-2-24"><a href="#annotated-cell-2-24"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="annotated-cell-2-25"><a href="#annotated-cell-2-25"></a></span>
<span id="annotated-cell-2-26"><a href="#annotated-cell-2-26"></a><span class="co">## Filter data</span></span>
<span id="annotated-cell-2-27"><a href="#annotated-cell-2-27"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="annotated-cell-2-28"><a href="#annotated-cell-2-28"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="annotated-cell-2-29"><a href="#annotated-cell-2-29"></a></span>
<span id="annotated-cell-2-30"><a href="#annotated-cell-2-30"></a><span class="co">## Count the number of readings with a number of free slots equal to 0</span></span>
<span id="annotated-cell-2-31"><a href="#annotated-cell-2-31"></a><span class="co">## for each stationId</span></span>
<span id="annotated-cell-2-32"><a href="#annotated-cell-2-32"></a><span class="co">## The standard groupBy method is used</span></span>
<span id="annotated-cell-2-33"><a href="#annotated-cell-2-33"></a>countsDF <span class="op">=</span> fullReadingsDF <span class="op">\</span></span>
<span id="annotated-cell-2-34"><a href="#annotated-cell-2-34"></a>    .groupBy(<span class="st">"stationId"</span>) <span class="op">\</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1">1</button><span id="annotated-cell-2-35" class="code-annotation-target"><a href="#annotated-cell-2-35"></a>    .agg({<span class="st">"*"</span>:<span class="st">"count"</span>})</span>
<span id="annotated-cell-2-36"><a href="#annotated-cell-2-36"></a></span>
<span id="annotated-cell-2-37"><a href="#annotated-cell-2-37"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="annotated-cell-2-38"><a href="#annotated-cell-2-38"></a><span class="co">## the console "sink"</span></span>
<span id="annotated-cell-2-39"><a href="#annotated-cell-2-39"></a><span class="co">## complete output mode</span></span>
<span id="annotated-cell-2-40"><a href="#annotated-cell-2-40"></a><span class="co">## (append mode cannot be used for aggregation queries)</span></span>
<span id="annotated-cell-2-41"><a href="#annotated-cell-2-41"></a>queryCountStreamWriter <span class="op">=</span> countsDF <span class="op">\</span></span>
<span id="annotated-cell-2-42"><a href="#annotated-cell-2-42"></a>    .writeStream <span class="op">\</span></span>
<span id="annotated-cell-2-43"><a href="#annotated-cell-2-43"></a>    .outputMode(<span class="st">"complete"</span>) <span class="op">\</span></span>
<span id="annotated-cell-2-44"><a href="#annotated-cell-2-44"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="annotated-cell-2-45"><a href="#annotated-cell-2-45"></a></span>
<span id="annotated-cell-2-46"><a href="#annotated-cell-2-46"></a><span class="co">## Start the execution of the query (it will be executed until it is explicitly stopped)</span></span>
<span id="annotated-cell-2-47"><a href="#annotated-cell-2-47"></a>queryCount <span class="op">=</span> queryCountStreamWriter.start()</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-annotation="1" data-code-cell="annotated-cell-2" data-code-lines="35"><code>groupBy</code> and <code>agg</code> are standard DataFrame transformations</span>
</dd>
</dl>
</section>
</section>
</section>
<section id="event-time-and-window-operations" class="level3">
<h3 class="anchored" data-anchor-id="event-time-and-window-operations">Event time and window operations</h3>
<p>Input streaming records are usually characterized by a time information: it is usually called event-time, and it is the time when the data was generated. For many applications, you want to operate by taking into consideration the event-time and windows containing data associated with the same event-time range.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Compute the number of events generated by each monitored IoT device every minute based on the event-time. For each window associated with one distinct minute, consider only the data with an event-time inside that minute/window and compute the number of events for each IoT device: one computation for each minute/window. You want to use the time when the data was generated (i.e., the event-time) rather than the time Spark receives them.</p>
</div>
</div>
</div>
<p>Spark allows defining windows based on the time-event input column, and then apply aggregation functions over each window.</p>
<p>For each structured streaming query on which you want to apply a window computation you must specify</p>
<ul>
<li>the name of the time-event column in the input (streaming) DataFrame</li>
<li>the characteristics of the (sliding) windows
<ul>
<li><code>windowDuration</code></li>
<li><code>slideDuration</code></li>
</ul></li>
</ul>
<p>Do not set it if you want non-overlapped windows, (i.e., if you want to a <code>slideDuration</code> equal to <code>windowDuration</code>). You can set different window characteristics for each query of your application.</p>
<p>The <code>window(timeColumn, windowDuration, slideDuration=None)</code> function is used inside the standard <code>groupBy()</code> one to specify the characteristics of the windows. Notice that windows can be used only with queries that are applying aggregation functions.</p>
<section id="event-time-and-window-operations-example-1" class="level4">
<h4 class="anchored" data-anchor-id="event-time-and-window-operations-example-1">Event time and window operations: example 1</h4>
<ul>
<li>Input
<ul>
<li>A stream of records retrieved from localhost:9999</li>
<li>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”</li>
<li>timestamp is the event-time column</li>
</ul></li>
<li>Output
<ul>
<li>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window</li>
<li>The query is executed for each window</li>
<li>Set <code>windowDuration</code> to 2 seconds and no <code>slideDuration</code> (i.e., non-overlapped windows)</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 1</figcaption>
<p><img src="images/22_structured_streaming/example_3_1.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The returned result has a column called window. It contains the time slot associated with the window <span class="math inline">\([\text{from timestamp}, \text{to timestamp})\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 2</figcaption>
<p><img src="images/22_structured_streaming/example_3_2.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 3</figcaption>
<p><img src="images/22_structured_streaming/example_3_3.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>    .load()</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">## of type string</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="cb3-16"><a href="#cb3-16"></a><span class="co">## New columns:</span></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="co">## - stationId</span></span>
<span id="cb3-18"><a href="#cb3-18"></a><span class="co">## - freeslots</span></span>
<span id="cb3-19"><a href="#cb3-19"></a><span class="co">## - usedslots</span></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="co">## - timestamp</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="cb3-22"><a href="#cb3-22"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) <span class="op">\</span></span>
<span id="cb3-23"><a href="#cb3-23"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb3-25"><a href="#cb3-25"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="cb3-26"><a href="#cb3-26"></a></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">## Filter data</span></span>
<span id="cb3-28"><a href="#cb3-28"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="cb3-29"><a href="#cb3-29"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="cb3-30"><a href="#cb3-30"></a></span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="co">## Count the number of readings with a number of free slots equal to 0</span></span>
<span id="cb3-32"><a href="#cb3-32"></a><span class="co">## for each stationId in each window.</span></span>
<span id="cb3-33"><a href="#cb3-33"></a><span class="co">## windowDuration = 2 seconds</span></span>
<span id="cb3-34"><a href="#cb3-34"></a><span class="co">## no overlapping windows</span></span>
<span id="cb3-35"><a href="#cb3-35"></a>countsDF <span class="op">=</span> fullReadingsDF <span class="op">\</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>    .groupBy(window(fullReadingsDF.timestamp, <span class="st">"2 seconds"</span>), <span class="st">"stationId"</span>) <span class="op">\</span></span>
<span id="cb3-37"><a href="#cb3-37"></a>    .agg({<span class="st">"*"</span>:<span class="st">"count"</span>}) <span class="op">\</span></span>
<span id="cb3-38"><a href="#cb3-38"></a>    .sort(<span class="st">"window"</span>)</span>
<span id="cb3-39"><a href="#cb3-39"></a></span>
<span id="cb3-40"><a href="#cb3-40"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="cb3-41"><a href="#cb3-41"></a><span class="co">## the console "sink"</span></span>
<span id="cb3-42"><a href="#cb3-42"></a><span class="co">## complete output mode</span></span>
<span id="cb3-43"><a href="#cb3-43"></a><span class="co">## (append mode cannot be used for aggregation queries)</span></span>
<span id="cb3-44"><a href="#cb3-44"></a>queryCountWindowStreamWriter <span class="op">=</span> countsDF <span class="op">\</span></span>
<span id="cb3-45"><a href="#cb3-45"></a>    .writeStream <span class="op">\</span></span>
<span id="cb3-46"><a href="#cb3-46"></a>    .outputMode(<span class="st">"complete"</span>) <span class="op">\</span></span>
<span id="cb3-47"><a href="#cb3-47"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>) <span class="op">\</span></span>
<span id="cb3-48"><a href="#cb3-48"></a>    .option(<span class="st">"truncate"</span>, <span class="st">"false"</span>)</span>
<span id="cb3-49"><a href="#cb3-49"></a></span>
<span id="cb3-50"><a href="#cb3-50"></a><span class="co">## Start the execution of the query (it will be executed until it is explicitly stopped)</span></span>
<span id="cb3-51"><a href="#cb3-51"></a>queryCountWindow <span class="op">=</span> queryCountWindowStreamWriter.start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="late-data" class="level4">
<h4 class="anchored" data-anchor-id="late-data">Late data</h4>
<p>Sparks handles data that have arrived later than expected based on its event-time; these are called late data. Spark has full control over updating old aggregates when there are late data: every time new data are processed the result is computed by combining old aggregate values and the new data by considering the event-time column instead of the time Spark receives the data.</p>
<section id="late-data-example" class="level5">
<h5 class="anchored" data-anchor-id="late-data-example">Late data example</h5>
<ul>
<li>Input
<ul>
<li>A stream of records retrieved from localhost:9999</li>
<li>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”</li>
<li>timestamp is the event-time column</li>
</ul></li>
<li>Output
<ul>
<li>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window</li>
<li>The query is executed for each window</li>
<li>Set <code>windowDuration</code> to 2 seconds and no <code>slideDuration</code> (i.e., non-overlapped windows)</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 1</figcaption>
<p><img src="images/22_structured_streaming/late_data_example_1.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 2</figcaption>
<p><img src="images/22_structured_streaming/late_data_example_2.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 3</figcaption>
<p><img src="images/22_structured_streaming/late_data_example_3.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Notice that late data that was generated at <strong><em>2016-03-11 09:00:06</em></strong>, but arrived at <strong><em>2016-03-11 09:00:08</em></strong>: the result consider also late data and assign them to the right window by considering the event-time information.</p>
<p>The code is the same of the previous example (Event time and window operations: example 1): late data are automatically handled by Spark.</p>
</section>
</section>
<section id="event-time-and-window-operations-example-2" class="level4">
<h4 class="anchored" data-anchor-id="event-time-and-window-operations-example-2">Event time and window operations: example 2</h4>
<ul>
<li>Input
<ul>
<li>A stream of records retrieved from localhost:9999</li>
<li>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp #- Each input reading has the format: “stationId”, “# free slots”, “#used slots”, “timestamp”</li>
<li>timestamp is the event-time column</li>
</ul></li>
<li>Output
<ul>
<li>For each window, print on the standard output the total number of received input readings with a number of free slots equal to 0</li>
<li>The query is executed for each window</li>
<li>Set windowDuration to 2 seconds and no slideDuration (i.e., non-overlapped windows)</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 1</figcaption>
<p><img src="images/22_structured_streaming/example_4_1.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 2</figcaption>
<p><img src="images/22_structured_streaming/example_4_2.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption class="figure-caption">Step 3</figcaption>
<p><img src="images/22_structured_streaming/example_4_3.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb4-10"><a href="#cb4-10"></a>    .load()</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="co">## of type string</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co">## New columns:</span></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co">## - stationId</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">## - freeslots</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">## - usedslots</span></span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="co">## - timestamp</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) <span class="op">\</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb4-24"><a href="#cb4-24"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="cb4-26"><a href="#cb4-26"></a></span>
<span id="cb4-27"><a href="#cb4-27"></a><span class="co">## Filter data</span></span>
<span id="cb4-28"><a href="#cb4-28"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="cb4-29"><a href="#cb4-29"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="cb4-30"><a href="#cb4-30"></a></span>
<span id="cb4-31"><a href="#cb4-31"></a><span class="co">## Count the number of readings with a number of free slots equal to 0</span></span>
<span id="cb4-32"><a href="#cb4-32"></a><span class="co">## for in each window.</span></span>
<span id="cb4-33"><a href="#cb4-33"></a><span class="co">## windowDuration = 2 seconds</span></span>
<span id="cb4-34"><a href="#cb4-34"></a><span class="co">## no overlapping windows</span></span>
<span id="cb4-35"><a href="#cb4-35"></a>countsDF <span class="op">=</span> fullReadingsDF <span class="op">\</span></span>
<span id="cb4-36"><a href="#cb4-36"></a>    .groupBy(window(fullReadingsDF.timestamp, <span class="st">"2 seconds"</span>)) <span class="op">\</span></span>
<span id="cb4-37"><a href="#cb4-37"></a>    .agg({<span class="st">"*"</span>:<span class="st">"count"</span>}) <span class="op">\</span></span>
<span id="cb4-38"><a href="#cb4-38"></a>    .sort(<span class="st">"window"</span>)</span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="cb4-41"><a href="#cb4-41"></a><span class="co">## the console "sink"</span></span>
<span id="cb4-42"><a href="#cb4-42"></a><span class="co">## complete output mode</span></span>
<span id="cb4-43"><a href="#cb4-43"></a><span class="co">## (append mode cannot be used for aggregation queries)</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>queryCountWindowStreamWriter <span class="op">=</span> countsDF <span class="op">\</span></span>
<span id="cb4-45"><a href="#cb4-45"></a>    .writeStream <span class="op">\</span></span>
<span id="cb4-46"><a href="#cb4-46"></a>    .outputMode(<span class="st">"complete"</span>) <span class="op">\</span></span>
<span id="cb4-47"><a href="#cb4-47"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>) <span class="op">\</span></span>
<span id="cb4-48"><a href="#cb4-48"></a>    .option(<span class="st">"truncate"</span>, <span class="st">"false"</span>)</span>
<span id="cb4-49"><a href="#cb4-49"></a></span>
<span id="cb4-50"><a href="#cb4-50"></a><span class="co">## Start the execution of the query (it will be executed until it is </span></span>
<span id="cb4-51"><a href="#cb4-51"></a><span class="co">## explicitly stopped)</span></span>
<span id="cb4-52"><a href="#cb4-52"></a>queryCountWindow <span class="op">=</span> queryCountWindowStreamWriter.start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="watermarking" class="level4">
<h4 class="anchored" data-anchor-id="watermarking">Watermarking</h4>
<p>Watermarking is a feature of Spark that allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state. Results related to old event-times are not needed in many real streaming applications: they can be dropped to improve the efficiency of the application, since keeping the state of old results is resource expensive; in this way every time new data are processed only recent records are considered.</p>
<p>Specifically, to run windowed queries for days, it is necessary for the system to bound the amount of intermediate in-memory state it accumulates. This means the system needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more; to enable this, in Spark 2.1, watermarking has been introduced.</p>
<p>Watermarking lets the Spark Structured Streaming engine automatically track the current event time in the data and attempt to clean up old state accordingly. It allows to define the watermark of a query by specifying the event time column and the threshold on how late the data is expected to be in terms of event time: for a specific window ending at time <span class="math inline">\(T\)</span>, the engine will maintain state and allow late data to update the state/the result until max event time seen by the engine <span class="math inline">\(&lt; T + \text{late threshold}\)</span>. In other words, late data within the threshold will be aggregated, but data later than <span class="math inline">\({T + \text{threshold}}\)</span> will be dropped.</p>
</section>
</section>
<section id="join-operations" class="level3">
<h3 class="anchored" data-anchor-id="join-operations">Join operations</h3>
<p>Spark Structured Streaming manages also join operations</p>
<ul>
<li>Between two streaming DataFrames</li>
<li>Between a streaming DataFrame and a static DataFrame</li>
</ul>
<p>The result of the streaming join is generated incrementally.</p>
<p>When joining between two streaming DataFrames, for both input streams, past input streaming data must be buffered/recorded in order to be able to match every future input record with past input data and accordingly generate joined results. Too many resources are needed for storing all the input data, hence, old data must be discarded. Watermark thresholds must be defined on both input streams such that the engine knows how delayed the input can be and drop old data.</p>
<p>The methods <code>join()</code> and <code>withWatermark()</code> are used to join streaming DataFrames: the join method is similar to the one available for static DataFrame.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> expr</span>
<span id="cb5-2"><a href="#cb5-2"></a>impressions <span class="op">=</span> spark.readStream. ...</span>
<span id="cb5-3"><a href="#cb5-3"></a>clicks <span class="op">=</span> spark.readStream. ...</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co">## Apply watermarks on event-time columns</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>impressionsWithWatermark <span class="op">=</span> impressions <span class="op">\</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>    .withWatermark(<span class="st">"impressionTime"</span>, <span class="st">"2 hours"</span>)</span>
<span id="cb5-8"><a href="#cb5-8"></a>    </span>
<span id="cb5-9"><a href="#cb5-9"></a>clicksWithWatermark <span class="op">=</span> clicks <span class="op">\</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>    .withWatermark(<span class="st">"clickTime"</span>, <span class="st">"3 hours"</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">## Join with event-time constraints</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>impressionsWithWatermark.join(</span>
<span id="cb5-14"><a href="#cb5-14"></a>    clicksWithWatermark,</span>
<span id="cb5-15"><a href="#cb5-15"></a>    expr(</span>
<span id="cb5-16"><a href="#cb5-16"></a>        <span class="st">"""</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="st">        clickAdId = impressionAdId AND </span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="st">        clickTime &gt;= impressionTime AND</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="st">        clickTime &lt;= impressionTime + interval 1 hour"""</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    ) </span>
<span id="cb5-21"><a href="#cb5-21"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Handle positioning of the toggle
      window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./21_streaming_analytics.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Streaming data analytics frameworks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./23_streaming_frameworks.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Streaming data analytics frameworks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu"># Spark structured streaming</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">## What is Spark structured streaming?</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>Structured Streaming is a scalable and fault-tolerant stream processing engine that is built on the Spark SQL engine, and input data are represented by means of (streaming) DataFrames. Structured Streaming uses the existing Spark SQL APIs to query data streams (the same methods used for analyzing static DataFrames).</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>A set of specific methods are used to define</span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="ss">- </span>Input and output streams</span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="ss">- </span>Windows</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="fu">### Input data model</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>Each input data stream is modeled as a table that is being continuously appended: every time new data arrive they are appended at the end of the table (i.e., each data stream is considered an unbounded input table).</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a>New input data in the stream are new rows appended to an unbounded table</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="al">![Input stream](images/22_structured_streaming/input_data_model.png)</span>{width=80%}</span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="fu">### Queries</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>The expressed queries are incremental queries that are run incrementally on the unbounded input tables. </span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a><span class="ss">- </span>The arrive of new data triggers the execution of the incremental queries</span>
<span id="cb6-21"><a href="#cb6-21"></a><span class="ss">- </span>The result of a query at a specific timestamp is the one obtained by running the query on all the data arrived until that timestamp (i.e., stateful queries are executed). </span>
<span id="cb6-22"><a href="#cb6-22"></a><span class="ss">- </span>Aggregation queries combine new data with the previous results to optimize the computation of the new results.</span>
<span id="cb6-23"><a href="#cb6-23"></a></span>
<span id="cb6-24"><a href="#cb6-24"></a>The queries can be executed</span>
<span id="cb6-25"><a href="#cb6-25"></a></span>
<span id="cb6-26"><a href="#cb6-26"></a><span class="ss">- </span>As micro-batch queries with a fixed batch interval: this is the standard behavior, with exactly-once fault-tolerance guarantees</span>
<span id="cb6-27"><a href="#cb6-27"></a><span class="ss">- </span>As continuous queries: this is experimental behavior, with at-least-once fault-tolerance guarantees</span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a>In this example the (micro-batch) query is executed every 1 second</span>
<span id="cb6-30"><a href="#cb6-30"></a></span>
<span id="cb6-31"><a href="#cb6-31"></a><span class="al">![Query execution](images/22_structured_streaming/query_execution.png)</span>{width=80%}</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a>Note that every time the query is executed, all data received so far are considered.</span>
<span id="cb6-34"><a href="#cb6-34"></a></span>
<span id="cb6-35"><a href="#cb6-35"></a>:::{.callout-note collapse="true"}</span>
<span id="cb6-36"><a href="#cb6-36"></a><span class="fu">### Example</span></span>
<span id="cb6-37"><a href="#cb6-37"></a><span class="ss">- </span>Input</span>
<span id="cb6-38"><a href="#cb6-38"></a><span class="ss">    - </span>A stream of records retrieved from localhost:9999</span>
<span id="cb6-39"><a href="#cb6-39"></a><span class="ss">    - </span>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp</span>
<span id="cb6-40"><a href="#cb6-40"></a>    #- Each input reading has the format <span class="in">`stationId,# free slots,#used slots,timestamp`</span></span>
<span id="cb6-41"><a href="#cb6-41"></a><span class="ss">- </span>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0</span>
<span id="cb6-42"><a href="#cb6-42"></a><span class="ss">    - </span>Print the requested information when new data are received by using the micro-batch processing mode</span>
<span id="cb6-43"><a href="#cb6-43"></a><span class="ss">    - </span>Suppose the batch-duration is set to 2 seconds</span>
<span id="cb6-44"><a href="#cb6-44"></a></span>
<span id="cb6-45"><a href="#cb6-45"></a><span class="al">![Example execution](images/22_structured_streaming/stream_example.png)</span>{width=80%}</span>
<span id="cb6-46"><a href="#cb6-46"></a></span>
<span id="cb6-47"><a href="#cb6-47"></a>:::</span>
<span id="cb6-48"><a href="#cb6-48"></a></span>
<span id="cb6-49"><a href="#cb6-49"></a><span class="fu">## Key concepts</span></span>
<span id="cb6-50"><a href="#cb6-50"></a><span class="ss">- </span>Input sources</span>
<span id="cb6-51"><a href="#cb6-51"></a><span class="ss">- </span>Transformations</span>
<span id="cb6-52"><a href="#cb6-52"></a><span class="ss">- </span>Outputs</span>
<span id="cb6-53"><a href="#cb6-53"></a><span class="ss">    - </span>External destinations/sinks</span>
<span id="cb6-54"><a href="#cb6-54"></a><span class="ss">    - </span>Output Modes</span>
<span id="cb6-55"><a href="#cb6-55"></a><span class="ss">- </span>Query run/execution</span>
<span id="cb6-56"><a href="#cb6-56"></a><span class="ss">- </span>Triggers</span>
<span id="cb6-57"><a href="#cb6-57"></a></span>
<span id="cb6-58"><a href="#cb6-58"></a><span class="fu">### Input sources</span></span>
<span id="cb6-59"><a href="#cb6-59"></a><span class="ss">- </span>File source</span>
<span id="cb6-60"><a href="#cb6-60"></a><span class="ss">    - </span>Reads files written in a directory as a stream of data</span>
<span id="cb6-61"><a href="#cb6-61"></a><span class="ss">    - </span>Each line of the input file is an input record</span>
<span id="cb6-62"><a href="#cb6-62"></a><span class="ss">    - </span>Supported file formats are text, csv, json, orc, parquet, ...</span>
<span id="cb6-63"><a href="#cb6-63"></a><span class="ss">- </span>Kafka source</span>
<span id="cb6-64"><a href="#cb6-64"></a><span class="ss">    - </span>Reads data from Kafka</span>
<span id="cb6-65"><a href="#cb6-65"></a><span class="ss">    - </span>Each Kafka message is one input record</span>
<span id="cb6-66"><a href="#cb6-66"></a><span class="ss">- </span>Socket source (for debugging purposes)</span>
<span id="cb6-67"><a href="#cb6-67"></a><span class="ss">    - </span>Reads UTF8 text data from a socket connection</span>
<span id="cb6-68"><a href="#cb6-68"></a><span class="ss">    - </span>This type of source does not provide end-to-end fault-tolerance guarantees</span>
<span id="cb6-69"><a href="#cb6-69"></a><span class="ss">- </span>Rate source (for debugging purposes)</span>
<span id="cb6-70"><a href="#cb6-70"></a><span class="ss">    - </span>Generates data at the specified number of rows per second</span>
<span id="cb6-71"><a href="#cb6-71"></a><span class="ss">    - </span>Each generated row contains a timestamp and value of type long</span>
<span id="cb6-72"><a href="#cb6-72"></a></span>
<span id="cb6-73"><a href="#cb6-73"></a>The <span class="in">`readStream`</span> property of the <span class="in">`SparkSession`</span> class is used to create <span class="in">`DataStreamReaders`</span>: the methods <span class="in">`format()`</span> and <span class="in">`option()`</span> of the <span class="in">`DataStreamReader`</span> class are used to specify the input streams (e.g., type, location). The method <span class="in">`load()`</span> of the <span class="in">`DataStreamReader`</span> class is used to return DataFrames associated with the input data streams.</span>
<span id="cb6-74"><a href="#cb6-74"></a></span>
<span id="cb6-75"><a href="#cb6-75"></a>:::{.callout-note collapse="true"}</span>
<span id="cb6-76"><a href="#cb6-76"></a>In this example the (streaming) DataFrame recordsDF is created and associated with the input stream of type socket</span>
<span id="cb6-77"><a href="#cb6-77"></a></span>
<span id="cb6-78"><a href="#cb6-78"></a><span class="ss">- </span>Address: localhost</span>
<span id="cb6-79"><a href="#cb6-79"></a><span class="ss">- </span>Input port: 9999</span>
<span id="cb6-80"><a href="#cb6-80"></a></span>
<span id="cb6-81"><a href="#cb6-81"></a><span class="in">```python</span></span>
<span id="cb6-82"><a href="#cb6-82"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb6-83"><a href="#cb6-83"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb6-84"><a href="#cb6-84"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb6-85"><a href="#cb6-85"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb6-86"><a href="#cb6-86"></a>    .load()</span>
<span id="cb6-87"><a href="#cb6-87"></a><span class="in">```</span></span>
<span id="cb6-88"><a href="#cb6-88"></a></span>
<span id="cb6-89"><a href="#cb6-89"></a>:::</span>
<span id="cb6-90"><a href="#cb6-90"></a></span>
<span id="cb6-91"><a href="#cb6-91"></a><span class="fu">### Transformations</span></span>
<span id="cb6-92"><a href="#cb6-92"></a>Transformations are the same of DataFrames, however there are restrictions on some types of queries/transformations that cannot be executed incrementally.</span>
<span id="cb6-93"><a href="#cb6-93"></a></span>
<span id="cb6-94"><a href="#cb6-94"></a>Unsupported operations:</span>
<span id="cb6-95"><a href="#cb6-95"></a></span>
<span id="cb6-96"><a href="#cb6-96"></a><span class="ss">- </span>Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DataFrame)</span>
<span id="cb6-97"><a href="#cb6-97"></a><span class="ss">- </span>Limit and take first N rows</span>
<span id="cb6-98"><a href="#cb6-98"></a><span class="ss">- </span>Distinct operations</span>
<span id="cb6-99"><a href="#cb6-99"></a><span class="ss">- </span>Sorting operations are supported on streaming DataFrames only after an aggregation and in complete output mode</span>
<span id="cb6-100"><a href="#cb6-100"></a><span class="ss">- </span>Few types of outer joins on streaming DataFrames are not supported</span>
<span id="cb6-101"><a href="#cb6-101"></a></span>
<span id="cb6-102"><a href="#cb6-102"></a><span class="fu">### Outputs</span></span>
<span id="cb6-103"><a href="#cb6-103"></a><span class="ss">- </span>Sinks: they are instances of the class DataStreamWriter and are used to specify the external destinations and store the results in the external destinations</span>
<span id="cb6-104"><a href="#cb6-104"></a><span class="ss">- </span>File sink: it stores the output to a directory; supported file formats are text, csv, json, orc, parquet, ...</span>
<span id="cb6-105"><a href="#cb6-105"></a><span class="ss">- </span>Kafka sink: it stores the output to one or more topics in Kafka</span>
<span id="cb6-106"><a href="#cb6-106"></a><span class="ss">- </span>Foreach sink: it runs arbitrary computation on the output records</span>
<span id="cb6-107"><a href="#cb6-107"></a><span class="ss">- </span>Console sink (for debugging purposes): it prints the computed output to the console every time a new batch of records has been analyzed; this should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after each computation</span>
<span id="cb6-108"><a href="#cb6-108"></a><span class="ss">- </span>Memory sink (for debugging purposes): the output is stored in memory as an in-memory table; this should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory</span>
<span id="cb6-109"><a href="#cb6-109"></a></span>
<span id="cb6-110"><a href="#cb6-110"></a><span class="fu">#### Output mods</span></span>
<span id="cb6-111"><a href="#cb6-111"></a>We must define how we want Spark to write output data in the external destinations. The supported output modes depend on the query type, and the possible output mods are the following</span>
<span id="cb6-112"><a href="#cb6-112"></a></span>
<span id="cb6-113"><a href="#cb6-113"></a><span class="fu">##### Append</span></span>
<span id="cb6-114"><a href="#cb6-114"></a>This is the default mode. Only the new rows added to the computed result since the last trigger (computation) will be outputted. This mode is supported for only those queries where rows added to the result is never going to change: this mode guarantees that each row will be output only once. So, the supported queries are only select, filter, map, flatMap, filter, join, etc.</span>
<span id="cb6-115"><a href="#cb6-115"></a></span>
<span id="cb6-116"><a href="#cb6-116"></a><span class="fu">##### Complete</span></span>
<span id="cb6-117"><a href="#cb6-117"></a>The whole computed result will be outputted to the sink after every trigger (computation). This mode is supported for aggregation queries.</span>
<span id="cb6-118"><a href="#cb6-118"></a></span>
<span id="cb6-119"><a href="#cb6-119"></a><span class="fu">##### Update</span></span>
<span id="cb6-120"><a href="#cb6-120"></a>Only the rows in the computed result that were updated since the last trigger (computation) will be outputted.</span>
<span id="cb6-121"><a href="#cb6-121"></a></span>
<span id="cb6-122"><a href="#cb6-122"></a>The complete list of supported output modes for each query type is available in the <span class="co">[</span><span class="ot">Apache Spark documentation</span><span class="co">](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)</span>.</span>
<span id="cb6-123"><a href="#cb6-123"></a></span>
<span id="cb6-124"><a href="#cb6-124"></a><span class="fu">#### Code</span></span>
<span id="cb6-125"><a href="#cb6-125"></a>The <span class="in">`writeStream`</span> property of the <span class="in">`SparkSession`</span> class is used to create <span class="in">`DataStreamWriters`</span>. The methods <span class="in">`outputMode()`</span>, <span class="in">`format()`</span>, and <span class="in">`option()`</span> of the <span class="in">`DataStreamWriter`</span> class are used to specify the output destination (data format, location, output mode, etc.).</span>
<span id="cb6-126"><a href="#cb6-126"></a></span>
<span id="cb6-127"><a href="#cb6-127"></a>:::{.callout-note collapse="true"}</span>
<span id="cb6-128"><a href="#cb6-128"></a>Example</span>
<span id="cb6-129"><a href="#cb6-129"></a>The <span class="in">`DataStreamWriter`</span> "streamWriterRes" is created and associated with the console. The output mode is set to append.</span>
<span id="cb6-130"><a href="#cb6-130"></a></span>
<span id="cb6-131"><a href="#cb6-131"></a><span class="in">```python</span></span>
<span id="cb6-132"><a href="#cb6-132"></a>streamWriterRes <span class="op">=</span> stationIdTimestampDF <span class="op">\</span></span>
<span id="cb6-133"><a href="#cb6-133"></a>    .writeStream <span class="op">\</span></span>
<span id="cb6-134"><a href="#cb6-134"></a>    .outputMode(<span class="st">"append"</span>) <span class="op">\</span></span>
<span id="cb6-135"><a href="#cb6-135"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="cb6-136"><a href="#cb6-136"></a><span class="in">```</span></span>
<span id="cb6-137"><a href="#cb6-137"></a></span>
<span id="cb6-138"><a href="#cb6-138"></a>:::</span>
<span id="cb6-139"><a href="#cb6-139"></a></span>
<span id="cb6-140"><a href="#cb6-140"></a><span class="fu">### Query run/execution</span></span>
<span id="cb6-141"><a href="#cb6-141"></a>To start executing the defined queries/structured streaming applications you must explicitly invoke the <span class="in">`start()`</span> action on the defined sinks (<span class="in">`DataStreamWriter`</span> objects associated with the external destinations in which the results will be stored). It is possible to start several queries in the same application, and structured streaming queries run forever (they must be explicitly stop/kill).</span>
<span id="cb6-142"><a href="#cb6-142"></a></span>
<span id="cb6-143"><a href="#cb6-143"></a><span class="fu">### Triggers</span></span>
<span id="cb6-144"><a href="#cb6-144"></a>For each Spark structured streaming query it is possible to specify when new input data must be processed, and whether the query is going to be executed as a micro-batch query with a fixed batch interval or as a continuous processing query (experimental). The trigger type for each query is specified by means of the <span class="in">`trigger()`</span> method of the <span class="in">`DataStreamWriter`</span> class.</span>
<span id="cb6-145"><a href="#cb6-145"></a></span>
<span id="cb6-146"><a href="#cb6-146"></a><span class="fu">#### Trigger types</span></span>
<span id="cb6-147"><a href="#cb6-147"></a>No trigger type is explicitly specified, by default the query will be executed in micro-batch mode, where each micro-batch is generated and processed as soon as the previous micro-batch has been processed.</span>
<span id="cb6-148"><a href="#cb6-148"></a></span>
<span id="cb6-149"><a href="#cb6-149"></a><span class="fu">##### Fixed interval micro-batches</span></span>
<span id="cb6-150"><a href="#cb6-150"></a>The query will be executed in micro-batch mode. Micro-batches will be processed at the user-specified intervals: the parameter <span class="in">`processingTime`</span> of the trigger <span class="in">`method()`</span> is used to specify the micro-batch size, and, if the previous micro-batch completes within its interval, then the engine will wait until the interval is over before processing the next micro-batch; if the previous micro-batch takes longer than the interval to complete (i.e. if an interval boundary is missed), then the next micro-batch will start as soon as the previous one completes.</span>
<span id="cb6-151"><a href="#cb6-151"></a></span>
<span id="cb6-152"><a href="#cb6-152"></a><span class="fu">##### One-time micro-batch</span></span>
<span id="cb6-153"><a href="#cb6-153"></a>The query will be executed in micro-batch mode, but the query will be executed only one time on one single micro-batch containing all the available data of the input stream; after the single execution the query stops on its own. This trigger type is useful when the goal is to periodically spin up a cluster, process everything that is available since the last period, and then shutdown the cluster. In some case, this may lead to significant cost savings.</span>
<span id="cb6-154"><a href="#cb6-154"></a></span>
<span id="cb6-155"><a href="#cb6-155"></a><span class="fu">##### Continuous with fixed checkpoint interval (experimental)</span></span>
<span id="cb6-156"><a href="#cb6-156"></a>The query will be executed in the new low-latency, continuous processing mode. It offers at-least-once fault-tolerance guarantees.</span>
<span id="cb6-157"><a href="#cb6-157"></a></span>
<span id="cb6-158"><a href="#cb6-158"></a><span class="fu">### Spark structured streaming examples</span></span>
<span id="cb6-159"><a href="#cb6-159"></a><span class="fu">#### Example 1</span></span>
<span id="cb6-160"><a href="#cb6-160"></a><span class="ss">- </span>Input</span>
<span id="cb6-161"><a href="#cb6-161"></a><span class="ss">    - </span>A stream of records retrieved from localhost:9999</span>
<span id="cb6-162"><a href="#cb6-162"></a><span class="ss">    - </span>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp</span>
<span id="cb6-163"><a href="#cb6-163"></a>    #- Each input reading has the format: "stationId", "# free slots", "#used slots", "timestamp"</span>
<span id="cb6-164"><a href="#cb6-164"></a><span class="ss">- </span>Output</span>
<span id="cb6-165"><a href="#cb6-165"></a><span class="ss">    - </span>For each input reading with a number of free slots equal to 0 print on the standard output the value of stationId and timestamp</span>
<span id="cb6-166"><a href="#cb6-166"></a><span class="ss">    - </span>Use the standard micro-batch processing mode</span>
<span id="cb6-167"><a href="#cb6-167"></a></span>
<span id="cb6-168"><a href="#cb6-168"></a><span class="in">```python</span></span>
<span id="cb6-169"><a href="#cb6-169"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="cb6-170"><a href="#cb6-170"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="cb6-171"><a href="#cb6-171"></a></span>
<span id="cb6-172"><a href="#cb6-172"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="cb6-173"><a href="#cb6-173"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb6-174"><a href="#cb6-174"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb6-175"><a href="#cb6-175"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb6-176"><a href="#cb6-176"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb6-177"><a href="#cb6-177"></a>    .load()</span>
<span id="cb6-178"><a href="#cb6-178"></a></span>
<span id="cb6-179"><a href="#cb6-179"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="cb6-180"><a href="#cb6-180"></a><span class="co">## of type string</span></span>
<span id="cb6-181"><a href="#cb6-181"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="cb6-182"><a href="#cb6-182"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="cb6-183"><a href="#cb6-183"></a><span class="co">## New columns:</span></span>
<span id="cb6-184"><a href="#cb6-184"></a><span class="co">## - stationId</span></span>
<span id="cb6-185"><a href="#cb6-185"></a><span class="co">## - freeslots</span></span>
<span id="cb6-186"><a href="#cb6-186"></a><span class="co">## - usedslots</span></span>
<span id="cb6-187"><a href="#cb6-187"></a><span class="co">## - timestamp</span></span>
<span id="cb6-188"><a href="#cb6-188"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="cb6-189"><a href="#cb6-189"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) \ </span>
<span id="cb6-190"><a href="#cb6-190"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-191"><a href="#cb6-191"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-192"><a href="#cb6-192"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>)) <span class="co"># &lt;1&gt;</span></span>
<span id="cb6-193"><a href="#cb6-193"></a></span>
<span id="cb6-194"><a href="#cb6-194"></a><span class="co">## Filter data</span></span>
<span id="cb6-195"><a href="#cb6-195"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="cb6-196"><a href="#cb6-196"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>) <span class="co"># &lt;2&gt;</span></span>
<span id="cb6-197"><a href="#cb6-197"></a></span>
<span id="cb6-198"><a href="#cb6-198"></a><span class="co">## Select stationid and timestamp</span></span>
<span id="cb6-199"><a href="#cb6-199"></a><span class="co">## Use the standard select transformation</span></span>
<span id="cb6-200"><a href="#cb6-200"></a>stationIdTimestampDF <span class="op">=</span> fullReadingsDF.select(<span class="st">"stationId"</span>, <span class="st">"timestamp"</span>) <span class="co"># &lt;2&gt;</span></span>
<span id="cb6-201"><a href="#cb6-201"></a></span>
<span id="cb6-202"><a href="#cb6-202"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="cb6-203"><a href="#cb6-203"></a><span class="co">## the console "sink“.</span></span>
<span id="cb6-204"><a href="#cb6-204"></a><span class="co">## append output mode</span></span>
<span id="cb6-205"><a href="#cb6-205"></a>queryFilterStreamWriter <span class="op">=</span> stationIdTimestampDF <span class="op">\</span></span>
<span id="cb6-206"><a href="#cb6-206"></a>    .writeStream <span class="op">\</span></span>
<span id="cb6-207"><a href="#cb6-207"></a>    .outputMode(<span class="st">"append"</span>) <span class="op">\</span></span>
<span id="cb6-208"><a href="#cb6-208"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="cb6-209"><a href="#cb6-209"></a></span>
<span id="cb6-210"><a href="#cb6-210"></a><span class="co">## Start the execution of the query (it will be executed until it is explicitly stopped)</span></span>
<span id="cb6-211"><a href="#cb6-211"></a>queryFilter <span class="op">=</span> queryFilterStreamWriter.start()</span>
<span id="cb6-212"><a href="#cb6-212"></a><span class="in">```</span></span>
<span id="cb6-213"><a href="#cb6-213"></a><span class="ss">1. </span><span class="in">`withColumn()`</span> is used to add new columns (it is a standard DataFrame method). It returns a DataFrame with the same columns of the input DataFrame and the new defined column. For each new column it is possible to specify name (e.g. "stationId") and the SQL function that is used to define its value in each record. The <span class="in">`cast()`</span> method is used to specify the data type of each defined column.</span>
<span id="cb6-214"><a href="#cb6-214"></a><span class="ss">2. </span><span class="in">`filter`</span> and <span class="in">`select`</span> are standard DataFrame transformations</span>
<span id="cb6-215"><a href="#cb6-215"></a></span>
<span id="cb6-216"><a href="#cb6-216"></a><span class="fu">#### Example 2</span></span>
<span id="cb6-217"><a href="#cb6-217"></a><span class="ss">- </span>Input</span>
<span id="cb6-218"><a href="#cb6-218"></a><span class="ss">    - </span>A stream of records retrieved from localhost:9999</span>
<span id="cb6-219"><a href="#cb6-219"></a><span class="ss">    - </span>Each input record is a reading about the status of a station of</span>
<span id="cb6-220"><a href="#cb6-220"></a><span class="ss">    - </span>bike sharing system in a specific timestamp</span>
<span id="cb6-221"><a href="#cb6-221"></a>    #- Each input reading has the format: "stationId", "# free slots", "#used slots", "timestamp"</span>
<span id="cb6-222"><a href="#cb6-222"></a><span class="ss">- </span>Output</span>
<span id="cb6-223"><a href="#cb6-223"></a><span class="ss">    - </span>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0</span>
<span id="cb6-224"><a href="#cb6-224"></a><span class="ss">    - </span>Print the requested information when new data are received by using the standard micro-batch processing mode</span>
<span id="cb6-225"><a href="#cb6-225"></a></span>
<span id="cb6-226"><a href="#cb6-226"></a><span class="in">```python</span></span>
<span id="cb6-227"><a href="#cb6-227"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="cb6-228"><a href="#cb6-228"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="cb6-229"><a href="#cb6-229"></a></span>
<span id="cb6-230"><a href="#cb6-230"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="cb6-231"><a href="#cb6-231"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb6-232"><a href="#cb6-232"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb6-233"><a href="#cb6-233"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb6-234"><a href="#cb6-234"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb6-235"><a href="#cb6-235"></a>    .load()</span>
<span id="cb6-236"><a href="#cb6-236"></a></span>
<span id="cb6-237"><a href="#cb6-237"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="cb6-238"><a href="#cb6-238"></a><span class="co">## of type string</span></span>
<span id="cb6-239"><a href="#cb6-239"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="cb6-240"><a href="#cb6-240"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="cb6-241"><a href="#cb6-241"></a><span class="co">## New columns:</span></span>
<span id="cb6-242"><a href="#cb6-242"></a><span class="co">## - stationId</span></span>
<span id="cb6-243"><a href="#cb6-243"></a><span class="co">## - freeslots</span></span>
<span id="cb6-244"><a href="#cb6-244"></a><span class="co">## - usedslots</span></span>
<span id="cb6-245"><a href="#cb6-245"></a><span class="co">## - timestamp</span></span>
<span id="cb6-246"><a href="#cb6-246"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="cb6-247"><a href="#cb6-247"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) <span class="op">\</span></span>
<span id="cb6-248"><a href="#cb6-248"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-249"><a href="#cb6-249"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-250"><a href="#cb6-250"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="cb6-251"><a href="#cb6-251"></a></span>
<span id="cb6-252"><a href="#cb6-252"></a><span class="co">## Filter data</span></span>
<span id="cb6-253"><a href="#cb6-253"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="cb6-254"><a href="#cb6-254"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="cb6-255"><a href="#cb6-255"></a></span>
<span id="cb6-256"><a href="#cb6-256"></a><span class="co">## Count the number of readings with a number of free slots equal to 0</span></span>
<span id="cb6-257"><a href="#cb6-257"></a><span class="co">## for each stationId</span></span>
<span id="cb6-258"><a href="#cb6-258"></a><span class="co">## The standard groupBy method is used</span></span>
<span id="cb6-259"><a href="#cb6-259"></a>countsDF <span class="op">=</span> fullReadingsDF <span class="op">\</span></span>
<span id="cb6-260"><a href="#cb6-260"></a>    .groupBy(<span class="st">"stationId"</span>) <span class="op">\</span></span>
<span id="cb6-261"><a href="#cb6-261"></a>    .agg({<span class="st">"*"</span>:<span class="st">"count"</span>}) <span class="co"># &lt;1&gt;</span></span>
<span id="cb6-262"><a href="#cb6-262"></a></span>
<span id="cb6-263"><a href="#cb6-263"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="cb6-264"><a href="#cb6-264"></a><span class="co">## the console "sink"</span></span>
<span id="cb6-265"><a href="#cb6-265"></a><span class="co">## complete output mode</span></span>
<span id="cb6-266"><a href="#cb6-266"></a><span class="co">## (append mode cannot be used for aggregation queries)</span></span>
<span id="cb6-267"><a href="#cb6-267"></a>queryCountStreamWriter <span class="op">=</span> countsDF <span class="op">\</span></span>
<span id="cb6-268"><a href="#cb6-268"></a>    .writeStream <span class="op">\</span></span>
<span id="cb6-269"><a href="#cb6-269"></a>    .outputMode(<span class="st">"complete"</span>) <span class="op">\</span></span>
<span id="cb6-270"><a href="#cb6-270"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="cb6-271"><a href="#cb6-271"></a></span>
<span id="cb6-272"><a href="#cb6-272"></a><span class="co">## Start the execution of the query (it will be executed until it is explicitly stopped)</span></span>
<span id="cb6-273"><a href="#cb6-273"></a>queryCount <span class="op">=</span> queryCountStreamWriter.start()</span>
<span id="cb6-274"><a href="#cb6-274"></a><span class="in">```</span></span>
<span id="cb6-275"><a href="#cb6-275"></a><span class="ss">1. </span><span class="in">`groupBy`</span> and <span class="in">`agg`</span> are standard DataFrame transformations</span>
<span id="cb6-276"><a href="#cb6-276"></a></span>
<span id="cb6-277"><a href="#cb6-277"></a><span class="fu">## Event time and window operations</span></span>
<span id="cb6-278"><a href="#cb6-278"></a>Input streaming records are usually characterized by a time information: it is usually called event-time, and it is the time when the data was generated. For many applications, you want to operate by taking into consideration the event-time and windows containing data associated with the same event-time range.</span>
<span id="cb6-279"><a href="#cb6-279"></a></span>
<span id="cb6-280"><a href="#cb6-280"></a>:::{.callout-note collapse="true"}</span>
<span id="cb6-281"><a href="#cb6-281"></a><span class="fu">### Example</span></span>
<span id="cb6-282"><a href="#cb6-282"></a>Compute the number of events generated by each monitored IoT device every minute based on the event-time. For each window associated with one distinct minute, consider only the data with an event-time inside that minute/window and compute the number of events for each IoT device: one computation for each minute/window. You want to use the time when the data was generated (i.e., the event-time) rather than the time Spark receives them.</span>
<span id="cb6-283"><a href="#cb6-283"></a>:::</span>
<span id="cb6-284"><a href="#cb6-284"></a></span>
<span id="cb6-285"><a href="#cb6-285"></a>Spark allows defining windows based on the time-event input column, and then apply aggregation functions over each window.</span>
<span id="cb6-286"><a href="#cb6-286"></a></span>
<span id="cb6-287"><a href="#cb6-287"></a>For each structured streaming query on which you want to apply a window computation you must specify</span>
<span id="cb6-288"><a href="#cb6-288"></a></span>
<span id="cb6-289"><a href="#cb6-289"></a><span class="ss">- </span>the name of the time-event column in the input (streaming) DataFrame</span>
<span id="cb6-290"><a href="#cb6-290"></a><span class="ss">- </span>the characteristics of the (sliding) windows</span>
<span id="cb6-291"><a href="#cb6-291"></a><span class="ss">    - </span><span class="in">`windowDuration`</span></span>
<span id="cb6-292"><a href="#cb6-292"></a><span class="ss">    - </span><span class="in">`slideDuration`</span></span>
<span id="cb6-293"><a href="#cb6-293"></a></span>
<span id="cb6-294"><a href="#cb6-294"></a>Do not set it if you want non-overlapped windows, (i.e., if you want to a <span class="in">`slideDuration`</span> equal to <span class="in">`windowDuration`</span>). You can set different window characteristics for each query of your application.</span>
<span id="cb6-295"><a href="#cb6-295"></a></span>
<span id="cb6-296"><a href="#cb6-296"></a>The <span class="in">`window(timeColumn, windowDuration, slideDuration=None)`</span> function is used inside the standard <span class="in">`groupBy()`</span> one to specify the characteristics of the windows. Notice that windows can be used only with queries that are applying aggregation functions.</span>
<span id="cb6-297"><a href="#cb6-297"></a></span>
<span id="cb6-298"><a href="#cb6-298"></a><span class="fu">### Event time and window operations: example 1</span></span>
<span id="cb6-299"><a href="#cb6-299"></a><span class="ss">- </span>Input</span>
<span id="cb6-300"><a href="#cb6-300"></a><span class="ss">    - </span>A stream of records retrieved from localhost:9999</span>
<span id="cb6-301"><a href="#cb6-301"></a><span class="ss">    - </span>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp</span>
<span id="cb6-302"><a href="#cb6-302"></a>    #- Each input reading has the format: "stationId", "# free slots", "#used slots", "timestamp"</span>
<span id="cb6-303"><a href="#cb6-303"></a><span class="ss">    - </span>timestamp is the event-time column</span>
<span id="cb6-304"><a href="#cb6-304"></a><span class="ss">- </span>Output</span>
<span id="cb6-305"><a href="#cb6-305"></a><span class="ss">    - </span>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window</span>
<span id="cb6-306"><a href="#cb6-306"></a><span class="ss">    - </span>The query is executed for each window</span>
<span id="cb6-307"><a href="#cb6-307"></a><span class="ss">    - </span>Set <span class="in">`windowDuration`</span> to 2 seconds and no <span class="in">`slideDuration`</span> (i.e., non-overlapped windows)</span>
<span id="cb6-308"><a href="#cb6-308"></a></span>
<span id="cb6-309"><a href="#cb6-309"></a><span class="al">![Step 1](images/22_structured_streaming/example_3_1.png)</span>{width=80%}</span>
<span id="cb6-310"><a href="#cb6-310"></a></span>
<span id="cb6-311"><a href="#cb6-311"></a>The returned result has a column called window. It contains the time slot associated with the window $[\text{from timestamp}, \text{to timestamp})$</span>
<span id="cb6-312"><a href="#cb6-312"></a></span>
<span id="cb6-313"><a href="#cb6-313"></a><span class="al">![Step 2](images/22_structured_streaming/example_3_2.png)</span>{width=80%}</span>
<span id="cb6-314"><a href="#cb6-314"></a></span>
<span id="cb6-315"><a href="#cb6-315"></a><span class="al">![Step 3](images/22_structured_streaming/example_3_3.png)</span>{width=80%}</span>
<span id="cb6-316"><a href="#cb6-316"></a></span>
<span id="cb6-317"><a href="#cb6-317"></a><span class="in">```python</span></span>
<span id="cb6-318"><a href="#cb6-318"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="cb6-319"><a href="#cb6-319"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="cb6-320"><a href="#cb6-320"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window</span>
<span id="cb6-321"><a href="#cb6-321"></a></span>
<span id="cb6-322"><a href="#cb6-322"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="cb6-323"><a href="#cb6-323"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb6-324"><a href="#cb6-324"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb6-325"><a href="#cb6-325"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb6-326"><a href="#cb6-326"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb6-327"><a href="#cb6-327"></a>    .load()</span>
<span id="cb6-328"><a href="#cb6-328"></a></span>
<span id="cb6-329"><a href="#cb6-329"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="cb6-330"><a href="#cb6-330"></a><span class="co">## of type string</span></span>
<span id="cb6-331"><a href="#cb6-331"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="cb6-332"><a href="#cb6-332"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="cb6-333"><a href="#cb6-333"></a><span class="co">## New columns:</span></span>
<span id="cb6-334"><a href="#cb6-334"></a><span class="co">## - stationId</span></span>
<span id="cb6-335"><a href="#cb6-335"></a><span class="co">## - freeslots</span></span>
<span id="cb6-336"><a href="#cb6-336"></a><span class="co">## - usedslots</span></span>
<span id="cb6-337"><a href="#cb6-337"></a><span class="co">## - timestamp</span></span>
<span id="cb6-338"><a href="#cb6-338"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="cb6-339"><a href="#cb6-339"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) <span class="op">\</span></span>
<span id="cb6-340"><a href="#cb6-340"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-341"><a href="#cb6-341"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-342"><a href="#cb6-342"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="cb6-343"><a href="#cb6-343"></a></span>
<span id="cb6-344"><a href="#cb6-344"></a><span class="co">## Filter data</span></span>
<span id="cb6-345"><a href="#cb6-345"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="cb6-346"><a href="#cb6-346"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="cb6-347"><a href="#cb6-347"></a></span>
<span id="cb6-348"><a href="#cb6-348"></a><span class="co">## Count the number of readings with a number of free slots equal to 0</span></span>
<span id="cb6-349"><a href="#cb6-349"></a><span class="co">## for each stationId in each window.</span></span>
<span id="cb6-350"><a href="#cb6-350"></a><span class="co">## windowDuration = 2 seconds</span></span>
<span id="cb6-351"><a href="#cb6-351"></a><span class="co">## no overlapping windows</span></span>
<span id="cb6-352"><a href="#cb6-352"></a>countsDF <span class="op">=</span> fullReadingsDF <span class="op">\</span></span>
<span id="cb6-353"><a href="#cb6-353"></a>    .groupBy(window(fullReadingsDF.timestamp, <span class="st">"2 seconds"</span>), <span class="st">"stationId"</span>) <span class="op">\</span></span>
<span id="cb6-354"><a href="#cb6-354"></a>    .agg({<span class="st">"*"</span>:<span class="st">"count"</span>}) <span class="op">\</span></span>
<span id="cb6-355"><a href="#cb6-355"></a>    .sort(<span class="st">"window"</span>)</span>
<span id="cb6-356"><a href="#cb6-356"></a></span>
<span id="cb6-357"><a href="#cb6-357"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="cb6-358"><a href="#cb6-358"></a><span class="co">## the console "sink"</span></span>
<span id="cb6-359"><a href="#cb6-359"></a><span class="co">## complete output mode</span></span>
<span id="cb6-360"><a href="#cb6-360"></a><span class="co">## (append mode cannot be used for aggregation queries)</span></span>
<span id="cb6-361"><a href="#cb6-361"></a>queryCountWindowStreamWriter <span class="op">=</span> countsDF <span class="op">\</span></span>
<span id="cb6-362"><a href="#cb6-362"></a>    .writeStream <span class="op">\</span></span>
<span id="cb6-363"><a href="#cb6-363"></a>    .outputMode(<span class="st">"complete"</span>) <span class="op">\</span></span>
<span id="cb6-364"><a href="#cb6-364"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>) <span class="op">\</span></span>
<span id="cb6-365"><a href="#cb6-365"></a>    .option(<span class="st">"truncate"</span>, <span class="st">"false"</span>)</span>
<span id="cb6-366"><a href="#cb6-366"></a></span>
<span id="cb6-367"><a href="#cb6-367"></a><span class="co">## Start the execution of the query (it will be executed until it is explicitly stopped)</span></span>
<span id="cb6-368"><a href="#cb6-368"></a>queryCountWindow <span class="op">=</span> queryCountWindowStreamWriter.start()</span>
<span id="cb6-369"><a href="#cb6-369"></a></span>
<span id="cb6-370"><a href="#cb6-370"></a><span class="in">```</span></span>
<span id="cb6-371"><a href="#cb6-371"></a></span>
<span id="cb6-372"><a href="#cb6-372"></a><span class="fu">### Late data</span></span>
<span id="cb6-373"><a href="#cb6-373"></a>Sparks handles data that have arrived later than expected based on its event-time; these are called late data. Spark has full control over updating old aggregates when there are late data: every time new data are processed the result is computed by combining old aggregate values and the new data by considering the event-time column instead of the time Spark receives the data.</span>
<span id="cb6-374"><a href="#cb6-374"></a></span>
<span id="cb6-375"><a href="#cb6-375"></a><span class="fu">#### Late data example</span></span>
<span id="cb6-376"><a href="#cb6-376"></a><span class="ss">- </span>Input</span>
<span id="cb6-377"><a href="#cb6-377"></a><span class="ss">    - </span>A stream of records retrieved from localhost:9999</span>
<span id="cb6-378"><a href="#cb6-378"></a><span class="ss">    - </span>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp</span>
<span id="cb6-379"><a href="#cb6-379"></a>    #- Each input reading has the format: "stationId", "# free slots", "#used slots", "timestamp"</span>
<span id="cb6-380"><a href="#cb6-380"></a><span class="ss">    - </span>timestamp is the event-time column</span>
<span id="cb6-381"><a href="#cb6-381"></a><span class="ss">- </span>Output</span>
<span id="cb6-382"><a href="#cb6-382"></a><span class="ss">    - </span>For each stationId, print on the standard output the total number of received input readings with a number of free slots equal to 0 in each window</span>
<span id="cb6-383"><a href="#cb6-383"></a><span class="ss">    - </span>The query is executed for each window</span>
<span id="cb6-384"><a href="#cb6-384"></a><span class="ss">    - </span>Set <span class="in">`windowDuration`</span> to 2 seconds and no <span class="in">`slideDuration`</span> (i.e., non-overlapped windows)</span>
<span id="cb6-385"><a href="#cb6-385"></a></span>
<span id="cb6-386"><a href="#cb6-386"></a><span class="al">![Step 1](images/22_structured_streaming/late_data_example_1.png)</span>{width=80%}</span>
<span id="cb6-387"><a href="#cb6-387"></a></span>
<span id="cb6-388"><a href="#cb6-388"></a><span class="al">![Step 2](images/22_structured_streaming/late_data_example_2.png)</span>{width=80%}</span>
<span id="cb6-389"><a href="#cb6-389"></a></span>
<span id="cb6-390"><a href="#cb6-390"></a><span class="al">![Step 3](images/22_structured_streaming/late_data_example_3.png)</span>{width=80%}</span>
<span id="cb6-391"><a href="#cb6-391"></a></span>
<span id="cb6-392"><a href="#cb6-392"></a>Notice that late data that was generated at ***2016-03-11 09:00:06***, but arrived at ***2016-03-11 09:00:08***: the result consider also late data and assign them to the right window by considering the event-time information.</span>
<span id="cb6-393"><a href="#cb6-393"></a></span>
<span id="cb6-394"><a href="#cb6-394"></a>The code is the same of the previous example (Event time and window operations: example 1): late data are automatically handled by Spark.</span>
<span id="cb6-395"><a href="#cb6-395"></a></span>
<span id="cb6-396"><a href="#cb6-396"></a><span class="fu">### Event time and window operations: example 2</span></span>
<span id="cb6-397"><a href="#cb6-397"></a><span class="ss">- </span>Input</span>
<span id="cb6-398"><a href="#cb6-398"></a><span class="ss">    - </span>A stream of records retrieved from localhost:9999</span>
<span id="cb6-399"><a href="#cb6-399"></a><span class="ss">    - </span>Each input record is a reading about the status of a station of a bike sharing system in a specific timestamp</span>
<span id="cb6-400"><a href="#cb6-400"></a>    #- Each input reading has the format: "stationId", "# free slots", "#used slots", "timestamp"</span>
<span id="cb6-401"><a href="#cb6-401"></a><span class="ss">    - </span>timestamp is the event-time column</span>
<span id="cb6-402"><a href="#cb6-402"></a><span class="ss">- </span>Output</span>
<span id="cb6-403"><a href="#cb6-403"></a><span class="ss">    - </span>For each window, print on the standard output the total number of received input readings with a number of free slots equal to 0</span>
<span id="cb6-404"><a href="#cb6-404"></a><span class="ss">    - </span>The query is executed for each window</span>
<span id="cb6-405"><a href="#cb6-405"></a><span class="ss">    - </span>Set windowDuration to 2 seconds and no slideDuration (i.e., non-overlapped windows)</span>
<span id="cb6-406"><a href="#cb6-406"></a></span>
<span id="cb6-407"><a href="#cb6-407"></a><span class="al">![Step 1](images/22_structured_streaming/example_4_1.png)</span>{width=80%}</span>
<span id="cb6-408"><a href="#cb6-408"></a></span>
<span id="cb6-409"><a href="#cb6-409"></a><span class="al">![Step 2](images/22_structured_streaming/example_4_2.png)</span>{width=80%}</span>
<span id="cb6-410"><a href="#cb6-410"></a></span>
<span id="cb6-411"><a href="#cb6-411"></a><span class="al">![Step 3](images/22_structured_streaming/example_4_3.png)</span>{width=80%}</span>
<span id="cb6-412"><a href="#cb6-412"></a></span>
<span id="cb6-413"><a href="#cb6-413"></a><span class="in">```python</span></span>
<span id="cb6-414"><a href="#cb6-414"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> <span class="op">*</span></span>
<span id="cb6-415"><a href="#cb6-415"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> split</span>
<span id="cb6-416"><a href="#cb6-416"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window</span>
<span id="cb6-417"><a href="#cb6-417"></a></span>
<span id="cb6-418"><a href="#cb6-418"></a><span class="co">## Create a "receiver" DataFrame that will connect to localhost:9999</span></span>
<span id="cb6-419"><a href="#cb6-419"></a>recordsDF <span class="op">=</span> spark.readStream <span class="op">\</span></span>
<span id="cb6-420"><a href="#cb6-420"></a>    .<span class="bu">format</span>(<span class="st">"socket"</span>) <span class="op">\</span></span>
<span id="cb6-421"><a href="#cb6-421"></a>    .option(<span class="st">"host"</span>, <span class="st">"localhost"</span>) <span class="op">\</span></span>
<span id="cb6-422"><a href="#cb6-422"></a>    .option(<span class="st">"port"</span>, <span class="dv">9999</span>) <span class="op">\</span></span>
<span id="cb6-423"><a href="#cb6-423"></a>    .load()</span>
<span id="cb6-424"><a href="#cb6-424"></a></span>
<span id="cb6-425"><a href="#cb6-425"></a><span class="co">## The input records are characterized by one single column called value</span></span>
<span id="cb6-426"><a href="#cb6-426"></a><span class="co">## of type string</span></span>
<span id="cb6-427"><a href="#cb6-427"></a><span class="co">## Example of an input record: s1,0,3,2016-03-11 09:00:04</span></span>
<span id="cb6-428"><a href="#cb6-428"></a><span class="co">## Define four more columns by splitting the input column value</span></span>
<span id="cb6-429"><a href="#cb6-429"></a><span class="co">## New columns:</span></span>
<span id="cb6-430"><a href="#cb6-430"></a><span class="co">## - stationId</span></span>
<span id="cb6-431"><a href="#cb6-431"></a><span class="co">## - freeslots</span></span>
<span id="cb6-432"><a href="#cb6-432"></a><span class="co">## - usedslots</span></span>
<span id="cb6-433"><a href="#cb6-433"></a><span class="co">## - timestamp</span></span>
<span id="cb6-434"><a href="#cb6-434"></a>readingsDF <span class="op">=</span> recordsDF <span class="op">\</span></span>
<span id="cb6-435"><a href="#cb6-435"></a>    .withColumn(<span class="st">"stationId"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">0</span>].cast(<span class="st">"string"</span>)) <span class="op">\</span></span>
<span id="cb6-436"><a href="#cb6-436"></a>    .withColumn(<span class="st">"freeslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">1</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-437"><a href="#cb6-437"></a>    .withColumn(<span class="st">"usedslots"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">2</span>].cast(<span class="st">"integer"</span>)) <span class="op">\</span></span>
<span id="cb6-438"><a href="#cb6-438"></a>    .withColumn(<span class="st">"timestamp"</span>, split(recordsDF.value, <span class="st">','</span>)[<span class="dv">3</span>].cast(<span class="st">"timestamp"</span>))</span>
<span id="cb6-439"><a href="#cb6-439"></a></span>
<span id="cb6-440"><a href="#cb6-440"></a><span class="co">## Filter data</span></span>
<span id="cb6-441"><a href="#cb6-441"></a><span class="co">## Use the standard filter transformation</span></span>
<span id="cb6-442"><a href="#cb6-442"></a>fullReadingsDF <span class="op">=</span> readingsDF.<span class="bu">filter</span>(<span class="st">"freeslots=0"</span>)</span>
<span id="cb6-443"><a href="#cb6-443"></a></span>
<span id="cb6-444"><a href="#cb6-444"></a><span class="co">## Count the number of readings with a number of free slots equal to 0</span></span>
<span id="cb6-445"><a href="#cb6-445"></a><span class="co">## for in each window.</span></span>
<span id="cb6-446"><a href="#cb6-446"></a><span class="co">## windowDuration = 2 seconds</span></span>
<span id="cb6-447"><a href="#cb6-447"></a><span class="co">## no overlapping windows</span></span>
<span id="cb6-448"><a href="#cb6-448"></a>countsDF <span class="op">=</span> fullReadingsDF <span class="op">\</span></span>
<span id="cb6-449"><a href="#cb6-449"></a>    .groupBy(window(fullReadingsDF.timestamp, <span class="st">"2 seconds"</span>)) <span class="op">\</span></span>
<span id="cb6-450"><a href="#cb6-450"></a>    .agg({<span class="st">"*"</span>:<span class="st">"count"</span>}) <span class="op">\</span></span>
<span id="cb6-451"><a href="#cb6-451"></a>    .sort(<span class="st">"window"</span>)</span>
<span id="cb6-452"><a href="#cb6-452"></a></span>
<span id="cb6-453"><a href="#cb6-453"></a><span class="co">## The result of the structured streaming query will be stored/printed on</span></span>
<span id="cb6-454"><a href="#cb6-454"></a><span class="co">## the console "sink"</span></span>
<span id="cb6-455"><a href="#cb6-455"></a><span class="co">## complete output mode</span></span>
<span id="cb6-456"><a href="#cb6-456"></a><span class="co">## (append mode cannot be used for aggregation queries)</span></span>
<span id="cb6-457"><a href="#cb6-457"></a>queryCountWindowStreamWriter <span class="op">=</span> countsDF <span class="op">\</span></span>
<span id="cb6-458"><a href="#cb6-458"></a>    .writeStream <span class="op">\</span></span>
<span id="cb6-459"><a href="#cb6-459"></a>    .outputMode(<span class="st">"complete"</span>) <span class="op">\</span></span>
<span id="cb6-460"><a href="#cb6-460"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>) <span class="op">\</span></span>
<span id="cb6-461"><a href="#cb6-461"></a>    .option(<span class="st">"truncate"</span>, <span class="st">"false"</span>)</span>
<span id="cb6-462"><a href="#cb6-462"></a></span>
<span id="cb6-463"><a href="#cb6-463"></a><span class="co">## Start the execution of the query (it will be executed until it is </span></span>
<span id="cb6-464"><a href="#cb6-464"></a><span class="co">## explicitly stopped)</span></span>
<span id="cb6-465"><a href="#cb6-465"></a>queryCountWindow <span class="op">=</span> queryCountWindowStreamWriter.start()</span>
<span id="cb6-466"><a href="#cb6-466"></a></span>
<span id="cb6-467"><a href="#cb6-467"></a><span class="in">```</span></span>
<span id="cb6-468"><a href="#cb6-468"></a></span>
<span id="cb6-469"><a href="#cb6-469"></a><span class="fu">### Watermarking</span></span>
<span id="cb6-470"><a href="#cb6-470"></a>Watermarking is a feature of Spark that allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state. Results related to old event-times are not needed in many real streaming applications: they can be dropped to improve the efficiency of the application, since keeping the state of old results is resource expensive; in this way every time new data are processed only recent records are considered.</span>
<span id="cb6-471"><a href="#cb6-471"></a></span>
<span id="cb6-472"><a href="#cb6-472"></a>Specifically, to run windowed queries for days, it is necessary for the system to bound the amount of intermediate in-memory state it accumulates. This means the system needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more; to enable this, in Spark 2.1, watermarking has been introduced.</span>
<span id="cb6-473"><a href="#cb6-473"></a></span>
<span id="cb6-474"><a href="#cb6-474"></a>Watermarking lets the Spark Structured Streaming engine automatically track the current event time in the data and attempt to clean up old state accordingly. It allows to define the watermark of a query by specifying the event time column and the threshold on how late the data is expected to be in terms of event time: for a specific window ending at time $T$, the engine will maintain state and allow late data to update the state/the result until max event time seen by the engine $&lt; T + \text{late threshold}$. In other words, late data within the threshold will be aggregated, but data later than ${T + \text{threshold}}$ will be dropped.</span>
<span id="cb6-475"><a href="#cb6-475"></a></span>
<span id="cb6-476"><a href="#cb6-476"></a><span class="fu">## Join operations</span></span>
<span id="cb6-477"><a href="#cb6-477"></a>Spark Structured Streaming manages also join operations</span>
<span id="cb6-478"><a href="#cb6-478"></a></span>
<span id="cb6-479"><a href="#cb6-479"></a><span class="ss">- </span>Between two streaming DataFrames</span>
<span id="cb6-480"><a href="#cb6-480"></a><span class="ss">- </span>Between a streaming DataFrame and a static DataFrame</span>
<span id="cb6-481"><a href="#cb6-481"></a></span>
<span id="cb6-482"><a href="#cb6-482"></a>The result of the streaming join is generated incrementally.</span>
<span id="cb6-483"><a href="#cb6-483"></a></span>
<span id="cb6-484"><a href="#cb6-484"></a>When joining between two streaming DataFrames, for both input streams, past input streaming data must be buffered/recorded in order to be able to match every future input record with past input data and accordingly generate joined results. Too many resources are needed for storing all the input data, hence, old data must be discarded. Watermark thresholds must be defined on both input streams such that the engine knows how delayed the input can be and drop old data.</span>
<span id="cb6-485"><a href="#cb6-485"></a></span>
<span id="cb6-486"><a href="#cb6-486"></a>The methods <span class="in">`join()`</span> and <span class="in">`withWatermark()`</span> are used to join streaming DataFrames: the join method is similar to the one available for static DataFrame.</span>
<span id="cb6-487"><a href="#cb6-487"></a></span>
<span id="cb6-488"><a href="#cb6-488"></a><span class="in">```python</span></span>
<span id="cb6-489"><a href="#cb6-489"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> expr</span>
<span id="cb6-490"><a href="#cb6-490"></a>impressions <span class="op">=</span> spark.readStream. ...</span>
<span id="cb6-491"><a href="#cb6-491"></a>clicks <span class="op">=</span> spark.readStream. ...</span>
<span id="cb6-492"><a href="#cb6-492"></a></span>
<span id="cb6-493"><a href="#cb6-493"></a><span class="co">## Apply watermarks on event-time columns</span></span>
<span id="cb6-494"><a href="#cb6-494"></a>impressionsWithWatermark <span class="op">=</span> impressions <span class="op">\</span></span>
<span id="cb6-495"><a href="#cb6-495"></a>    .withWatermark(<span class="st">"impressionTime"</span>, <span class="st">"2 hours"</span>)</span>
<span id="cb6-496"><a href="#cb6-496"></a>    </span>
<span id="cb6-497"><a href="#cb6-497"></a>clicksWithWatermark <span class="op">=</span> clicks <span class="op">\</span></span>
<span id="cb6-498"><a href="#cb6-498"></a>    .withWatermark(<span class="st">"clickTime"</span>, <span class="st">"3 hours"</span>)</span>
<span id="cb6-499"><a href="#cb6-499"></a></span>
<span id="cb6-500"><a href="#cb6-500"></a><span class="co">## Join with event-time constraints</span></span>
<span id="cb6-501"><a href="#cb6-501"></a>impressionsWithWatermark.join(</span>
<span id="cb6-502"><a href="#cb6-502"></a>    clicksWithWatermark,</span>
<span id="cb6-503"><a href="#cb6-503"></a>    expr(</span>
<span id="cb6-504"><a href="#cb6-504"></a>        <span class="st">"""</span></span>
<span id="cb6-505"><a href="#cb6-505"></a><span class="st">        clickAdId = impressionAdId AND </span></span>
<span id="cb6-506"><a href="#cb6-506"></a><span class="st">        clickTime &gt;= impressionTime AND</span></span>
<span id="cb6-507"><a href="#cb6-507"></a><span class="st">        clickTime &lt;= impressionTime + interval 1 hour"""</span></span>
<span id="cb6-508"><a href="#cb6-508"></a>    ) </span>
<span id="cb6-509"><a href="#cb6-509"></a>)</span>
<span id="cb6-510"><a href="#cb6-510"></a></span>
<span id="cb6-511"><a href="#cb6-511"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>