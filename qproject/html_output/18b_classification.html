<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Distributed architectures for big data processing and analytics - 20&nbsp; Classification algorithms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./18c_clustering.html" rel="next">
<link href="./18a_spark_mllib.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18b_classification.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Classification algorithms</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Distributed architectures for big data processing and analytics</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">About this Book</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Big data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Big data architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03b_HDFS_clc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">HDFS and Hadoop: command line commands</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_intro_hadoop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Hadoop and MapReduce</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hadoop_implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">How to write MapReduce programs in Hadoop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_mapreduce_patterns_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">MapReduce patterns - 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_mapreduce_advanced_topics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">MapReduce and Hadoop Advanced Topics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_mapreduce_patterns_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">MapReduce patterns - 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_sql_operators_mapreduce.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Relational Algebra Operations and MapReduce</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10b_spark_submit_execute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">How to submit/execute a Spark application</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_intro_spark.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_rdd_based_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">RDD based programming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_rdd_keyvalue_pairs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">RDDs and key-value pairs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_rdd_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RDD of numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_cache_accumulators_broadcast.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cache, Accumulators, Broadcast Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15b_pagerank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to PageRank</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_sparksql_dataframes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Spark SQL and DataFrames</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18a_spark_mllib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Spark MLlib</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18b_classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Classification algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18c_clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Clustering algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18d_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Regression algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18e_mining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Itemset and Association rule mining</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_graph_analytics_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Graph analytics in Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_graph_analytics_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Graph Analytics in Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21_streaming_analytics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Streaming data analytics frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22_structured_streaming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Spark structured streaming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23_streaming_frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Streaming data analytics frameworks</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#structured-data-classification" id="toc-structured-data-classification" class="nav-link active" data-scroll-target="#structured-data-classification">Structured data classification</a></li>
  <li><a href="#categorical-class-labels" id="toc-categorical-class-labels" class="nav-link" data-scroll-target="#categorical-class-labels">Categorical class labels</a></li>
  <li><a href="#textual-data-management-and-classification" id="toc-textual-data-management-and-classification" class="nav-link" data-scroll-target="#textual-data-management-and-classification">Textual data management and classification</a></li>
  <li><a href="#performance-evaluation" id="toc-performance-evaluation" class="nav-link" data-scroll-target="#performance-evaluation">Performance evaluation</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter tuning</a></li>
  <li><a href="#sparse-labeled-data" id="toc-sparse-labeled-data" class="nav-link" data-scroll-target="#sparse-labeled-data">Sparse labeled data</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Classification algorithms</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Spark MLlib provides a (limited) set of classification algorithms</p>
<ul>
<li>Logistic regression
<ul>
<li>Binomial logistic regression</li>
<li>Multinomial logistic regression</li>
</ul></li>
<li>Decision tree classifier</li>
<li>Random forest classifier</li>
<li>Gradient-boosted tree classifier</li>
<li>Multilayer perceptron classifier</li>
<li>Linear Support Vector Machine</li>
</ul>
<p>All the available classification algorithms are based on two phases:</p>
<ol type="1">
<li>Model generation based on a set of training data</li>
<li>Prediction of the class label of new unlabeled data</li>
</ol>
<p>All the classification algorithms available in Spark work only on numerical attributes: categorical values must be mapped to integer values (one distinct value per class) before applying the MLlib classification algorithms.</p>
<p>All the Spark classification algorithms are trained on top of an input DataFrame containing (at least) two columns</p>
<ul>
<li>label: the class label, (i.e., the attribute to be predicted by the classification model); it is an integer value (casted to a double)</li>
<li>features: a vector of doubles containing the values of the predictive attributes of the input records/data points; the data type of this column is <code>pyspark.ml.linalg.Vector</code>, and both dense and sparse vectors can be used</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider the following classification problem: the goal is to predict if new customers are good customers or not based on their monthly income and number of children.</p>
<p>The predictive attributes are</p>
<ul>
<li>Monthly income</li>
<li>Number of children</li>
</ul>
<p>The class label (target attribute) is “Customer type”:</p>
<ul>
<li>“Good customer”, mapped to 1</li>
<li>“Bad customer”, mapped to 0</li>
</ul>
<p><strong>Example of input training data</strong></p>
<p>The training data is the set of customers for which the value of the class label is known: they are used by the classification algorithm to infer/train a classification model.</p>
<table class="table">
<thead>
<tr class="header">
<th>CustomerType</th>
<th>MonthlyIncome</th>
<th>NumChildren</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Good customer</td>
<td><span class="math inline">\(1400.0\)</span></td>
<td><span class="math inline">\(2\)</span></td>
</tr>
<tr class="even">
<td>Bad customer</td>
<td><span class="math inline">\(11105.5\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>Good customer</td>
<td><span class="math inline">\(2150.0\)</span></td>
<td><span class="math inline">\(2\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Example of input training DataFrame</strong></p>
<p>The input training DataFrame that must be provided as input to train an MLlib classification algorithm must have the following structure</p>
<table class="table">
<thead>
<tr class="header">
<th>label</th>
<th>features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\([1400.0,2.0]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(0.0\)</span></td>
<td><span class="math inline">\([11105.5,0.0]\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\([2150.0,2.0]\)</span></td>
</tr>
</tbody>
</table>
<p>Notice that</p>
<ul>
<li>The categorical values of “CustomerType” (the class label column) must be mapped to integer data values (then casted to doubles).</li>
<li>The values of the predictive attributes are stored in vectors of doubles. One single vector for each input record.</li>
<li>In the generated DataFrame the names of the predictive attributes are not preserved.</li>
</ul>
</div>
</div>
</div>
<section id="structured-data-classification" class="level3">
<h3 class="anchored" data-anchor-id="structured-data-classification">Structured data classification</h3>
<section id="example-of-logistic-regression-and-structured-data" class="level4">
<h4 class="anchored" data-anchor-id="example-of-logistic-regression-and-structured-data">Example of logistic regression and structured data</h4>
<p>The following paragraphs show how to</p>
<ul>
<li>Create a classification model based on the logistic regression algorithm on structured data: the model is inferred by analyzing the training data, (i.e., the example records/data points for which the value of the class label is known).</li>
<li>Apply the model to new unlabeled data: the inferred model is applied to predict the value of the class label of new unlabeled records/data points.</li>
</ul>
<section id="training-data" class="level5">
<h5 class="anchored" data-anchor-id="training-data">Training data</h5>
<p>The input training data is stored in a text file that contains one record/data point per line. The records/data points are structured data with a fixed number of attributes (four)</p>
<ul>
<li>One attribute is the class label: it assumed that the first column of each record contains the class label;</li>
<li>The other three attributes are the predictive attributes that are used to predict the value of the class label;</li>
</ul>
<p>The values are already doubles (no need to convert them), and the input file has the header line.</p>
<p>Consider the following example input training data file</p>
<pre><code>label,attr1,attr2,attr3
1.0,0.0,1.1,0.1
0.0,2.0,1.0,-1.0
0.0,2.0,1.3,1.0
1.0,0.0,1.2,-0.5</code></pre>
<p>It contains four records/data points. This is a binary classification problem because the class label assumes only two values: 0 and 1.</p>
<p>The first operation consists in transforming the content of the input training file into a DataFrame containing two columns</p>
<ul>
<li>label: the double value that is used to specify the label of each training record;</li>
<li>features: it is a vector of doubles associated with the values of the predictive features.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>label</th>
<th>features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\([0.0,1.1,0.1]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(0.0\)</span></td>
<td><span class="math inline">\([2.0,1.0,-1.0]\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(0.0\)</span></td>
<td><span class="math inline">\([2.0,1.3,1.0]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\([0.0,1.2,-0.5]\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Data type of “label” is double</li>
<li>Data type of “features” is <code>pyspark.ml.linalg.Vector</code></li>
</ul>
</section>
<section id="unlabeled-data" class="level5">
<h5 class="anchored" data-anchor-id="unlabeled-data">Unlabeled data</h5>
<p>The file containing the unlabeled data has the same format of the training data file, however the first column is empty because the class label is unknown. The goal is to predict the class label value of each unlabeled data by applying the classification model that has been trained on the training data: the predicted class label value of the unlabeled data is stored in a new column, called “prediction”, of the returned DataFrame.</p>
<p>Consider the following input unlabeled data file</p>
<pre><code>label,attr1,attr2,attr3
,-1.0,1.5,1.3
,3.0,2.0,-0.1
,0.0,2.2,-1.5</code></pre>
<p>It contains three unlabeled records/data points. Notice that the first column is empty (the content before the first comma is the empty string).</p>
<p>Also the unlabeled data must be stored into a DataFrame containing two columns: “label” and “features”. So, “label” column is required also for unlabeled data, but its value is set to null for all records.</p>
<table class="table">
<thead>
<tr class="header">
<th>label</th>
<th>features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>null</td>
<td><span class="math inline">\([-1.0,1.5,1.3]\)</span></td>
</tr>
<tr class="even">
<td>null</td>
<td><span class="math inline">\([3.0,2.0,-0.1]\)</span></td>
</tr>
<tr class="odd">
<td>null</td>
<td><span class="math inline">\([0.0,2.2,-1.5]\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="prediction-column" class="level5">
<h5 class="anchored" data-anchor-id="prediction-column">Prediction column</h5>
<p>After the application of the classification model on the unlabeled data, Spark returns a new DataFrame containing</p>
<ul>
<li>The same columns of the input DataFrame</li>
<li>A new column called prediction, that, for each input unlabeled record, contains the predicted class label value</li>
<li>Two columns, associated with the probabilities of the predictions (these columns are not considered in the example)</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>label</th>
<th>features</th>
<th>prediction</th>
<th>rawPrediction</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>null</td>
<td><span class="math inline">\([-1.0,1.5,1.3]\)</span></td>
<td><span class="math inline">\(1.0\)</span></td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>null</td>
<td><span class="math inline">\([3.0,2.0,-0.1]\)</span></td>
<td><span class="math inline">\(0.0\)</span></td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>null</td>
<td><span class="math inline">\([0.0,2.2,-1.5]\)</span></td>
<td><span class="math inline">\(1.0\)</span></td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>The “prediction” column contains the predicted class label values.</p>
</section>
<section id="example-code" class="level5">
<h5 class="anchored" data-anchor-id="example-code">Example code</h5>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">## input and output folders</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>trainingData <span class="op">=</span> <span class="st">"ex_data/trainingData.csv"</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_data/unlabeledData.csv"</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLR/"</span></span>
<span id="cb3-9"><a href="#cb3-9"></a></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">## *************************</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">## Training step</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">## *************************</span></span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co">## Training data in raw format</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="cb3-17"><a href="#cb3-17"></a>    trainingData,</span>
<span id="cb3-18"><a href="#cb3-18"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb3-19"><a href="#cb3-19"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-20"><a href="#cb3-20"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>)</span>
<span id="cb3-22"><a href="#cb3-22"></a></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb3-24"><a href="#cb3-24"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb3-25"><a href="#cb3-25"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb3-26"><a href="#cb3-26"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="cb3-27"><a href="#cb3-27"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb3-28"><a href="#cb3-28"></a>)</span>
<span id="cb3-29"><a href="#cb3-29"></a></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="co">## Apply the assembler to create column features for the training data</span></span>
<span id="cb3-31"><a href="#cb3-31"></a>trainingDataDF <span class="op">=</span> assembler.transform(trainingData)</span>
<span id="cb3-32"><a href="#cb3-32"></a></span>
<span id="cb3-33"><a href="#cb3-33"></a><span class="co">## Create a LogisticRegression object.</span></span>
<span id="cb3-34"><a href="#cb3-34"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="cb3-35"><a href="#cb3-35"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="cb3-37"><a href="#cb3-37"></a></span>
<span id="cb3-38"><a href="#cb3-38"></a><span class="co">## It is possible to set the values of the parameters of the</span></span>
<span id="cb3-39"><a href="#cb3-39"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="cb3-40"><a href="#cb3-40"></a><span class="co">## There is one set method for each parameter</span></span>
<span id="cb3-41"><a href="#cb3-41"></a><span class="co">## For example, the number of maximum iterations is set to 10</span></span>
<span id="cb3-42"><a href="#cb3-42"></a><span class="co">## and the regularization parameter is set to 0.01</span></span>
<span id="cb3-43"><a href="#cb3-43"></a>lr.setMaxIter(<span class="dv">10</span>)</span>
<span id="cb3-44"><a href="#cb3-44"></a>lr.setRegParam(<span class="fl">0.01</span>)</span>
<span id="cb3-45"><a href="#cb3-45"></a></span>
<span id="cb3-46"><a href="#cb3-46"></a><span class="co">## Train a logistic regression model on the training data</span></span>
<span id="cb3-47"><a href="#cb3-47"></a>classificationModel <span class="op">=</span> lr.fit(trainingDataDF)</span>
<span id="cb3-48"><a href="#cb3-48"></a></span>
<span id="cb3-49"><a href="#cb3-49"></a><span class="co">## *************************</span></span>
<span id="cb3-50"><a href="#cb3-50"></a><span class="co">## Prediction step</span></span>
<span id="cb3-51"><a href="#cb3-51"></a><span class="co">## *************************</span></span>
<span id="cb3-52"><a href="#cb3-52"></a></span>
<span id="cb3-53"><a href="#cb3-53"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb3-54"><a href="#cb3-54"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb3-55"><a href="#cb3-55"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb3-56"><a href="#cb3-56"></a>    unlabeledData,</span>
<span id="cb3-57"><a href="#cb3-57"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb3-58"><a href="#cb3-58"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-59"><a href="#cb3-59"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-60"><a href="#cb3-60"></a>)</span>
<span id="cb3-61"><a href="#cb3-61"></a></span>
<span id="cb3-62"><a href="#cb3-62"></a><span class="co">## Apply the same assembler we created before also on the unlabeled data</span></span>
<span id="cb3-63"><a href="#cb3-63"></a><span class="co">## to create the features column</span></span>
<span id="cb3-64"><a href="#cb3-64"></a>unlabeledDataDF <span class="op">=</span> assembler.transform(unlabeledData)</span>
<span id="cb3-65"><a href="#cb3-65"></a></span>
<span id="cb3-66"><a href="#cb3-66"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="cb3-67"><a href="#cb3-67"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="cb3-68"><a href="#cb3-68"></a><span class="co">## to perform the predictions</span></span>
<span id="cb3-69"><a href="#cb3-69"></a>predictionsDF <span class="op">=</span> classificationModel.transform(unlabeledDataDF)</span>
<span id="cb3-70"><a href="#cb3-70"></a></span>
<span id="cb3-71"><a href="#cb3-71"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb3-72"><a href="#cb3-72"></a><span class="co">## - attr1</span></span>
<span id="cb3-73"><a href="#cb3-73"></a><span class="co">## - attr2</span></span>
<span id="cb3-74"><a href="#cb3-74"></a><span class="co">## - attr3</span></span>
<span id="cb3-75"><a href="#cb3-75"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="cb3-76"><a href="#cb3-76"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="cb3-77"><a href="#cb3-77"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="cb3-78"><a href="#cb3-78"></a><span class="co">## - probability: vector (The i-th cell contains the probability that </span></span>
<span id="cb3-79"><a href="#cb3-79"></a><span class="co">## the current record belongs to the i-th class</span></span>
<span id="cb3-80"><a href="#cb3-80"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="cb3-81"><a href="#cb3-81"></a></span>
<span id="cb3-82"><a href="#cb3-82"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="cb3-83"><a href="#cb3-83"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="cb3-84"><a href="#cb3-84"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>, <span class="st">"prediction"</span>)</span>
<span id="cb3-85"><a href="#cb3-85"></a></span>
<span id="cb3-86"><a href="#cb3-86"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb3-87"><a href="#cb3-87"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="pipelines" class="level4">
<h4 class="anchored" data-anchor-id="pipelines">Pipelines</h4>
<p>In the previous solution the same preprocessing steps were applied on both training and unlabeled data (the same assembler on both input data). It is possible to use a pipeline to specify the common phases we apply on both input data sets.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a></span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a><span class="co">## input and output folders</span></span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a>trainingData <span class="op">=</span> <span class="st">"ex_data/trainingData.csv"</span></span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_data/unlabeledData.csv"</span></span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLR/"</span></span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11"></a></span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13"></a><span class="co">## Training step</span></span>
<span id="annotated-cell-1-14"><a href="#annotated-cell-1-14"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-15"><a href="#annotated-cell-1-15"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="annotated-cell-1-16"><a href="#annotated-cell-1-16"></a><span class="co">## Training data in raw format</span></span>
<span id="annotated-cell-1-17"><a href="#annotated-cell-1-17"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-1-18"><a href="#annotated-cell-1-18"></a>    trainingData,</span>
<span id="annotated-cell-1-19"><a href="#annotated-cell-1-19"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-1-20"><a href="#annotated-cell-1-20"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="annotated-cell-1-21"><a href="#annotated-cell-1-21"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-1-22"><a href="#annotated-cell-1-22"></a>)</span>
<span id="annotated-cell-1-23"><a href="#annotated-cell-1-23"></a></span>
<span id="annotated-cell-1-24"><a href="#annotated-cell-1-24"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="annotated-cell-1-25"><a href="#annotated-cell-1-25"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="annotated-cell-1-26"><a href="#annotated-cell-1-26"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="annotated-cell-1-27"><a href="#annotated-cell-1-27"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="annotated-cell-1-28"><a href="#annotated-cell-1-28"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="annotated-cell-1-29"><a href="#annotated-cell-1-29"></a>)</span>
<span id="annotated-cell-1-30"><a href="#annotated-cell-1-30"></a></span>
<span id="annotated-cell-1-31"><a href="#annotated-cell-1-31"></a><span class="co">## Create a LogisticRegression object</span></span>
<span id="annotated-cell-1-32"><a href="#annotated-cell-1-32"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="annotated-cell-1-33"><a href="#annotated-cell-1-33"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-34" class="code-annotation-target"><a href="#annotated-cell-1-34"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="annotated-cell-1-35"><a href="#annotated-cell-1-35"></a></span>
<span id="annotated-cell-1-36"><a href="#annotated-cell-1-36"></a><span class="co">## Set the values of the parameters of the</span></span>
<span id="annotated-cell-1-37"><a href="#annotated-cell-1-37"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="annotated-cell-1-38"><a href="#annotated-cell-1-38"></a><span class="co">## There is one set method for each parameter</span></span>
<span id="annotated-cell-1-39"><a href="#annotated-cell-1-39"></a><span class="co">## For example, we are setting the number of maximum iterations to 10</span></span>
<span id="annotated-cell-1-40"><a href="#annotated-cell-1-40"></a><span class="co">## and the regularization parameter to 0.01</span></span>
<span id="annotated-cell-1-41"><a href="#annotated-cell-1-41"></a>lr.setMaxIter(<span class="dv">10</span>)</span>
<span id="annotated-cell-1-42"><a href="#annotated-cell-1-42"></a>lr.setRegParam(<span class="fl">0.01</span>)</span>
<span id="annotated-cell-1-43"><a href="#annotated-cell-1-43"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-44" class="code-annotation-target"><a href="#annotated-cell-1-44"></a><span class="co">## Define a pipeline that is used to create the logistic regression</span></span>
<span id="annotated-cell-1-45"><a href="#annotated-cell-1-45"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="annotated-cell-1-46"><a href="#annotated-cell-1-46"></a><span class="co">## the preprocessing step</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-47" class="code-annotation-target"><a href="#annotated-cell-1-47"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, lr])</span>
<span id="annotated-cell-1-48"><a href="#annotated-cell-1-48"></a></span>
<span id="annotated-cell-1-49"><a href="#annotated-cell-1-49"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="annotated-cell-1-50"><a href="#annotated-cell-1-50"></a><span class="co">## classification model</span></span>
<span id="annotated-cell-1-51"><a href="#annotated-cell-1-51"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-52" class="code-annotation-target"><a href="#annotated-cell-1-52"></a></span>
<span id="annotated-cell-1-53"><a href="#annotated-cell-1-53"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="annotated-cell-1-54"><a href="#annotated-cell-1-54"></a><span class="co">## of new unlabeled data</span></span>
<span id="annotated-cell-1-55"><a href="#annotated-cell-1-55"></a></span>
<span id="annotated-cell-1-56"><a href="#annotated-cell-1-56"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-57"><a href="#annotated-cell-1-57"></a><span class="co">## Prediction step</span></span>
<span id="annotated-cell-1-58"><a href="#annotated-cell-1-58"></a><span class="co">## *************************</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2">2</button><span id="annotated-cell-1-59" class="code-annotation-target"><a href="#annotated-cell-1-59"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="annotated-cell-1-60"><a href="#annotated-cell-1-60"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="annotated-cell-1-61"><a href="#annotated-cell-1-61"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-1-62"><a href="#annotated-cell-1-62"></a>    unlabeledData,</span>
<span id="annotated-cell-1-63"><a href="#annotated-cell-1-63"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-1-64"><a href="#annotated-cell-1-64"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2">2</button><span id="annotated-cell-1-65" class="code-annotation-target"><a href="#annotated-cell-1-65"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-1-66"><a href="#annotated-cell-1-66"></a>)</span>
<span id="annotated-cell-1-67"><a href="#annotated-cell-1-67"></a></span>
<span id="annotated-cell-1-68"><a href="#annotated-cell-1-68"></a><span class="co">## Make predictions on the unlabled data using the transform() </span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3">3</button><span id="annotated-cell-1-69" class="code-annotation-target"><a href="#annotated-cell-1-69"></a><span class="co">## method of the trained classification model transform uses only </span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3">3</button><span id="annotated-cell-1-70" class="code-annotation-target"><a href="#annotated-cell-1-70"></a><span class="co">## the content of 'features' to perform the predictions. The model </span></span>
<span id="annotated-cell-1-71"><a href="#annotated-cell-1-71"></a><span class="co">## is associated with the pipeline and hence also the assembler is executed</span></span>
<span id="annotated-cell-1-72"><a href="#annotated-cell-1-72"></a>predictions <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="annotated-cell-1-73"><a href="#annotated-cell-1-73"></a></span>
<span id="annotated-cell-1-74"><a href="#annotated-cell-1-74"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="annotated-cell-1-75"><a href="#annotated-cell-1-75"></a><span class="co">## - attr1</span></span>
<span id="annotated-cell-1-76"><a href="#annotated-cell-1-76"></a><span class="co">## - attr2</span></span>
<span id="annotated-cell-1-77"><a href="#annotated-cell-1-77"></a><span class="co">## - attr3</span></span>
<span id="annotated-cell-1-78"><a href="#annotated-cell-1-78"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="annotated-cell-1-79"><a href="#annotated-cell-1-79"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="annotated-cell-1-80"><a href="#annotated-cell-1-80"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="annotated-cell-1-81"><a href="#annotated-cell-1-81"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the current</span></span>
<span id="annotated-cell-1-82"><a href="#annotated-cell-1-82"></a><span class="co">## record belongs to the i-th class</span></span>
<span id="annotated-cell-1-83"><a href="#annotated-cell-1-83"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="annotated-cell-1-84"><a href="#annotated-cell-1-84"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="annotated-cell-1-85"><a href="#annotated-cell-1-85"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="annotated-cell-1-86"><a href="#annotated-cell-1-86"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>,<span class="st">"prediction"</span>)</span>
<span id="annotated-cell-1-87"><a href="#annotated-cell-1-87"></a></span>
<span id="annotated-cell-1-88"><a href="#annotated-cell-1-88"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="annotated-cell-1-89"><a href="#annotated-cell-1-89"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-lines="47" data-code-annotation="1" data-code-cell="annotated-cell-1"><code>assembler</code>: the sequence of transformers and estimators to apply on the input data</span>
</dd>
</dl>
</div>
</div>
</div>
</section>
<section id="decision-trees-and-structured-data" class="level4">
<h4 class="anchored" data-anchor-id="decision-trees-and-structured-data">Decision trees and structured data</h4>
<p>The following paragraphs show how to</p>
<ul>
<li>Create a classification model based on the decision tree algorithm on structured data: the model is inferred by analyzing the training data, i.e., the example records/data points for which the value of the class label is known;</li>
<li>Apply the model to new unlabeled data: the inferred model is applied to predict the value of the class label of new unlabeled records/data points.</li>
</ul>
<p>The same example structured data already used in the running example related to the logistic regression algorithm are used also in this example related to the decision tree algorithm. The main steps are the same of the previous example, the only difference is the definition and configuration of the used classification algorithm.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> DecisionTreeClassifier</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a></span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a><span class="co">## input and output folders</span></span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a>trainingData <span class="op">=</span> <span class="st">"ex_data/trainingData.csv"</span></span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_data/unlabeledData.csv"</span></span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLR/"</span></span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11"></a></span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13"></a><span class="co">## Training step</span></span>
<span id="annotated-cell-1-14"><a href="#annotated-cell-1-14"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-15"><a href="#annotated-cell-1-15"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="annotated-cell-1-16"><a href="#annotated-cell-1-16"></a><span class="co">## Training data in raw format</span></span>
<span id="annotated-cell-1-17"><a href="#annotated-cell-1-17"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-1-18"><a href="#annotated-cell-1-18"></a>    trainingData,</span>
<span id="annotated-cell-1-19"><a href="#annotated-cell-1-19"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-1-20"><a href="#annotated-cell-1-20"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="annotated-cell-1-21"><a href="#annotated-cell-1-21"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-1-22"><a href="#annotated-cell-1-22"></a>)</span>
<span id="annotated-cell-1-23"><a href="#annotated-cell-1-23"></a></span>
<span id="annotated-cell-1-24"><a href="#annotated-cell-1-24"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="annotated-cell-1-25"><a href="#annotated-cell-1-25"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="annotated-cell-1-26"><a href="#annotated-cell-1-26"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="annotated-cell-1-27"><a href="#annotated-cell-1-27"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="annotated-cell-1-28"><a href="#annotated-cell-1-28"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="annotated-cell-1-29"><a href="#annotated-cell-1-29"></a>)</span>
<span id="annotated-cell-1-30"><a href="#annotated-cell-1-30"></a></span>
<span id="annotated-cell-1-31"><a href="#annotated-cell-1-31"></a><span class="co">## Create a DecisionTreeClassifier object.</span></span>
<span id="annotated-cell-1-32"><a href="#annotated-cell-1-32"></a><span class="co">## DecisionTreeClassifier is an Estimator that is used to</span></span>
<span id="annotated-cell-1-33"><a href="#annotated-cell-1-33"></a><span class="co">## create a classification model based on decision trees.</span></span>
<span id="annotated-cell-1-34"><a href="#annotated-cell-1-34"></a>dt <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="annotated-cell-1-35"><a href="#annotated-cell-1-35"></a></span>
<span id="annotated-cell-1-36"><a href="#annotated-cell-1-36"></a><span class="co">## We can set the values of the parameters of the Decision Tree</span></span>
<span id="annotated-cell-1-37"><a href="#annotated-cell-1-37"></a><span class="co">## For example we can set the measure that is used to decide if a</span></span>
<span id="annotated-cell-1-38"><a href="#annotated-cell-1-38"></a><span class="co">## node must be split. In this case we set gini index</span></span>
<span id="annotated-cell-1-39"><a href="#annotated-cell-1-39"></a>dt.setImpurity(<span class="st">"gini"</span>)</span>
<span id="annotated-cell-1-40"><a href="#annotated-cell-1-40"></a></span>
<span id="annotated-cell-1-41"><a href="#annotated-cell-1-41"></a><span class="co">## Define a pipeline that is used to create the decision tree</span></span>
<span id="annotated-cell-1-42"><a href="#annotated-cell-1-42"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="annotated-cell-1-43"><a href="#annotated-cell-1-43"></a><span class="co">## the preprocessing step</span></span>
<span id="annotated-cell-1-44"><a href="#annotated-cell-1-44"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, dt])</span>
<span id="annotated-cell-1-45"><a href="#annotated-cell-1-45"></a></span>
<span id="annotated-cell-1-46"><a href="#annotated-cell-1-46"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="annotated-cell-1-47"><a href="#annotated-cell-1-47"></a><span class="co">## classification model</span></span>
<span id="annotated-cell-1-48"><a href="#annotated-cell-1-48"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="annotated-cell-1-49"><a href="#annotated-cell-1-49"></a></span>
<span id="annotated-cell-1-50"><a href="#annotated-cell-1-50"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="annotated-cell-1-51"><a href="#annotated-cell-1-51"></a><span class="co">## of new unlabeled data</span></span>
<span id="annotated-cell-1-52"><a href="#annotated-cell-1-52"></a></span>
<span id="annotated-cell-1-53"><a href="#annotated-cell-1-53"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-54"><a href="#annotated-cell-1-54"></a><span class="co">## Prediction step</span></span>
<span id="annotated-cell-1-55"><a href="#annotated-cell-1-55"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-56"><a href="#annotated-cell-1-56"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="annotated-cell-1-57"><a href="#annotated-cell-1-57"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="annotated-cell-1-58"><a href="#annotated-cell-1-58"></a>unlabeledData <span class="op">=</span> spark.read.load(unlabeledData,<span class="op">\</span></span>
<span id="annotated-cell-1-59"><a href="#annotated-cell-1-59"></a><span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>, header<span class="op">=</span><span class="va">True</span>, inferSchema<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-1-60"><a href="#annotated-cell-1-60"></a></span>
<span id="annotated-cell-1-61"><a href="#annotated-cell-1-61"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="annotated-cell-1-62"><a href="#annotated-cell-1-62"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="annotated-cell-1-63"><a href="#annotated-cell-1-63"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="annotated-cell-1-64"><a href="#annotated-cell-1-64"></a><span class="co">## also the assembler is executed</span></span>
<span id="annotated-cell-1-65"><a href="#annotated-cell-1-65"></a>predictions <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="annotated-cell-1-66"><a href="#annotated-cell-1-66"></a></span>
<span id="annotated-cell-1-67"><a href="#annotated-cell-1-67"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="annotated-cell-1-68"><a href="#annotated-cell-1-68"></a><span class="co">## - attr1</span></span>
<span id="annotated-cell-1-69"><a href="#annotated-cell-1-69"></a><span class="co">## - attr2</span></span>
<span id="annotated-cell-1-70"><a href="#annotated-cell-1-70"></a><span class="co">## - attr3</span></span>
<span id="annotated-cell-1-71"><a href="#annotated-cell-1-71"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="annotated-cell-1-72"><a href="#annotated-cell-1-72"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="annotated-cell-1-73"><a href="#annotated-cell-1-73"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="annotated-cell-1-74"><a href="#annotated-cell-1-74"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the current</span></span>
<span id="annotated-cell-1-75"><a href="#annotated-cell-1-75"></a><span class="co">## record belongs to the i-th class</span></span>
<span id="annotated-cell-1-76"><a href="#annotated-cell-1-76"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="annotated-cell-1-77"><a href="#annotated-cell-1-77"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="annotated-cell-1-78"><a href="#annotated-cell-1-78"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="annotated-cell-1-79"><a href="#annotated-cell-1-79"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>,<span class="st">"prediction"</span>)</span>
<span id="annotated-cell-1-80"><a href="#annotated-cell-1-80"></a></span>
<span id="annotated-cell-1-81"><a href="#annotated-cell-1-81"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="annotated-cell-1-82"><a href="#annotated-cell-1-82"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl>
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-lines="44" data-code-annotation="1" data-code-cell="annotated-cell-1"><code>assembler</code>: the sequence of transformers and estimators to apply on the input data. A decision tree algorithm is used in this case.</span>
</dd>
</dl>
</div>
</div>
</div>
</section>
</section>
<section id="categorical-class-labels" class="level3">
<h3 class="anchored" data-anchor-id="categorical-class-labels">Categorical class labels</h3>
<p>Usually the class label is a categorical value (i.e., a string). However, as reported before, Spark MLlib works only with numerical values and hence categorical class label values must be mapped to integer (and then double) values: processing and postprocessing steps are used to manage this transformation.</p>
<p>Consider the following input training data</p>
<table class="table">
<thead>
<tr class="header">
<th>categoricalLabel</th>
<th>Attr1</th>
<th>Attr2</th>
<th>Attr3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td><span class="math inline">\(0.0\)</span></td>
<td><span class="math inline">\(1.1\)</span></td>
<td><span class="math inline">\(0.1\)</span></td>
</tr>
<tr class="even">
<td>Negative</td>
<td><span class="math inline">\(2.0\)</span></td>
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\(-1.0\)</span></td>
</tr>
<tr class="odd">
<td>Negative</td>
<td><span class="math inline">\(2.0\)</span></td>
<td><span class="math inline">\(1.3\)</span></td>
<td><span class="math inline">\(1.0\)</span></td>
</tr>
</tbody>
</table>
<p>A modified input DataFrame must be generated as input for the MLlib classification algorithms</p>
<table class="table">
<thead>
<tr class="header">
<th>label</th>
<th>features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\([0.0,1.1,0.1]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(1.0\)</span></td>
<td><span class="math inline">\([2.0,1.0,-1.0]\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(0.0\)</span></td>
<td><span class="math inline">\([2.0,1.3,1.0]\)</span></td>
</tr>
</tbody>
</table>
<p>Notice that the categorical values of “categoricalLabel” (the class label column) must mapped to integer data values (finally casted to doubles).</p>
<section id="stringindexer-and-indextostring" class="level4">
<h4 class="anchored" data-anchor-id="stringindexer-and-indextostring"><code>StringIndexer</code> and <code>IndexToString</code></h4>
<p>The Estimator <code>StringIndexer</code> and the Transformer <code>IndexToString</code> support the transformation of categorical class label into numerical one and vice versa:</p>
<ul>
<li><code>StringIndexer</code> maps each categorical value of the class label to an integer (then casted to a double);</li>
<li><code>IndexToString</code> is used to perform the opposite operation.</li>
</ul>
<p>All in all, these are the main steps</p>
<ol type="1">
<li>Use <code>StringIndexer</code> to extend the input DataFrame with a new column, called “label”, containing the numerical representation of the class label column;</li>
<li>Create a column, called “features”, of type vector containing the predictive features;</li>
<li>Infer a classification model by using a classification algorithm (e.g., Decision Tree, Logistic regression);</li>
<li>Apply the model on a set of unlabeled data to predict their numerical class label;</li>
<li>Use <code>IndexToString</code> to convert the predicted numerical class label values to the original categorical values.</li>
</ol>
<p>Notice that the model is built by considering only the values of features and label. All the other columns are not considered by the classification algorithm during the generation of the prediction model.</p>
<section id="training-data-1" class="level5">
<h5 class="anchored" data-anchor-id="training-data-1">Training data</h5>
<p>Given the following input training file</p>
<pre><code>categoricalLabel,attr1,attr2,attr3
Positive,0.0,1.1,0.1
Negative,2.0,1.0,-1.0
Negative,2.0,1.3,1.0</code></pre>
<p>The initial training DataFrame will be</p>
<table class="table">
<thead>
<tr class="header">
<th>categoricalLabel</th>
<th>features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td><span class="math inline">\([0.0,1.1,0.1]\)</span></td>
</tr>
<tr class="even">
<td>Negative</td>
<td><span class="math inline">\([2.0,1.0,-1.0]\)</span></td>
</tr>
<tr class="odd">
<td>Negative</td>
<td><span class="math inline">\([2.0,1.3,1.0]\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>The type of “categoricalLabel” is String</li>
<li>The type of “features” is Vector</li>
</ul>
<p>After applying <code>StringIndexer</code>, the training DataFrame will be</p>
<table class="table">
<thead>
<tr class="header">
<th>categoricalLabel</th>
<th>features</th>
<th>label!</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td><span class="math inline">\([0.0,1.1,0.1]\)</span></td>
<td><span class="math inline">\(1.0\)</span></td>
</tr>
<tr class="even">
<td>Negative</td>
<td><span class="math inline">\([2.0,1.0,-1.0]\)</span></td>
<td><span class="math inline">\(0.0\)</span></td>
</tr>
<tr class="odd">
<td>Negative</td>
<td><span class="math inline">\([2.0,1.3,1.0]\)</span></td>
<td><span class="math inline">\(0.0\)</span></td>
</tr>
</tbody>
</table>
<p>“label” contains the mapping generated by <code>StringIndexer</code>:</p>
<ul>
<li>“Positive”: <span class="math inline">\(1.0\)</span></li>
<li>“Negative”: <span class="math inline">\(0.0\)</span></li>
</ul>
</section>
<section id="unalabeled-data" class="level5">
<h5 class="anchored" data-anchor-id="unalabeled-data">Unalabeled data</h5>
<p>Given the input unlabeled data file</p>
<pre><code>categoricalLabel,attr1,attr2,attr3
,-1.0,1.5,1.3
,3.0,2.0,-0.1
,0.0,2.2,-1.5</code></pre>
<p>The initial unlabeled DataFrame will be</p>
<table class="table">
<thead>
<tr class="header">
<th>categoricalLabel</th>
<th>features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>null</td>
<td><span class="math inline">\([-1.0,1.5,1.3]\)</span></td>
</tr>
<tr class="even">
<td>null</td>
<td><span class="math inline">\([3.0,2.0,-0.1]\)</span></td>
</tr>
<tr class="odd">
<td>null</td>
<td><span class="math inline">\([0.0,2.2,-1.5]\)</span></td>
</tr>
</tbody>
</table>
<p>After performing the prediction, and applying <code>IndexToString</code>, the output DataFrame will be</p>
<table class="table">
<thead>
<tr class="header">
<th>categoricalLabel</th>
<th>features</th>
<th>label</th>
<th>prediction</th>
<th>predictedLabel</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>…</td>
<td><span class="math inline">\([-1.0,1.5,1.3]\)</span></td>
<td>…</td>
<td><span class="math inline">\(1.0\)</span></td>
<td>Positive</td>
<td></td>
</tr>
<tr class="even">
<td>…</td>
<td><span class="math inline">\([3.0,2.0,-0.1]\)</span></td>
<td>…</td>
<td><span class="math inline">\(0.0\)</span></td>
<td>Negative</td>
<td></td>
</tr>
<tr class="odd">
<td>…</td>
<td><span class="math inline">\([0.0,2.2,-1.5]\)</span></td>
<td>…</td>
<td><span class="math inline">\(1.0\)</span></td>
<td>Negative</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>“prediction” contains the predicted label, expressed as a number</li>
<li>“predictedLabel” contains the predicted label, expressed as a category (original name)</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this example, the input training data is stored in a text file that contains one record/data point per line, and the records/data points are structured data with a fixed number of attributes (four)</p>
<ul>
<li>One attribute is the class label (“categoricalLabel”): this is a categorical attribute that can assume two values, “Positive” or “Negative”;</li>
<li>The other three attributes (“attr1”, “attr2”, “attr3”) are the predictive attributes that are used to predict the value of the class label.</li>
</ul>
<p>The input file has the header line.</p>
<p>The file containing the unlabeled data has the same format of the training data file, however the first column is empty because the class label is unknown.</p>
<p>The goal is to predict the class label value of each unlabeled data by applying the classification model that has been inferred on the training data.</p>
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> StringIndexer</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> IndexToString</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> DecisionTreeClassifier</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a></span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a><span class="co">## input and output folders</span></span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a>trainingData <span class="op">=</span> <span class="st">"ex_dataCategorical/trainingData.csv"</span></span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_dataCategorical/unlabeledData.csv"</span></span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a>outputPath <span class="op">=</span> <span class="st">"predictionsDTCategoricalPipeline/"</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13"></a></span>
<span id="annotated-cell-1-14"><a href="#annotated-cell-1-14"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-15"><a href="#annotated-cell-1-15"></a><span class="co">## Training step</span></span>
<span id="annotated-cell-1-16"><a href="#annotated-cell-1-16"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-17"><a href="#annotated-cell-1-17"></a></span>
<span id="annotated-cell-1-18"><a href="#annotated-cell-1-18"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="annotated-cell-1-19"><a href="#annotated-cell-1-19"></a><span class="co">## Training data in raw format</span></span>
<span id="annotated-cell-1-20"><a href="#annotated-cell-1-20"></a>trainingData <span class="op">=</span> spark.read.load(trainingData,<span class="op">\</span></span>
<span id="annotated-cell-1-21"><a href="#annotated-cell-1-21"></a><span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>, header<span class="op">=</span><span class="va">True</span>, inferSchema<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-1-22"><a href="#annotated-cell-1-22"></a></span>
<span id="annotated-cell-1-23"><a href="#annotated-cell-1-23"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="annotated-cell-1-24"><a href="#annotated-cell-1-24"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="annotated-cell-1-25"><a href="#annotated-cell-1-25"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="annotated-cell-1-26"><a href="#annotated-cell-1-26"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="annotated-cell-1-27"><a href="#annotated-cell-1-27"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="annotated-cell-1-28"><a href="#annotated-cell-1-28"></a>)</span>
<span id="annotated-cell-1-29"><a href="#annotated-cell-1-29"></a></span>
<span id="annotated-cell-1-30"><a href="#annotated-cell-1-30"></a><span class="co">## The StringIndexer Estimator is used to map each class label</span></span>
<span id="annotated-cell-1-31"><a href="#annotated-cell-1-31"></a><span class="co">## value to an integer value (casted to a double).</span></span>
<span id="annotated-cell-1-32"><a href="#annotated-cell-1-32"></a><span class="co">## A new attribute called label is generated by applying</span></span>
<span id="annotated-cell-1-33"><a href="#annotated-cell-1-33"></a><span class="co">## transforming the content of the categoricalLabel attribute.</span></span>
<span id="annotated-cell-1-34"><a href="#annotated-cell-1-34"></a>labelIndexer <span class="op">=</span> StringIndexer(</span>
<span id="annotated-cell-1-35"><a href="#annotated-cell-1-35"></a>    inputCol<span class="op">=</span><span class="st">"categoricalLabel"</span></span>
<span id="annotated-cell-1-36"><a href="#annotated-cell-1-36"></a>    outputCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="annotated-cell-1-37"><a href="#annotated-cell-1-37"></a>    handleInvalid<span class="op">=</span><span class="st">"keep"</span></span>
<span id="annotated-cell-1-38"><a href="#annotated-cell-1-38"></a>).fit(trainingData) </span>
<span id="annotated-cell-1-39"><a href="#annotated-cell-1-39"></a></span>
<span id="annotated-cell-1-40"><a href="#annotated-cell-1-40"></a><span class="co">## Create a DecisionTreeClassifier object.</span></span>
<span id="annotated-cell-1-41"><a href="#annotated-cell-1-41"></a><span class="co">## DecisionTreeClassifier is an Estimator that is used to</span></span>
<span id="annotated-cell-1-42"><a href="#annotated-cell-1-42"></a><span class="co">## create a classification model based on decision trees.</span></span>
<span id="annotated-cell-1-43"><a href="#annotated-cell-1-43"></a>dt <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="annotated-cell-1-44"><a href="#annotated-cell-1-44"></a></span>
<span id="annotated-cell-1-45"><a href="#annotated-cell-1-45"></a><span class="co">## Set the values of the parameters of the Decision Tree</span></span>
<span id="annotated-cell-1-46"><a href="#annotated-cell-1-46"></a><span class="co">## For example set the measure that is used to decide if a</span></span>
<span id="annotated-cell-1-47"><a href="#annotated-cell-1-47"></a><span class="co">## node must be split.</span></span>
<span id="annotated-cell-1-48"><a href="#annotated-cell-1-48"></a><span class="co">## In this case we set gini index</span></span>
<span id="annotated-cell-1-49"><a href="#annotated-cell-1-49"></a>dt.setImpurity(<span class="st">"gini"</span>)</span>
<span id="annotated-cell-1-50"><a href="#annotated-cell-1-50"></a></span>
<span id="annotated-cell-1-51"><a href="#annotated-cell-1-51"></a><span class="co">## At the end of the pipeline we must convert indexed labels back</span></span>
<span id="annotated-cell-1-52"><a href="#annotated-cell-1-52"></a><span class="co">## to original labels (from numerical to string).</span></span>
<span id="annotated-cell-1-53"><a href="#annotated-cell-1-53"></a><span class="co">## The content of the prediction attribute is the index of the predicted class</span></span>
<span id="annotated-cell-1-54"><a href="#annotated-cell-1-54"></a><span class="co">## The original name of the predicted class is stored in the predictedLabel</span></span>
<span id="annotated-cell-1-55"><a href="#annotated-cell-1-55"></a><span class="co">## attribute.</span></span>
<span id="annotated-cell-1-56"><a href="#annotated-cell-1-56"></a><span class="co">## IndexToString creates a new column (called predictedLabel in</span></span>
<span id="annotated-cell-1-57"><a href="#annotated-cell-1-57"></a><span class="co">## this example) that is based on the content of the prediction column.</span></span>
<span id="annotated-cell-1-58"><a href="#annotated-cell-1-58"></a><span class="co">## prediction is a double while predictedLabel is a string</span></span>
<span id="annotated-cell-1-59"><a href="#annotated-cell-1-59"></a>labelConverter <span class="op">=</span> IndexToString(</span>
<span id="annotated-cell-1-60"><a href="#annotated-cell-1-60"></a>    inputCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="annotated-cell-1-61"><a href="#annotated-cell-1-61"></a>    outputCol<span class="op">=</span><span class="st">"predictedLabel"</span>,</span>
<span id="annotated-cell-1-62"><a href="#annotated-cell-1-62"></a>    labels<span class="op">=</span>labelIndexer.labels</span>
<span id="annotated-cell-1-63"><a href="#annotated-cell-1-63"></a>)</span>
<span id="annotated-cell-1-64"><a href="#annotated-cell-1-64"></a></span>
<span id="annotated-cell-1-65"><a href="#annotated-cell-1-65"></a><span class="co">## Define a pipeline that is used to create the decision tree</span></span>
<span id="annotated-cell-1-66"><a href="#annotated-cell-1-66"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="annotated-cell-1-67"><a href="#annotated-cell-1-67"></a><span class="co">## the preprocessing and postprocessing steps</span></span>
<span id="annotated-cell-1-68"><a href="#annotated-cell-1-68"></a>pipeline <span class="op">=</span> Pipeline() \ </span>
<span id="annotated-cell-1-69"><a href="#annotated-cell-1-69"></a>    .setStages([assembler, labelIndexer, dt, labelConverter])</span>
<span id="annotated-cell-1-70"><a href="#annotated-cell-1-70"></a></span>
<span id="annotated-cell-1-71"><a href="#annotated-cell-1-71"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="annotated-cell-1-72"><a href="#annotated-cell-1-72"></a><span class="co">## classification model</span></span>
<span id="annotated-cell-1-73"><a href="#annotated-cell-1-73"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="annotated-cell-1-74"><a href="#annotated-cell-1-74"></a></span>
<span id="annotated-cell-1-75"><a href="#annotated-cell-1-75"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="annotated-cell-1-76"><a href="#annotated-cell-1-76"></a><span class="co">## of new unlabeled data</span></span>
<span id="annotated-cell-1-77"><a href="#annotated-cell-1-77"></a></span>
<span id="annotated-cell-1-78"><a href="#annotated-cell-1-78"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-79"><a href="#annotated-cell-1-79"></a><span class="co">## Prediction step</span></span>
<span id="annotated-cell-1-80"><a href="#annotated-cell-1-80"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-81"><a href="#annotated-cell-1-81"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="annotated-cell-1-82"><a href="#annotated-cell-1-82"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="annotated-cell-1-83"><a href="#annotated-cell-1-83"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-1-84"><a href="#annotated-cell-1-84"></a>    unlabeledData,</span>
<span id="annotated-cell-1-85"><a href="#annotated-cell-1-85"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-1-86"><a href="#annotated-cell-1-86"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="annotated-cell-1-87"><a href="#annotated-cell-1-87"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-1-88"><a href="#annotated-cell-1-88"></a>)</span>
<span id="annotated-cell-1-89"><a href="#annotated-cell-1-89"></a></span>
<span id="annotated-cell-1-90"><a href="#annotated-cell-1-90"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="annotated-cell-1-91"><a href="#annotated-cell-1-91"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="annotated-cell-1-92"><a href="#annotated-cell-1-92"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="annotated-cell-1-93"><a href="#annotated-cell-1-93"></a><span class="co">## also the assembler is executed</span></span>
<span id="annotated-cell-1-94"><a href="#annotated-cell-1-94"></a>predictions <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="annotated-cell-1-95"><a href="#annotated-cell-1-95"></a></span>
<span id="annotated-cell-1-96"><a href="#annotated-cell-1-96"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="annotated-cell-1-97"><a href="#annotated-cell-1-97"></a><span class="co">## - attr1: double (nullable = true)</span></span>
<span id="annotated-cell-1-98"><a href="#annotated-cell-1-98"></a><span class="co">## - attr2: double (nullable = true)</span></span>
<span id="annotated-cell-1-99"><a href="#annotated-cell-1-99"></a><span class="co">## - attr3: double (nullable = true)</span></span>
<span id="annotated-cell-1-100"><a href="#annotated-cell-1-100"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="annotated-cell-1-101"><a href="#annotated-cell-1-101"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="annotated-cell-1-102"><a href="#annotated-cell-1-102"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="annotated-cell-1-103"><a href="#annotated-cell-1-103"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the</span></span>
<span id="annotated-cell-1-104"><a href="#annotated-cell-1-104"></a><span class="co">##   current record belongs to the i-th class</span></span>
<span id="annotated-cell-1-105"><a href="#annotated-cell-1-105"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="annotated-cell-1-106"><a href="#annotated-cell-1-106"></a><span class="co">## - predictedLabel: string (nullable = true)</span></span>
<span id="annotated-cell-1-107"><a href="#annotated-cell-1-107"></a></span>
<span id="annotated-cell-1-108"><a href="#annotated-cell-1-108"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="annotated-cell-1-109"><a href="#annotated-cell-1-109"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="annotated-cell-1-110"><a href="#annotated-cell-1-110"></a>predictions <span class="op">=</span> predictionsDF <span class="op">\</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="4">4</button><span id="annotated-cell-1-111" class="code-annotation-target"><a href="#annotated-cell-1-111"></a>    .select(<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>, <span class="st">"predictedLabel"</span>)</span>
<span id="annotated-cell-1-112"><a href="#annotated-cell-1-112"></a></span>
<span id="annotated-cell-1-113"><a href="#annotated-cell-1-113"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="annotated-cell-1-114"><a href="#annotated-cell-1-114"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl>
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-lines="34" data-code-annotation="1" data-code-cell="annotated-cell-1">This <code>StringIndexer</code> estimator is used to infer a transformer that maps the categorical values of column “categoricalLabel” to a set of integer values stored in the new column called “label”. The list of valid label values are extracted from the training data.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-lines="59" data-code-annotation="2" data-code-cell="annotated-cell-1">This <code>IndexToString</code> component is used to remap the numerical predictions available in the “prediction” column to the original categorical values that are stored in the new column called “predictedLabel”. The mapping of integer to original string value is the one of “labelIndexer”.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-lines="69" data-code-annotation="3" data-code-cell="annotated-cell-1">This <code>Pipeline</code> is composed of four steps.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="4">4</dt>
<dd>
<span data-code-lines="111" data-code-annotation="4" data-code-cell="annotated-cell-1">The “predictedLabel” field is the column containing the predicted categorical class label for the unlabeled data.</span>
</dd>
</dl>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="textual-data-management-and-classification" class="level3">
<h3 class="anchored" data-anchor-id="textual-data-management-and-classification">Textual data management and classification</h3>
<p>The following paragraphs show how to</p>
<ul>
<li>Create a classification model based on the logistic regression algorithm for textual documents: a set of specific preprocessing estimators and transformers are used to preprocess textual data.</li>
<li>Apply the model to new textual documents</li>
</ul>
<p>The input training dataset represents a textual document collection, where each line contains one document and its class</p>
<ul>
<li>The class label</li>
<li>A list of words (the text of the document)</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Given the following example training file</p>
<pre><code>Label,Text
1,The Spark system is based on scala
1,Spark is a new distributed system
0,Turin is a beautiful city
0,Turin is in the north of Italy</code></pre>
<p>It contains four textual documents, and each line contains two attributes, that are the class label (first attribute) and the text of the document (second attribute).</p>
<p>The input data before preprocessing, represented as a DataFrame, is</p>
<table class="table">
<thead>
<tr class="header">
<th>Label</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>The Spark system is based on scala</td>
</tr>
<tr class="even">
<td>1</td>
<td>Spark is a new distributed system</td>
</tr>
<tr class="odd">
<td>0</td>
<td>Turin is a beautiful city</td>
</tr>
<tr class="even">
<td>0</td>
<td>Turin is in the north of Italy</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>A set of preprocessing steps must be applied on the textual attribute before generating a classification model.</p>
<ol type="1">
<li>Since Spark ML algorithms work only on “Tables” and double values, the textual part of the input data must be translated in a set of attributes to represent the data as a table: usually a table with an attribute for each word is generated.</li>
<li>Many words are useless (e.g., conjunctions): stopwords are usually removed. In general,
<ul>
<li>the words appearing in almost all documents are not characterizing the data, and so they are not very important for the classification problem;</li>
<li>the words appearing in few documents allow to distinguish the content of those documents (and hence the class label) with respect to the others, and so they are very important for the classification problem.</li>
</ul></li>
<li>Traditionally a weight, based on the TF-IDF measure, is used to assign a difference importance to the words based on their frequency in the collection.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Input data after the preprocessing transformations (tokenization, stopword removal, TF-IDF computation)</p>
<table class="table">
<thead>
<tr class="header">
<th>Label</th>
<th>Spark</th>
<th>system</th>
<th>scala</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(0.5\)</span></td>
<td><span class="math inline">\(0.3\)</span></td>
<td><span class="math inline">\(0.75\)</span></td>
<td>…</td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(0.5\)</span></td>
<td><span class="math inline">\(0.3\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td>…</td>
</tr>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td>…</td>
</tr>
<tr class="even">
<td>0</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The DataFrame associated with the input data after the preprocessing transformations must contain, as usual, the columns</p>
<ul>
<li>label: class label value</li>
<li>features: the preprocessed version of the input text</li>
</ul>
<p>There are also some other intermediate columns, related to applied transformations, but they are not considered by the classification algorithm.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The DataFrame associated with the input data after the preprocessing transformations</p>
<table class="table">
<thead>
<tr class="header">
<th>label</th>
<th>features</th>
<th>text</th>
<th>…</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\([0.5,0.3,0.75,...]\)</span></td>
<td>The Spark system is based on scala</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\([0.5,0.3,0,...]\)</span></td>
<td>Spark is a new distributed system</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\([0,0,0,...]\)</span></td>
<td>Turin is a beautiful city</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>0</td>
<td><span class="math inline">\([0,0,0,...]\)</span></td>
<td>Turin is in the north of Italy</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>Only “label” and “features” are considered by the classification algorithm.</p>
</div>
</div>
</div>
<p>In the following solution we will use a set of new Transformers to prepare input data</p>
<ul>
<li><code>Tokenizer</code>: to split the input text in words;</li>
<li><code>StopWordsRemover</code>: to remove stopwords;</li>
<li><code>HashingTF</code>: to compute the (approximate) term frequency of each input term;</li>
<li><code>IDF</code>: to compute the inverse document frequency of each input word.</li>
</ul>
<p>The input data (training and unlabeled data) are stored in input csv files. Each line contains two attributes:</p>
<ul>
<li>The class label (label)</li>
<li>The text of the document (text)</li>
</ul>
<p>We infer a linear regression model on the training data and apply the model on the unlabeled data.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> Tokenizer</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> StopWordsRemover</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> HashingTF</span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> IDF</span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co">## input and output folders</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>trainingData <span class="op">=</span> <span class="st">"ex_dataText/trainingData.csv"</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_dataText/unlabeledData.csv"</span></span>
<span id="cb7-14"><a href="#cb7-14"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLRPipelineText/"</span></span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="co">## *************************</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="co">## Training step</span></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="co">## *************************</span></span>
<span id="cb7-19"><a href="#cb7-19"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb7-20"><a href="#cb7-20"></a><span class="co">## Training data in raw format</span></span>
<span id="cb7-21"><a href="#cb7-21"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="cb7-22"><a href="#cb7-22"></a>    trainingData,</span>
<span id="cb7-23"><a href="#cb7-23"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb7-24"><a href="#cb7-24"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-25"><a href="#cb7-25"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-26"><a href="#cb7-26"></a>)</span>
<span id="cb7-27"><a href="#cb7-27"></a></span>
<span id="cb7-28"><a href="#cb7-28"></a><span class="co">## Configure an ML pipeline, which consists of five stages:</span></span>
<span id="cb7-29"><a href="#cb7-29"></a><span class="co">## tokenizer -&gt; split sentences in set of words</span></span>
<span id="cb7-30"><a href="#cb7-30"></a><span class="co">## remover -&gt; remove stopwords</span></span>
<span id="cb7-31"><a href="#cb7-31"></a><span class="co">## hashingTF -&gt; map set of words to a fixed-length feature vectors (each</span></span>
<span id="cb7-32"><a href="#cb7-32"></a><span class="co">## word becomes a feature and the value of the feature is the frequency of</span></span>
<span id="cb7-33"><a href="#cb7-33"></a><span class="co">## the word in the sentence)</span></span>
<span id="cb7-34"><a href="#cb7-34"></a><span class="co">## idf -&gt; compute the idf component of the TF-IDF measure</span></span>
<span id="cb7-35"><a href="#cb7-35"></a><span class="co">## lr -&gt; logistic regression classification algorithm</span></span>
<span id="cb7-36"><a href="#cb7-36"></a><span class="co">## The Tokenizer splits each sentence in a set of words.</span></span>
<span id="cb7-37"><a href="#cb7-37"></a><span class="co">## It analyzes the content of column "text" and adds the</span></span>
<span id="cb7-38"><a href="#cb7-38"></a><span class="co">## new column "words" in the returned DataFrame</span></span>
<span id="cb7-39"><a href="#cb7-39"></a>tokenizer <span class="op">=</span> Tokenizer() <span class="op">\</span></span>
<span id="cb7-40"><a href="#cb7-40"></a>    .setInputCol(<span class="st">"text"</span>) <span class="op">\</span></span>
<span id="cb7-41"><a href="#cb7-41"></a>    .setOutputCol(<span class="st">"words"</span>)</span>
<span id="cb7-42"><a href="#cb7-42"></a></span>
<span id="cb7-43"><a href="#cb7-43"></a><span class="co">## Remove stopwords.</span></span>
<span id="cb7-44"><a href="#cb7-44"></a><span class="co">## The StopWordsRemover component returns a new DataFrame with</span></span>
<span id="cb7-45"><a href="#cb7-45"></a><span class="co">## a new column called "filteredWords". "filteredWords" is generated</span></span>
<span id="cb7-46"><a href="#cb7-46"></a><span class="co">## by removing the stopwords from the content of column "words"</span></span>
<span id="cb7-47"><a href="#cb7-47"></a>remover <span class="op">=</span> StopWordsRemover() <span class="op">\</span></span>
<span id="cb7-48"><a href="#cb7-48"></a>    .setInputCol(<span class="st">"words"</span>) <span class="op">\</span></span>
<span id="cb7-49"><a href="#cb7-49"></a>    .setOutputCol(<span class="st">"filteredWords"</span>)</span>
<span id="cb7-50"><a href="#cb7-50"></a></span>
<span id="cb7-51"><a href="#cb7-51"></a><span class="co">## Map words to a features</span></span>
<span id="cb7-52"><a href="#cb7-52"></a><span class="co">## Each word in filteredWords must become a feature in a Vector object</span></span>
<span id="cb7-53"><a href="#cb7-53"></a><span class="co">## The HashingTF Transformer can be used to perform this operation.</span></span>
<span id="cb7-54"><a href="#cb7-54"></a><span class="co">## This operations is based on a hash function and can potentially</span></span>
<span id="cb7-55"><a href="#cb7-55"></a><span class="co">## map two different words to the same "feature". The number of conflicts</span></span>
<span id="cb7-56"><a href="#cb7-56"></a><span class="co">## in influenced by the value of the numFeatures parameter.</span></span>
<span id="cb7-57"><a href="#cb7-57"></a><span class="co">## The "feature" version of the words is stored in Column "rawFeatures".</span></span>
<span id="cb7-58"><a href="#cb7-58"></a><span class="co">## Each feature, for a document, contains the number of occurrences</span></span>
<span id="cb7-59"><a href="#cb7-59"></a><span class="co">## of that feature in the document (TF component of the TF-IDF measure)</span></span>
<span id="cb7-60"><a href="#cb7-60"></a>hashingTF <span class="op">=</span> HashingTF() <span class="op">\</span></span>
<span id="cb7-61"><a href="#cb7-61"></a>    .setNumFeatures(<span class="dv">1000</span>) <span class="op">\</span></span>
<span id="cb7-62"><a href="#cb7-62"></a>    .setInputCol(<span class="st">"filteredWords"</span>) <span class="op">\</span></span>
<span id="cb7-63"><a href="#cb7-63"></a>    .setOutputCol(<span class="st">"rawFeatures"</span>)</span>
<span id="cb7-64"><a href="#cb7-64"></a></span>
<span id="cb7-65"><a href="#cb7-65"></a><span class="co">## Apply the IDF transformation/computation.</span></span>
<span id="cb7-66"><a href="#cb7-66"></a><span class="co">## Update the weight associated with each feature by considering also the</span></span>
<span id="cb7-67"><a href="#cb7-67"></a><span class="co">## inverse document frequency component. The returned new column</span></span>
<span id="cb7-68"><a href="#cb7-68"></a><span class="co">## is called "features", that is the standard name for the column that</span></span>
<span id="cb7-69"><a href="#cb7-69"></a><span class="co">## contains the predictive features used to create a classification model</span></span>
<span id="cb7-70"><a href="#cb7-70"></a>idf <span class="op">=</span> IDF() <span class="op">\</span></span>
<span id="cb7-71"><a href="#cb7-71"></a>    .setInputCol(<span class="st">"rawFeatures"</span>) <span class="op">\</span></span>
<span id="cb7-72"><a href="#cb7-72"></a>    .setOutputCol(<span class="st">"features"</span>)</span>
<span id="cb7-73"><a href="#cb7-73"></a></span>
<span id="cb7-74"><a href="#cb7-74"></a><span class="co">## Create a classification model based on the logistic regression algorithm</span></span>
<span id="cb7-75"><a href="#cb7-75"></a><span class="co">## We can set the values of the parameters of the</span></span>
<span id="cb7-76"><a href="#cb7-76"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="cb7-77"><a href="#cb7-77"></a>lr <span class="op">=</span> LogisticRegression() <span class="op">\</span></span>
<span id="cb7-78"><a href="#cb7-78"></a>    .setMaxIter(<span class="dv">10</span>) <span class="op">\</span></span>
<span id="cb7-79"><a href="#cb7-79"></a>    .setRegParam(<span class="fl">0.01</span>)</span>
<span id="cb7-80"><a href="#cb7-80"></a></span>
<span id="cb7-81"><a href="#cb7-81"></a><span class="co">## Define the pipeline that is used to create the logistic regression</span></span>
<span id="cb7-82"><a href="#cb7-82"></a><span class="co">## model on the training data.</span></span>
<span id="cb7-83"><a href="#cb7-83"></a><span class="co">## In this case the pipeline is composed of five steps</span></span>
<span id="cb7-84"><a href="#cb7-84"></a><span class="co">## - text tokenizer</span></span>
<span id="cb7-85"><a href="#cb7-85"></a><span class="co">## - stopword removal</span></span>
<span id="cb7-86"><a href="#cb7-86"></a><span class="co">## - TF-IDF computation (performed in two steps)</span></span>
<span id="cb7-87"><a href="#cb7-87"></a><span class="co">## - Logistic regression model generation</span></span>
<span id="cb7-88"><a href="#cb7-88"></a>pipeline <span class="op">=</span> Pipeline()<span class="op">\</span></span>
<span id="cb7-89"><a href="#cb7-89"></a>    .setStages([tokenizer, remover, hashingTF, idf, lr])</span>
<span id="cb7-90"><a href="#cb7-90"></a></span>
<span id="cb7-91"><a href="#cb7-91"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="cb7-92"><a href="#cb7-92"></a><span class="co">## classification model</span></span>
<span id="cb7-93"><a href="#cb7-93"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="cb7-94"><a href="#cb7-94"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="cb7-95"><a href="#cb7-95"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb7-96"><a href="#cb7-96"></a></span>
<span id="cb7-97"><a href="#cb7-97"></a><span class="co">## *************************</span></span>
<span id="cb7-98"><a href="#cb7-98"></a><span class="co">## Prediction step</span></span>
<span id="cb7-99"><a href="#cb7-99"></a><span class="co">## *************************</span></span>
<span id="cb7-100"><a href="#cb7-100"></a><span class="co">## Read unlabeled data</span></span>
<span id="cb7-101"><a href="#cb7-101"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb7-102"><a href="#cb7-102"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb7-103"><a href="#cb7-103"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb7-104"><a href="#cb7-104"></a>    unlabeledData,</span>
<span id="cb7-105"><a href="#cb7-105"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb7-106"><a href="#cb7-106"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-107"><a href="#cb7-107"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-108"><a href="#cb7-108"></a>)</span>
<span id="cb7-109"><a href="#cb7-109"></a></span>
<span id="cb7-110"><a href="#cb7-110"></a><span class="co">## Make predictions on unlabeled documents by using the</span></span>
<span id="cb7-111"><a href="#cb7-111"></a><span class="co">## Transformer.transform() method.</span></span>
<span id="cb7-112"><a href="#cb7-112"></a><span class="co">## The transform will only use the 'features' columns</span></span>
<span id="cb7-113"><a href="#cb7-113"></a>predictionsDF <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="cb7-114"><a href="#cb7-114"></a></span>
<span id="cb7-115"><a href="#cb7-115"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb7-116"><a href="#cb7-116"></a><span class="co">## |-- label: string (nullable = true)</span></span>
<span id="cb7-117"><a href="#cb7-117"></a><span class="co">## |-- text: string (nullable = true)</span></span>
<span id="cb7-118"><a href="#cb7-118"></a><span class="co">## |-- words: array (nullable = true)</span></span>
<span id="cb7-119"><a href="#cb7-119"></a><span class="co">## | |-- element: string (containsNull = true)</span></span>
<span id="cb7-120"><a href="#cb7-120"></a><span class="co">## |-- filteredWords: array (nullable = true)</span></span>
<span id="cb7-121"><a href="#cb7-121"></a><span class="co">## | |-- element: string (containsNull = true)</span></span>
<span id="cb7-122"><a href="#cb7-122"></a><span class="co">## |-- rawFeatures: vector (nullable = true)</span></span>
<span id="cb7-123"><a href="#cb7-123"></a><span class="co">## |-- features: vector (nullable = true)</span></span>
<span id="cb7-124"><a href="#cb7-124"></a><span class="co">## |-- rawPrediction: vector (nullable = true)</span></span>
<span id="cb7-125"><a href="#cb7-125"></a><span class="co">## |-- probability: vector (nullable = true)</span></span>
<span id="cb7-126"><a href="#cb7-126"></a><span class="co">## |-- prediction: double (nullable = false)</span></span>
<span id="cb7-127"><a href="#cb7-127"></a></span>
<span id="cb7-128"><a href="#cb7-128"></a><span class="co">## Select only the original features (i.e., the value of the original text attribute) and</span></span>
<span id="cb7-129"><a href="#cb7-129"></a><span class="co">## the predicted class for each record</span></span>
<span id="cb7-130"><a href="#cb7-130"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"text"</span>, <span class="st">"prediction"</span>)</span>
<span id="cb7-131"><a href="#cb7-131"></a></span>
<span id="cb7-132"><a href="#cb7-132"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb7-133"><a href="#cb7-133"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="performance-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="performance-evaluation">Performance evaluation</h3>
<p>In order to test the goodness of algorithms there are some evaluators. The Evaluator can be</p>
<ul>
<li>a <code>BinaryClassificationEvaluator</code> for binary data</li>
<li>a <code>MulticlassClassificationEvaluator</code> for multiclass problems</li>
</ul>
<p>Provided metrics are:</p>
<ul>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
<li>F-measure</li>
</ul>
<p>Use the <code>MulticlassClassificationEvaluator</code> estimator from <code>pyspark.ml.evaluator</code> on a DataFrame. The instantiated estimator has the method <code>.evaluate()</code> that is applied on a DataFrame: it compares the predictions with the true label values, and the output is the double value of the computed performance metric.</p>
<p>The parameters of <code>MulticlassClassificationEvaluator</code> are</p>
<ul>
<li><code>metricName</code>: type of metric to compute. It can assume the following values
<ul>
<li><code>"accuracy"</code></li>
<li><code>"f1"</code></li>
<li><code>"weightedPrecision"</code></li>
<li><code>"weightedRecall"</code></li>
</ul></li>
<li><code>labelCol</code>: input column with the true label/class value</li>
<li><code>predictionCol</code>: input column with the predicted class/label value</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this example, the set of labeled data is read from a text file that contains one record/data point per line, and the records/data points are structured data with a fixed number of attributes (four)</p>
<ul>
<li>One attribute is the class label (“label”);</li>
<li>The other three attributes (“attr1”, “attr2”, “attr3”) are the predictive attributes that are used to predict the value of the class label.</li>
</ul>
<p>All attributes are already double attributes, and the input file has the header line.</p>
<p>Consider the following example input labeled data file</p>
<pre><code>label,attr1,attr2,attr3
1,0.0,1.1,0.1
0,2.0,1.0,-1.0
0,2.0,1.3,1.0
1,0.0,1.2,-0.5</code></pre>
<p>Follow these steps</p>
<ol type="1">
<li>Split the labeled data set in two subsets
<ul>
<li>Training set: <span class="math inline">\(75\%\)</span> of the labeled data</li>
<li>Test set: <span class="math inline">\(25\%\)</span> of the labeled data</li>
</ul></li>
<li>Infer/train a logistic regression model on the training set</li>
<li>Evaluate the prediction quality of the inferred model on both the test set and the training set</li>
</ol>
<div class="sourceCode" id="annotated-cell-2"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="annotated-cell-2-2"><a href="#annotated-cell-2-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="annotated-cell-2-3"><a href="#annotated-cell-2-3"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="annotated-cell-2-4"><a href="#annotated-cell-2-4"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> MulticlassClassificationEvaluator</span>
<span id="annotated-cell-2-5"><a href="#annotated-cell-2-5"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="annotated-cell-2-6"><a href="#annotated-cell-2-6"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="annotated-cell-2-7"><a href="#annotated-cell-2-7"></a></span>
<span id="annotated-cell-2-8"><a href="#annotated-cell-2-8"></a><span class="co">## input and output folders</span></span>
<span id="annotated-cell-2-9"><a href="#annotated-cell-2-9"></a>labeledData <span class="op">=</span> <span class="st">"ex_dataValidation/labeledData.csv"</span></span>
<span id="annotated-cell-2-10"><a href="#annotated-cell-2-10"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLRPipelineValidation/"</span></span>
<span id="annotated-cell-2-11"><a href="#annotated-cell-2-11"></a></span>
<span id="annotated-cell-2-12"><a href="#annotated-cell-2-12"></a><span class="co">## Create a DataFrame from labeledData.csv</span></span>
<span id="annotated-cell-2-13"><a href="#annotated-cell-2-13"></a><span class="co">## Training data in raw format</span></span>
<span id="annotated-cell-2-14"><a href="#annotated-cell-2-14"></a>labeledDataDF <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-2-15"><a href="#annotated-cell-2-15"></a>    labeledData,</span>
<span id="annotated-cell-2-16"><a href="#annotated-cell-2-16"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-2-17"><a href="#annotated-cell-2-17"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="annotated-cell-2-18"><a href="#annotated-cell-2-18"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-2-19"><a href="#annotated-cell-2-19"></a>)</span>
<span id="annotated-cell-2-20"><a href="#annotated-cell-2-20"></a></span>
<span id="annotated-cell-2-21"><a href="#annotated-cell-2-21"></a><span class="co">## Split labeled data in training and test set</span></span>
<span id="annotated-cell-2-22"><a href="#annotated-cell-2-22"></a><span class="co">## training data : 75%</span></span>
<span id="annotated-cell-2-23"><a href="#annotated-cell-2-23"></a><span class="co">## test data: 25%</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1">1</button><span id="annotated-cell-2-24" class="code-annotation-target"><a href="#annotated-cell-2-24"></a>trainDF, testDF <span class="op">=</span> labeledDataDF.randomSplit([<span class="fl">0.75</span>, <span class="fl">0.25</span>], seed<span class="op">=</span><span class="dv">10</span>)</span>
<span id="annotated-cell-2-25"><a href="#annotated-cell-2-25"></a></span>
<span id="annotated-cell-2-26"><a href="#annotated-cell-2-26"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-2-27"><a href="#annotated-cell-2-27"></a><span class="co">## Training step</span></span>
<span id="annotated-cell-2-28"><a href="#annotated-cell-2-28"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-2-29"><a href="#annotated-cell-2-29"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="annotated-cell-2-30"><a href="#annotated-cell-2-30"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="annotated-cell-2-31"><a href="#annotated-cell-2-31"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="annotated-cell-2-32"><a href="#annotated-cell-2-32"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>],</span>
<span id="annotated-cell-2-33"><a href="#annotated-cell-2-33"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="annotated-cell-2-34"><a href="#annotated-cell-2-34"></a>)</span>
<span id="annotated-cell-2-35"><a href="#annotated-cell-2-35"></a></span>
<span id="annotated-cell-2-36"><a href="#annotated-cell-2-36"></a><span class="co">## Create a LogisticRegression object.</span></span>
<span id="annotated-cell-2-37"><a href="#annotated-cell-2-37"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="annotated-cell-2-38"><a href="#annotated-cell-2-38"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="annotated-cell-2-39"><a href="#annotated-cell-2-39"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="annotated-cell-2-40"><a href="#annotated-cell-2-40"></a></span>
<span id="annotated-cell-2-41"><a href="#annotated-cell-2-41"></a><span class="co">## Set the values of the parameters of the</span></span>
<span id="annotated-cell-2-42"><a href="#annotated-cell-2-42"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="annotated-cell-2-43"><a href="#annotated-cell-2-43"></a><span class="co">## There is one set method for each parameter</span></span>
<span id="annotated-cell-2-44"><a href="#annotated-cell-2-44"></a><span class="co">## For example, we are setting the number of maximum iterations to 10</span></span>
<span id="annotated-cell-2-45"><a href="#annotated-cell-2-45"></a><span class="co">## and the regularization parameter to 0.01</span></span>
<span id="annotated-cell-2-46"><a href="#annotated-cell-2-46"></a>lr.setMaxIter(<span class="dv">10</span>)</span>
<span id="annotated-cell-2-47"><a href="#annotated-cell-2-47"></a>lr.setRegParam(<span class="fl">0.01</span>)</span>
<span id="annotated-cell-2-48"><a href="#annotated-cell-2-48"></a></span>
<span id="annotated-cell-2-49"><a href="#annotated-cell-2-49"></a><span class="co">## Define a pipeline that is used to create the logistic regression</span></span>
<span id="annotated-cell-2-50"><a href="#annotated-cell-2-50"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="annotated-cell-2-51"><a href="#annotated-cell-2-51"></a><span class="co">## the preprocessing step</span></span>
<span id="annotated-cell-2-52"><a href="#annotated-cell-2-52"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, lr])</span>
<span id="annotated-cell-2-53"><a href="#annotated-cell-2-53"></a></span>
<span id="annotated-cell-2-54"><a href="#annotated-cell-2-54"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="annotated-cell-2-55"><a href="#annotated-cell-2-55"></a><span class="co">## classification model</span></span>
<span id="annotated-cell-2-56"><a href="#annotated-cell-2-56"></a>classificationModel <span class="op">=</span> pipeline.fit(trainDF)</span>
<span id="annotated-cell-2-57"><a href="#annotated-cell-2-57"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="annotated-cell-2-58"><a href="#annotated-cell-2-58"></a><span class="co">## of new unlabeled data</span></span>
<span id="annotated-cell-2-59"><a href="#annotated-cell-2-59"></a></span>
<span id="annotated-cell-2-60"><a href="#annotated-cell-2-60"></a><span class="co">## Make predictions on the test data using the transform() method of the</span></span>
<span id="annotated-cell-2-61"><a href="#annotated-cell-2-61"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="annotated-cell-2-62"><a href="#annotated-cell-2-62"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="annotated-cell-2-63"><a href="#annotated-cell-2-63"></a><span class="co">## also the assembler is executed</span></span>
<span id="annotated-cell-2-64"><a href="#annotated-cell-2-64"></a>predictionsDF <span class="op">=</span> classificationModel.transform(testDF)</span>
<span id="annotated-cell-2-65"><a href="#annotated-cell-2-65"></a></span>
<span id="annotated-cell-2-66"><a href="#annotated-cell-2-66"></a><span class="co">## The predicted value is column prediction</span></span>
<span id="annotated-cell-2-67"><a href="#annotated-cell-2-67"></a><span class="co">## The actual label is column label</span></span>
<span id="annotated-cell-2-68"><a href="#annotated-cell-2-68"></a><span class="co">## Define a set of evaluators</span></span>
<span id="annotated-cell-2-69"><a href="#annotated-cell-2-69"></a>myEvaluatorAcc <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="annotated-cell-2-70"><a href="#annotated-cell-2-70"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="annotated-cell-2-71"><a href="#annotated-cell-2-71"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="annotated-cell-2-72"><a href="#annotated-cell-2-72"></a>    metricName<span class="op">=</span><span class="st">'accuracy'</span></span>
<span id="annotated-cell-2-73"><a href="#annotated-cell-2-73"></a>)</span>
<span id="annotated-cell-2-74"><a href="#annotated-cell-2-74"></a></span>
<span id="annotated-cell-2-75"><a href="#annotated-cell-2-75"></a>myEvaluatorF1 <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="annotated-cell-2-76"><a href="#annotated-cell-2-76"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="annotated-cell-2-77"><a href="#annotated-cell-2-77"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="annotated-cell-2-78"><a href="#annotated-cell-2-78"></a>    metricName<span class="op">=</span><span class="st">'f1'</span></span>
<span id="annotated-cell-2-79"><a href="#annotated-cell-2-79"></a>)</span>
<span id="annotated-cell-2-80"><a href="#annotated-cell-2-80"></a></span>
<span id="annotated-cell-2-81"><a href="#annotated-cell-2-81"></a>myEvaluatorWeightedPrecision <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="annotated-cell-2-82"><a href="#annotated-cell-2-82"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="annotated-cell-2-83"><a href="#annotated-cell-2-83"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="annotated-cell-2-84"><a href="#annotated-cell-2-84"></a>    metricName<span class="op">=</span><span class="st">'weightedPrecision'</span></span>
<span id="annotated-cell-2-85"><a href="#annotated-cell-2-85"></a>)</span>
<span id="annotated-cell-2-86"><a href="#annotated-cell-2-86"></a></span>
<span id="annotated-cell-2-87"><a href="#annotated-cell-2-87"></a>myEvaluatorWeightedRecall <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="annotated-cell-2-88"><a href="#annotated-cell-2-88"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="annotated-cell-2-89"><a href="#annotated-cell-2-89"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="annotated-cell-2-90"><a href="#annotated-cell-2-90"></a>    metricName<span class="op">=</span><span class="st">'weightedRecall'</span></span>
<span id="annotated-cell-2-91"><a href="#annotated-cell-2-91"></a>)</span>
<span id="annotated-cell-2-92"><a href="#annotated-cell-2-92"></a></span>
<span id="annotated-cell-2-93"><a href="#annotated-cell-2-93"></a><span class="co">## Apply the evaluators on the predictions associated with the test data</span></span>
<span id="annotated-cell-2-94"><a href="#annotated-cell-2-94"></a><span class="co">## Print the results on the standard output</span></span>
<span id="annotated-cell-2-95"><a href="#annotated-cell-2-95"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-96"><a href="#annotated-cell-2-96"></a>    <span class="st">"Accuracy on test data "</span>, </span>
<span id="annotated-cell-2-97"><a href="#annotated-cell-2-97"></a>    myEvaluatorAcc.evaluate(predictionsDF)</span>
<span id="annotated-cell-2-98"><a href="#annotated-cell-2-98"></a>)</span>
<span id="annotated-cell-2-99"><a href="#annotated-cell-2-99"></a></span>
<span id="annotated-cell-2-100"><a href="#annotated-cell-2-100"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-101"><a href="#annotated-cell-2-101"></a>    <span class="st">"F1 on test data "</span>, </span>
<span id="annotated-cell-2-102"><a href="#annotated-cell-2-102"></a>    myEvaluatorF1.evaluate(predictionsDF)</span>
<span id="annotated-cell-2-103"><a href="#annotated-cell-2-103"></a>)</span>
<span id="annotated-cell-2-104"><a href="#annotated-cell-2-104"></a></span>
<span id="annotated-cell-2-105"><a href="#annotated-cell-2-105"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-106"><a href="#annotated-cell-2-106"></a>    <span class="st">"Weighted recall on test data "</span>,</span>
<span id="annotated-cell-2-107"><a href="#annotated-cell-2-107"></a>    myEvaluatorWeightedRecall.evaluate(predictionsDF)</span>
<span id="annotated-cell-2-108"><a href="#annotated-cell-2-108"></a>)</span>
<span id="annotated-cell-2-109"><a href="#annotated-cell-2-109"></a></span>
<span id="annotated-cell-2-110"><a href="#annotated-cell-2-110"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-111"><a href="#annotated-cell-2-111"></a>    <span class="st">"Weighted precision on test data "</span>,</span>
<span id="annotated-cell-2-112"><a href="#annotated-cell-2-112"></a>    myEvaluatorWeightedPrecision.evaluate(predictionsDF)</span>
<span id="annotated-cell-2-113"><a href="#annotated-cell-2-113"></a>)</span>
<span id="annotated-cell-2-114"><a href="#annotated-cell-2-114"></a></span>
<span id="annotated-cell-2-115"><a href="#annotated-cell-2-115"></a><span class="co">## Compute the prediction quality also for the training data.</span></span>
<span id="annotated-cell-2-116"><a href="#annotated-cell-2-116"></a><span class="co">## To check if the model is overfitted on the training data</span></span>
<span id="annotated-cell-2-117"><a href="#annotated-cell-2-117"></a></span>
<span id="annotated-cell-2-118"><a href="#annotated-cell-2-118"></a><span class="co">## Make predictions on the training data using the transform() method of the</span></span>
<span id="annotated-cell-2-119"><a href="#annotated-cell-2-119"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="annotated-cell-2-120"><a href="#annotated-cell-2-120"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="annotated-cell-2-121"><a href="#annotated-cell-2-121"></a><span class="co">## also the assembler is executed</span></span>
<span id="annotated-cell-2-122"><a href="#annotated-cell-2-122"></a>predictionsTrainingDF <span class="op">=</span> classificationModel.transform(trainDF)</span>
<span id="annotated-cell-2-123"><a href="#annotated-cell-2-123"></a></span>
<span id="annotated-cell-2-124"><a href="#annotated-cell-2-124"></a><span class="co">## Apply the evaluators on the predictions associated with the test data</span></span>
<span id="annotated-cell-2-125"><a href="#annotated-cell-2-125"></a><span class="co">## Print the results on the standard output</span></span>
<span id="annotated-cell-2-126"><a href="#annotated-cell-2-126"></a></span>
<span id="annotated-cell-2-127"><a href="#annotated-cell-2-127"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-128"><a href="#annotated-cell-2-128"></a>    <span class="st">"Accuracy on training data "</span>,</span>
<span id="annotated-cell-2-129"><a href="#annotated-cell-2-129"></a>    myEvaluatorAcc.evaluate(predictionsTrainingDF)</span>
<span id="annotated-cell-2-130"><a href="#annotated-cell-2-130"></a>)</span>
<span id="annotated-cell-2-131"><a href="#annotated-cell-2-131"></a></span>
<span id="annotated-cell-2-132"><a href="#annotated-cell-2-132"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-133"><a href="#annotated-cell-2-133"></a>    <span class="st">"F1 on training data "</span>,</span>
<span id="annotated-cell-2-134"><a href="#annotated-cell-2-134"></a>    myEvaluatorF1.evaluate(predictionsTrainingDF)</span>
<span id="annotated-cell-2-135"><a href="#annotated-cell-2-135"></a>)</span>
<span id="annotated-cell-2-136"><a href="#annotated-cell-2-136"></a></span>
<span id="annotated-cell-2-137"><a href="#annotated-cell-2-137"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-138"><a href="#annotated-cell-2-138"></a>    <span class="st">"Weighted recall on training data "</span>,</span>
<span id="annotated-cell-2-139"><a href="#annotated-cell-2-139"></a>    myEvaluatorWeightedRecall.evaluate(predictionsTrainingDF)</span>
<span id="annotated-cell-2-140"><a href="#annotated-cell-2-140"></a>)</span>
<span id="annotated-cell-2-141"><a href="#annotated-cell-2-141"></a></span>
<span id="annotated-cell-2-142"><a href="#annotated-cell-2-142"></a><span class="bu">print</span>(</span>
<span id="annotated-cell-2-143"><a href="#annotated-cell-2-143"></a>    <span class="st">"Weighted precision on training data "</span>,</span>
<span id="annotated-cell-2-144"><a href="#annotated-cell-2-144"></a>    myEvaluatorWeightedPrecision.evaluate(predictionsTrainingDF)</span>
<span id="annotated-cell-2-145"><a href="#annotated-cell-2-145"></a>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-lines="24" data-code-annotation="1" data-code-cell="annotated-cell-2"><code>randomSplit</code> can be used to split the content of an input DataFrame in subsets</span>
</dd>
</dl>
</div>
</div>
</div>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter tuning</h3>
<p>The setting of the parameters of an algorithm is always a difficult task. A brute force approach can be used to find the setting optimizing a quality index, by splitting the training data in two subsets:</p>
<ul>
<li>The first set is used to build a model</li>
<li>The second one is used to evaluate the quality of the model</li>
</ul>
<p>The setting that maximizes a quality index (e.g., the prediction accuracy) is used to build the final model on the whole training dataset.</p>
<p>Using one single split of the training set usually leads to biased results, so the cross-validation approach is normally used</p>
<ul>
<li>Create <span class="math inline">\(k\)</span> splits and <span class="math inline">\(k\)</span> models</li>
<li>The parameter setting that achieves, on the average, the best result on the <span class="math inline">\(k\)</span> models is selected as final setting of the algorithm parameters</li>
</ul>
<p>Spark supports a brute-force grid-based approach to evaluate a set of possible parameter settings on a pipeline</p>
<ul>
<li>Input
<ul>
<li>An MLlib pipeline</li>
<li>A set of values to be evaluated for each input parameter of the pipeline: all the possible combinations of the specified parameter values are considered and the related models are automatically generated and evaluated by Spark</li>
<li>A quality evaluation metric to evaluate the result of the input pipeline</li>
</ul></li>
<li>Output: the model associated with the best parameter setting, in term of quality evaluation metric</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This example shows how a grid-based approach can be used to tune a logistic regression classifier on a structured dataset: the pipeline that is repeated multiple times is based on the cross validation component. The input data set is the same structured dataset used for the example of the evaluators.</p>
<p>The following parameters of the logistic regression algorithm are considered in the brute-force search/parameter tuning</p>
<ul>
<li>Maximum iteration: <span class="math inline">\([10, 100, 1000]\)</span></li>
<li>Regulation parameter: <span class="math inline">\([0.1, 0.01]\)</span></li>
</ul>
<p>In total, 6 parameter configurations are evaluated (<span class="math inline">\(3*2\)</span>).</p>
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> MulticlassClassificationEvaluator</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> BinaryClassificationEvaluator</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a><span class="im">from</span> pyspark.ml.tuning <span class="im">import</span> ParamGridBuilder</span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a><span class="im">from</span> pyspark.ml.tuning <span class="im">import</span> CrossValidator</span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a></span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11"></a><span class="co">## input and output folders</span></span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a>labeledData <span class="op">=</span> <span class="st">"ex_dataValidation/labeledData.csv"</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_dataValidation/unlabeledData.csv"</span></span>
<span id="annotated-cell-1-14"><a href="#annotated-cell-1-14"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLRPipelineTuning/"</span></span>
<span id="annotated-cell-1-15"><a href="#annotated-cell-1-15"></a></span>
<span id="annotated-cell-1-16"><a href="#annotated-cell-1-16"></a><span class="co">## Create a DataFrame from labeledData.csv</span></span>
<span id="annotated-cell-1-17"><a href="#annotated-cell-1-17"></a><span class="co">## Training data in raw format</span></span>
<span id="annotated-cell-1-18"><a href="#annotated-cell-1-18"></a>labeledDataDF <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-1-19"><a href="#annotated-cell-1-19"></a>    labeledData,</span>
<span id="annotated-cell-1-20"><a href="#annotated-cell-1-20"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-1-21"><a href="#annotated-cell-1-21"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="annotated-cell-1-22"><a href="#annotated-cell-1-22"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-1-23"><a href="#annotated-cell-1-23"></a>)</span>
<span id="annotated-cell-1-24"><a href="#annotated-cell-1-24"></a></span>
<span id="annotated-cell-1-25"><a href="#annotated-cell-1-25"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-26"><a href="#annotated-cell-1-26"></a><span class="co">## Training step</span></span>
<span id="annotated-cell-1-27"><a href="#annotated-cell-1-27"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-28"><a href="#annotated-cell-1-28"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="annotated-cell-1-29"><a href="#annotated-cell-1-29"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="annotated-cell-1-30"><a href="#annotated-cell-1-30"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="annotated-cell-1-31"><a href="#annotated-cell-1-31"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>],</span>
<span id="annotated-cell-1-32"><a href="#annotated-cell-1-32"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="annotated-cell-1-33"><a href="#annotated-cell-1-33"></a>)</span>
<span id="annotated-cell-1-34"><a href="#annotated-cell-1-34"></a></span>
<span id="annotated-cell-1-35"><a href="#annotated-cell-1-35"></a><span class="co">## Create a LogisticRegression object.</span></span>
<span id="annotated-cell-1-36"><a href="#annotated-cell-1-36"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="annotated-cell-1-37"><a href="#annotated-cell-1-37"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="annotated-cell-1-38"><a href="#annotated-cell-1-38"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="annotated-cell-1-39"><a href="#annotated-cell-1-39"></a></span>
<span id="annotated-cell-1-40"><a href="#annotated-cell-1-40"></a><span class="co">## Define a pipeline that is used to create the logistic regression</span></span>
<span id="annotated-cell-1-41"><a href="#annotated-cell-1-41"></a><span class="co">## model on the training data. The pipeline includes also the preprocessing step</span></span>
<span id="annotated-cell-1-42"><a href="#annotated-cell-1-42"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, lr])</span>
<span id="annotated-cell-1-43"><a href="#annotated-cell-1-43"></a></span>
<span id="annotated-cell-1-44"><a href="#annotated-cell-1-44"></a><span class="co">## We use a ParamGridBuilder to construct a grid of parameter values to</span></span>
<span id="annotated-cell-1-45"><a href="#annotated-cell-1-45"></a><span class="co">## search over.</span></span>
<span id="annotated-cell-1-46"><a href="#annotated-cell-1-46"></a><span class="co">## We set 3 values for lr.setMaxIter and 2 values for lr.regParam.</span></span>
<span id="annotated-cell-1-47"><a href="#annotated-cell-1-47"></a><span class="co">## This grid will evaluate 3 x 2 = 6 parameter settings for</span></span>
<span id="annotated-cell-1-48"><a href="#annotated-cell-1-48"></a><span class="co">## the input pipeline.</span></span>
<span id="annotated-cell-1-49"><a href="#annotated-cell-1-49"></a>paramGrid <span class="op">=</span> ParamGridBuilder() <span class="op">\</span></span>
<span id="annotated-cell-1-50"><a href="#annotated-cell-1-50"></a>    .addGrid(lr.maxIter, [<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>]) <span class="op">\</span></span>
<span id="annotated-cell-1-51"><a href="#annotated-cell-1-51"></a>    .addGrid(lr.regParam, [<span class="fl">0.1</span>,<span class="fl">0.01</span>]) <span class="op">\</span></span>
<span id="annotated-cell-1-52"><a href="#annotated-cell-1-52"></a>    .build()</span>
<span id="annotated-cell-1-53"><a href="#annotated-cell-1-53"></a></span>
<span id="annotated-cell-1-54"><a href="#annotated-cell-1-54"></a><span class="co">## We now treat the Pipeline as an Estimator, wrapping it in a</span></span>
<span id="annotated-cell-1-55"><a href="#annotated-cell-1-55"></a><span class="co">## CrossValidator instance. This allows us to jointly choose parameters</span></span>
<span id="annotated-cell-1-56"><a href="#annotated-cell-1-56"></a><span class="co">## for all Pipeline stages.</span></span>
<span id="annotated-cell-1-57"><a href="#annotated-cell-1-57"></a><span class="co">## CrossValidator requires</span></span>
<span id="annotated-cell-1-58"><a href="#annotated-cell-1-58"></a><span class="co">## - an Estimator</span></span>
<span id="annotated-cell-1-59"><a href="#annotated-cell-1-59"></a><span class="co">## - a set of Estimator ParamMaps</span></span>
<span id="annotated-cell-1-60"><a href="#annotated-cell-1-60"></a><span class="co">## - an Evaluator.</span></span>
<span id="annotated-cell-1-61"><a href="#annotated-cell-1-61"></a>cv <span class="op">=</span> CrossValidator() <span class="op">\</span></span>
<span id="annotated-cell-1-62"><a href="#annotated-cell-1-62"></a>    .setEstimator(pipeline) <span class="op">\</span></span>
<span id="annotated-cell-1-63"><a href="#annotated-cell-1-63"></a>    .setEstimatorParamMaps(paramGrid) <span class="op">\</span></span>
<span id="annotated-cell-1-64"><a href="#annotated-cell-1-64"></a>    .setEvaluator(BinaryClassificationEvaluator()) <span class="op">\</span></span>
<span id="annotated-cell-1-65"><a href="#annotated-cell-1-65"></a>    .setNumFolds(<span class="dv">3</span>)</span>
<span id="annotated-cell-1-66"><a href="#annotated-cell-1-66"></a></span>
<span id="annotated-cell-1-67"><a href="#annotated-cell-1-67"></a><span class="co">## Run cross-validation. The result is the logistic regression model</span></span>
<span id="annotated-cell-1-68"><a href="#annotated-cell-1-68"></a><span class="co">## based on the best set of parameters (based on the results of the</span></span>
<span id="annotated-cell-1-69"><a href="#annotated-cell-1-69"></a><span class="co">## cross-validation operation).</span></span>
<span id="annotated-cell-1-70"><a href="#annotated-cell-1-70"></a>tunedLRmodel <span class="op">=</span> cv.fit(labeledDataDF)</span>
<span id="annotated-cell-1-71"><a href="#annotated-cell-1-71"></a></span>
<span id="annotated-cell-1-72"><a href="#annotated-cell-1-72"></a><span class="co">## Now, the tuned classification model can be used to predict the class label</span></span>
<span id="annotated-cell-1-73"><a href="#annotated-cell-1-73"></a><span class="co">## of new unlabeled data</span></span>
<span id="annotated-cell-1-74"><a href="#annotated-cell-1-74"></a></span>
<span id="annotated-cell-1-75"><a href="#annotated-cell-1-75"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-76"><a href="#annotated-cell-1-76"></a><span class="co">## Prediction step</span></span>
<span id="annotated-cell-1-77"><a href="#annotated-cell-1-77"></a><span class="co">## *************************</span></span>
<span id="annotated-cell-1-78"><a href="#annotated-cell-1-78"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="annotated-cell-1-79"><a href="#annotated-cell-1-79"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="annotated-cell-1-80"><a href="#annotated-cell-1-80"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="annotated-cell-1-81"><a href="#annotated-cell-1-81"></a>    unlabeledData,</span>
<span id="annotated-cell-1-82"><a href="#annotated-cell-1-82"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="annotated-cell-1-83"><a href="#annotated-cell-1-83"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="annotated-cell-1-84"><a href="#annotated-cell-1-84"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="annotated-cell-1-85"><a href="#annotated-cell-1-85"></a>)</span>
<span id="annotated-cell-1-86"><a href="#annotated-cell-1-86"></a></span>
<span id="annotated-cell-1-87"><a href="#annotated-cell-1-87"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="annotated-cell-1-88"><a href="#annotated-cell-1-88"></a><span class="co">## trained tuned classification model transform uses only the content of 'features'</span></span>
<span id="annotated-cell-1-89"><a href="#annotated-cell-1-89"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="annotated-cell-1-90"><a href="#annotated-cell-1-90"></a><span class="co">## also the assembler is executed</span></span>
<span id="annotated-cell-1-91"><a href="#annotated-cell-1-91"></a>predictionsDF <span class="op">=</span> tunedLRmodel.transform(unlabeledData)</span>
<span id="annotated-cell-1-92"><a href="#annotated-cell-1-92"></a></span>
<span id="annotated-cell-1-93"><a href="#annotated-cell-1-93"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="annotated-cell-1-94"><a href="#annotated-cell-1-94"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="annotated-cell-1-95"><a href="#annotated-cell-1-95"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="annotated-cell-1-96"><a href="#annotated-cell-1-96"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="annotated-cell-1-97"><a href="#annotated-cell-1-97"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the current</span></span>
<span id="annotated-cell-1-98"><a href="#annotated-cell-1-98"></a><span class="co">## record belongs to the i-th class</span></span>
<span id="annotated-cell-1-99"><a href="#annotated-cell-1-99"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="annotated-cell-1-100"><a href="#annotated-cell-1-100"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="annotated-cell-1-101"><a href="#annotated-cell-1-101"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="annotated-cell-1-102"><a href="#annotated-cell-1-102"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>, <span class="st">"prediction"</span>)</span>
<span id="annotated-cell-1-103"><a href="#annotated-cell-1-103"></a></span>
<span id="annotated-cell-1-104"><a href="#annotated-cell-1-104"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="annotated-cell-1-105"><a href="#annotated-cell-1-105"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl>
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-lines="52" data-code-annotation="1" data-code-cell="annotated-cell-1">There is one call to the addGrid method for each parameter that we want to set: each call to the addGrid method is characterized by the parameter we want to consider, and the list of values to test/to consider.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-lines="65" data-code-annotation="2" data-code-cell="annotated-cell-1">Here the characteristics of the cross validation are set: the pipeline to be evaluated, the set of parameters to be considered, the evaluator (i.e., the object that is used to evaluate the quality of the model), and the number of folds to consider (i.e., the number of repetitions).</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-lines="70" data-code-annotation="3" data-code-cell="annotated-cell-1">The returned model is the one associated with the best parameter setting, based on the result of the cross-validation test</span>
</dd>
</dl>
</div>
</div>
</div>
</section>
<section id="sparse-labeled-data" class="level3">
<h3 class="anchored" data-anchor-id="sparse-labeled-data">Sparse labeled data</h3>
<p>Frequently the training data are sparse (e.g., textual data are sparse: each document contains only a subset of the possible words), so sparse vectors are frequently used. MLlib supports reading training examples stored in the LIBSVM format: this is a commonly used textual format that is used to represent sparse documents/data points.</p>
<p>The LIBSVM format is a textual format in which each line represents an input record/data point by using a sparse feature vector: each line has the format</p>
<pre><code>label index1:value1 index2:value2 ...</code></pre>
<p>where</p>
<ul>
<li><code>label</code> is an integer associated with the class label. It is the first value of each line.</li>
<li><code>index#</code> are integer values representing the features</li>
<li><code>value#</code> are the (double) values of the features</li>
</ul>
<p>Consider the following two records/data points characterized by 4 predictive features and a class label</p>
<table class="table">
<tbody>
<tr class="odd">
<td><span class="math inline">\(\textbf{Features} = [5.8, 1.7, 0 , 0 ]\)</span></td>
<td><span class="math inline">\(\textbf{Label} = 1\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\textbf{Features} = [4.1, 0 , 2.5, 1.2]\)</span></td>
<td><span class="math inline">\(\textbf{Label} = 0\)</span></td>
</tr>
</tbody>
</table>
<p>Their LIBSVM format-based representation is the following</p>
<pre><code>1 1:5.8 2:1.7
0 1:4.1 3:2.5 4:1.2</code></pre>
<p>LIBSVM files can be loaded into DataFrames by combining the following methods</p>
<ul>
<li><code>read()</code></li>
<li><code>format("libsvm")</code></li>
<li><code>load(inputpath)</code></li>
</ul>
<p>The returned DataFrame has two columns:</p>
<ul>
<li>label: the double value associated with the label</li>
<li>features: the sparse vector associated with the predictive features</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>spark.read.<span class="bu">format</span>(<span class="st">"libsvm"</span>).load(<span class="st">"sample_libsvm_data.txt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Handle positioning of the toggle
      window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./18a_spark_mllib.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Spark MLlib</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./18c_clustering.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Clustering algorithms</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1"></a><span class="fu"># Classification algorithms</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>Spark MLlib provides a (limited) set of classification algorithms</span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="ss">- </span>Logistic regression</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="ss">    - </span>Binomial logistic regression</span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="ss">    - </span>Multinomial logistic regression</span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="ss">- </span>Decision tree classifier</span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="ss">- </span>Random forest classifier</span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="ss">- </span>Gradient-boosted tree classifier</span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="ss">- </span>Multilayer perceptron classifier</span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="ss">- </span>Linear Support Vector Machine</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a>All the available classification algorithms are based on two phases:</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="ss">1. </span>Model generation based on a set of training data</span>
<span id="cb12-16"><a href="#cb12-16"></a><span class="ss">2. </span>Prediction of the class label of new unlabeled data</span>
<span id="cb12-17"><a href="#cb12-17"></a></span>
<span id="cb12-18"><a href="#cb12-18"></a>All the classification algorithms available in Spark work only on numerical attributes: categorical values must be mapped to integer values (one distinct value per class) before applying the MLlib classification algorithms.</span>
<span id="cb12-19"><a href="#cb12-19"></a></span>
<span id="cb12-20"><a href="#cb12-20"></a>All the Spark classification algorithms are trained on top of an input DataFrame containing (at least) two columns</span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a><span class="ss">- </span>label: the class label, (i.e., the attribute to be predicted by the classification model); it is an integer value (casted to a double)</span>
<span id="cb12-23"><a href="#cb12-23"></a><span class="ss">- </span>features: a vector of doubles containing the values of the predictive attributes of the input records/data points; the data type of this column is <span class="in">`pyspark.ml.linalg.Vector`</span>, and both dense and sparse vectors can be used</span>
<span id="cb12-24"><a href="#cb12-24"></a></span>
<span id="cb12-25"><a href="#cb12-25"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-26"><a href="#cb12-26"></a><span class="fu">### Example</span></span>
<span id="cb12-27"><a href="#cb12-27"></a>Consider the following classification problem: the goal is to predict if new customers are good customers or not based on their monthly income and number of children. </span>
<span id="cb12-28"><a href="#cb12-28"></a></span>
<span id="cb12-29"><a href="#cb12-29"></a>The predictive attributes are</span>
<span id="cb12-30"><a href="#cb12-30"></a></span>
<span id="cb12-31"><a href="#cb12-31"></a><span class="ss">- </span>Monthly income</span>
<span id="cb12-32"><a href="#cb12-32"></a><span class="ss">- </span>Number of children</span>
<span id="cb12-33"><a href="#cb12-33"></a></span>
<span id="cb12-34"><a href="#cb12-34"></a>The class label (target attribute) is "Customer type": </span>
<span id="cb12-35"><a href="#cb12-35"></a></span>
<span id="cb12-36"><a href="#cb12-36"></a><span class="ss">- </span>"Good customer", mapped to 1</span>
<span id="cb12-37"><a href="#cb12-37"></a><span class="ss">- </span>"Bad customer", mapped to 0</span>
<span id="cb12-38"><a href="#cb12-38"></a></span>
<span id="cb12-39"><a href="#cb12-39"></a>**Example of input training data**</span>
<span id="cb12-40"><a href="#cb12-40"></a></span>
<span id="cb12-41"><a href="#cb12-41"></a>The training data is the set of customers for which the value of the class label is known: they are used by the classification algorithm to infer/train a classification model.</span>
<span id="cb12-42"><a href="#cb12-42"></a></span>
<span id="cb12-43"><a href="#cb12-43"></a>|CustomerType|MonthlyIncome|NumChildren|</span>
<span id="cb12-44"><a href="#cb12-44"></a>|-|-|-|</span>
<span id="cb12-45"><a href="#cb12-45"></a>|Good customer|$1400.0$|$2$|</span>
<span id="cb12-46"><a href="#cb12-46"></a>|Bad customer|$11105.5$|$0$|</span>
<span id="cb12-47"><a href="#cb12-47"></a>|Good customer|$2150.0$|$2$|</span>
<span id="cb12-48"><a href="#cb12-48"></a></span>
<span id="cb12-49"><a href="#cb12-49"></a>**Example of input training DataFrame**</span>
<span id="cb12-50"><a href="#cb12-50"></a></span>
<span id="cb12-51"><a href="#cb12-51"></a>The input training DataFrame that must be provided as input to train an MLlib classification algorithm must have the following structure</span>
<span id="cb12-52"><a href="#cb12-52"></a></span>
<span id="cb12-53"><a href="#cb12-53"></a>|label|features|</span>
<span id="cb12-54"><a href="#cb12-54"></a>|-|-|</span>
<span id="cb12-55"><a href="#cb12-55"></a>|$1.0$|$<span class="co">[</span><span class="ot">1400.0,2.0</span><span class="co">]</span>$|</span>
<span id="cb12-56"><a href="#cb12-56"></a>|$0.0$|$<span class="co">[</span><span class="ot">11105.5,0.0</span><span class="co">]</span>$|</span>
<span id="cb12-57"><a href="#cb12-57"></a>|$1.0$|$<span class="co">[</span><span class="ot">2150.0,2.0</span><span class="co">]</span>$|</span>
<span id="cb12-58"><a href="#cb12-58"></a></span>
<span id="cb12-59"><a href="#cb12-59"></a>Notice that</span>
<span id="cb12-60"><a href="#cb12-60"></a></span>
<span id="cb12-61"><a href="#cb12-61"></a><span class="ss">- </span>The categorical values of "CustomerType" (the class label column) must be mapped to integer data values (then casted to doubles).</span>
<span id="cb12-62"><a href="#cb12-62"></a><span class="ss">- </span>The values of the predictive attributes are stored in vectors of doubles. One single vector for each input record. </span>
<span id="cb12-63"><a href="#cb12-63"></a><span class="ss">- </span>In the generated DataFrame the names of the predictive attributes are not preserved.</span>
<span id="cb12-64"><a href="#cb12-64"></a></span>
<span id="cb12-65"><a href="#cb12-65"></a>:::</span>
<span id="cb12-66"><a href="#cb12-66"></a></span>
<span id="cb12-67"><a href="#cb12-67"></a><span class="fu">## Structured data classification</span></span>
<span id="cb12-68"><a href="#cb12-68"></a><span class="fu">### Example of logistic regression and structured data</span></span>
<span id="cb12-69"><a href="#cb12-69"></a>The following paragraphs show how to</span>
<span id="cb12-70"><a href="#cb12-70"></a></span>
<span id="cb12-71"><a href="#cb12-71"></a><span class="ss">- </span>Create a classification model based on the logistic regression algorithm on structured data: the model is inferred by analyzing the training data, (i.e., the example records/data points for which the value of the class label is known).</span>
<span id="cb12-72"><a href="#cb12-72"></a><span class="ss">- </span>Apply the model to new unlabeled data: the inferred model is applied to predict the value of the class label of new unlabeled records/data points.</span>
<span id="cb12-73"><a href="#cb12-73"></a></span>
<span id="cb12-74"><a href="#cb12-74"></a><span class="fu">#### Training data</span></span>
<span id="cb12-75"><a href="#cb12-75"></a></span>
<span id="cb12-76"><a href="#cb12-76"></a>The input training data is stored in a text file that contains one record/data point per line. The records/data points are structured data with a fixed number of attributes (four)</span>
<span id="cb12-77"><a href="#cb12-77"></a></span>
<span id="cb12-78"><a href="#cb12-78"></a><span class="ss">- </span>One attribute is the class label: it assumed that the first column of each record contains the class label;</span>
<span id="cb12-79"><a href="#cb12-79"></a><span class="ss">- </span>The other three attributes are the predictive attributes that are used to predict the value of the class label;</span>
<span id="cb12-80"><a href="#cb12-80"></a></span>
<span id="cb12-81"><a href="#cb12-81"></a>The values are already doubles (no need to convert them), and the input file has the header line.</span>
<span id="cb12-82"><a href="#cb12-82"></a></span>
<span id="cb12-83"><a href="#cb12-83"></a>Consider the following example input training data file</span>
<span id="cb12-84"><a href="#cb12-84"></a></span>
<span id="cb12-85"><a href="#cb12-85"></a><span class="in">```</span></span>
<span id="cb12-86"><a href="#cb12-86"></a><span class="in">label,attr1,attr2,attr3</span></span>
<span id="cb12-87"><a href="#cb12-87"></a><span class="in">1.0,0.0,1.1,0.1</span></span>
<span id="cb12-88"><a href="#cb12-88"></a><span class="in">0.0,2.0,1.0,-1.0</span></span>
<span id="cb12-89"><a href="#cb12-89"></a><span class="in">0.0,2.0,1.3,1.0</span></span>
<span id="cb12-90"><a href="#cb12-90"></a><span class="in">1.0,0.0,1.2,-0.5</span></span>
<span id="cb12-91"><a href="#cb12-91"></a><span class="in">```</span></span>
<span id="cb12-92"><a href="#cb12-92"></a></span>
<span id="cb12-93"><a href="#cb12-93"></a>It contains four records/data points. This is a binary classification problem because the class label assumes only two values: 0 and 1.</span>
<span id="cb12-94"><a href="#cb12-94"></a></span>
<span id="cb12-95"><a href="#cb12-95"></a>The first operation consists in transforming the content of the input training file into a DataFrame containing two columns</span>
<span id="cb12-96"><a href="#cb12-96"></a></span>
<span id="cb12-97"><a href="#cb12-97"></a><span class="ss">- </span>label: the double value that is used to specify the label of each training record;</span>
<span id="cb12-98"><a href="#cb12-98"></a><span class="ss">- </span>features: it is a vector of doubles associated with the values of the predictive features.</span>
<span id="cb12-99"><a href="#cb12-99"></a></span>
<span id="cb12-100"><a href="#cb12-100"></a>|label|features|</span>
<span id="cb12-101"><a href="#cb12-101"></a>|-|-|</span>
<span id="cb12-102"><a href="#cb12-102"></a>|$1.0$|$<span class="co">[</span><span class="ot">0.0,1.1,0.1</span><span class="co">]</span>$|</span>
<span id="cb12-103"><a href="#cb12-103"></a>|$0.0$|$<span class="co">[</span><span class="ot">2.0,1.0,-1.0</span><span class="co">]</span>$|</span>
<span id="cb12-104"><a href="#cb12-104"></a>|$0.0$|$<span class="co">[</span><span class="ot">2.0,1.3,1.0</span><span class="co">]</span>$|</span>
<span id="cb12-105"><a href="#cb12-105"></a>|$1.0$|$<span class="co">[</span><span class="ot">0.0,1.2,-0.5</span><span class="co">]</span>$|</span>
<span id="cb12-106"><a href="#cb12-106"></a></span>
<span id="cb12-107"><a href="#cb12-107"></a><span class="ss">- </span>Data type of "label" is double</span>
<span id="cb12-108"><a href="#cb12-108"></a><span class="ss">- </span>Data type of "features" is <span class="in">`pyspark.ml.linalg.Vector`</span></span>
<span id="cb12-109"><a href="#cb12-109"></a></span>
<span id="cb12-110"><a href="#cb12-110"></a><span class="fu">#### Unlabeled data</span></span>
<span id="cb12-111"><a href="#cb12-111"></a></span>
<span id="cb12-112"><a href="#cb12-112"></a>The file containing the unlabeled data has the same format of the training data file, however the first column is empty because the class label is unknown. The goal is to predict the class label value of each unlabeled data by applying the classification model that has been trained on the training data: the predicted class label value of the unlabeled data is stored in a new column, called "prediction", of the returned DataFrame.</span>
<span id="cb12-113"><a href="#cb12-113"></a></span>
<span id="cb12-114"><a href="#cb12-114"></a>Consider the following input unlabeled data file</span>
<span id="cb12-115"><a href="#cb12-115"></a></span>
<span id="cb12-116"><a href="#cb12-116"></a><span class="in">```</span></span>
<span id="cb12-117"><a href="#cb12-117"></a><span class="in">label,attr1,attr2,attr3</span></span>
<span id="cb12-118"><a href="#cb12-118"></a><span class="in">,-1.0,1.5,1.3</span></span>
<span id="cb12-119"><a href="#cb12-119"></a><span class="in">,3.0,2.0,-0.1</span></span>
<span id="cb12-120"><a href="#cb12-120"></a><span class="in">,0.0,2.2,-1.5</span></span>
<span id="cb12-121"><a href="#cb12-121"></a><span class="in">```</span></span>
<span id="cb12-122"><a href="#cb12-122"></a></span>
<span id="cb12-123"><a href="#cb12-123"></a>It contains three unlabeled records/data points. Notice that the first column is empty (the content before the first comma is the empty string).</span>
<span id="cb12-124"><a href="#cb12-124"></a></span>
<span id="cb12-125"><a href="#cb12-125"></a>Also the unlabeled data must be stored into a DataFrame containing two columns: "label" and "features". So, "label" column is required also for unlabeled data, but its value is set to null for all records.</span>
<span id="cb12-126"><a href="#cb12-126"></a></span>
<span id="cb12-127"><a href="#cb12-127"></a>|label|features|</span>
<span id="cb12-128"><a href="#cb12-128"></a>|-|-|</span>
<span id="cb12-129"><a href="#cb12-129"></a>|null|$<span class="co">[</span><span class="ot">-1.0,1.5,1.3</span><span class="co">]</span>$|</span>
<span id="cb12-130"><a href="#cb12-130"></a>|null|$<span class="co">[</span><span class="ot">3.0,2.0,-0.1</span><span class="co">]</span>$|</span>
<span id="cb12-131"><a href="#cb12-131"></a>|null|$<span class="co">[</span><span class="ot">0.0,2.2,-1.5</span><span class="co">]</span>$|</span>
<span id="cb12-132"><a href="#cb12-132"></a></span>
<span id="cb12-133"><a href="#cb12-133"></a><span class="fu">#### Prediction column</span></span>
<span id="cb12-134"><a href="#cb12-134"></a>After the application of the classification model on the unlabeled data, Spark returns a new DataFrame containing</span>
<span id="cb12-135"><a href="#cb12-135"></a></span>
<span id="cb12-136"><a href="#cb12-136"></a><span class="ss">- </span>The same columns of the input DataFrame</span>
<span id="cb12-137"><a href="#cb12-137"></a><span class="ss">- </span>A new column called prediction, that, for each input unlabeled record, contains the predicted class label value</span>
<span id="cb12-138"><a href="#cb12-138"></a><span class="ss">- </span>Two columns, associated with the probabilities of the predictions (these columns are not considered in the example)</span>
<span id="cb12-139"><a href="#cb12-139"></a></span>
<span id="cb12-140"><a href="#cb12-140"></a>|label|features|prediction|rawPrediction|probability|</span>
<span id="cb12-141"><a href="#cb12-141"></a>|-|-|-|-|-|</span>
<span id="cb12-142"><a href="#cb12-142"></a>|null|$<span class="co">[</span><span class="ot">-1.0,1.5,1.3</span><span class="co">]</span>$|$1.0$|...|...|</span>
<span id="cb12-143"><a href="#cb12-143"></a>|null|$<span class="co">[</span><span class="ot">3.0,2.0,-0.1</span><span class="co">]</span>$|$0.0$|...|...|</span>
<span id="cb12-144"><a href="#cb12-144"></a>|null|$<span class="co">[</span><span class="ot">0.0,2.2,-1.5</span><span class="co">]</span>$|$1.0$|...|...|</span>
<span id="cb12-145"><a href="#cb12-145"></a></span>
<span id="cb12-146"><a href="#cb12-146"></a>The "prediction" column contains the predicted class label values.</span>
<span id="cb12-147"><a href="#cb12-147"></a></span>
<span id="cb12-148"><a href="#cb12-148"></a><span class="fu">#### Example code</span></span>
<span id="cb12-149"><a href="#cb12-149"></a></span>
<span id="cb12-150"><a href="#cb12-150"></a><span class="in">```python</span></span>
<span id="cb12-151"><a href="#cb12-151"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-152"><a href="#cb12-152"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-153"><a href="#cb12-153"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb12-154"><a href="#cb12-154"></a></span>
<span id="cb12-155"><a href="#cb12-155"></a><span class="co">## input and output folders</span></span>
<span id="cb12-156"><a href="#cb12-156"></a>trainingData <span class="op">=</span> <span class="st">"ex_data/trainingData.csv"</span></span>
<span id="cb12-157"><a href="#cb12-157"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_data/unlabeledData.csv"</span></span>
<span id="cb12-158"><a href="#cb12-158"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLR/"</span></span>
<span id="cb12-159"><a href="#cb12-159"></a></span>
<span id="cb12-160"><a href="#cb12-160"></a><span class="co">## *************************</span></span>
<span id="cb12-161"><a href="#cb12-161"></a><span class="co">## Training step</span></span>
<span id="cb12-162"><a href="#cb12-162"></a><span class="co">## *************************</span></span>
<span id="cb12-163"><a href="#cb12-163"></a></span>
<span id="cb12-164"><a href="#cb12-164"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb12-165"><a href="#cb12-165"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-166"><a href="#cb12-166"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-167"><a href="#cb12-167"></a>    trainingData,</span>
<span id="cb12-168"><a href="#cb12-168"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-169"><a href="#cb12-169"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-170"><a href="#cb12-170"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-171"><a href="#cb12-171"></a>)</span>
<span id="cb12-172"><a href="#cb12-172"></a></span>
<span id="cb12-173"><a href="#cb12-173"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb12-174"><a href="#cb12-174"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb12-175"><a href="#cb12-175"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb12-176"><a href="#cb12-176"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="cb12-177"><a href="#cb12-177"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb12-178"><a href="#cb12-178"></a>)</span>
<span id="cb12-179"><a href="#cb12-179"></a></span>
<span id="cb12-180"><a href="#cb12-180"></a><span class="co">## Apply the assembler to create column features for the training data</span></span>
<span id="cb12-181"><a href="#cb12-181"></a>trainingDataDF <span class="op">=</span> assembler.transform(trainingData)</span>
<span id="cb12-182"><a href="#cb12-182"></a></span>
<span id="cb12-183"><a href="#cb12-183"></a><span class="co">## Create a LogisticRegression object.</span></span>
<span id="cb12-184"><a href="#cb12-184"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="cb12-185"><a href="#cb12-185"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="cb12-186"><a href="#cb12-186"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-187"><a href="#cb12-187"></a></span>
<span id="cb12-188"><a href="#cb12-188"></a><span class="co">## It is possible to set the values of the parameters of the</span></span>
<span id="cb12-189"><a href="#cb12-189"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="cb12-190"><a href="#cb12-190"></a><span class="co">## There is one set method for each parameter</span></span>
<span id="cb12-191"><a href="#cb12-191"></a><span class="co">## For example, the number of maximum iterations is set to 10</span></span>
<span id="cb12-192"><a href="#cb12-192"></a><span class="co">## and the regularization parameter is set to 0.01</span></span>
<span id="cb12-193"><a href="#cb12-193"></a>lr.setMaxIter(<span class="dv">10</span>)</span>
<span id="cb12-194"><a href="#cb12-194"></a>lr.setRegParam(<span class="fl">0.01</span>)</span>
<span id="cb12-195"><a href="#cb12-195"></a></span>
<span id="cb12-196"><a href="#cb12-196"></a><span class="co">## Train a logistic regression model on the training data</span></span>
<span id="cb12-197"><a href="#cb12-197"></a>classificationModel <span class="op">=</span> lr.fit(trainingDataDF)</span>
<span id="cb12-198"><a href="#cb12-198"></a></span>
<span id="cb12-199"><a href="#cb12-199"></a><span class="co">## *************************</span></span>
<span id="cb12-200"><a href="#cb12-200"></a><span class="co">## Prediction step</span></span>
<span id="cb12-201"><a href="#cb12-201"></a><span class="co">## *************************</span></span>
<span id="cb12-202"><a href="#cb12-202"></a></span>
<span id="cb12-203"><a href="#cb12-203"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb12-204"><a href="#cb12-204"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb12-205"><a href="#cb12-205"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-206"><a href="#cb12-206"></a>    unlabeledData,</span>
<span id="cb12-207"><a href="#cb12-207"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-208"><a href="#cb12-208"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-209"><a href="#cb12-209"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-210"><a href="#cb12-210"></a>)</span>
<span id="cb12-211"><a href="#cb12-211"></a></span>
<span id="cb12-212"><a href="#cb12-212"></a><span class="co">## Apply the same assembler we created before also on the unlabeled data</span></span>
<span id="cb12-213"><a href="#cb12-213"></a><span class="co">## to create the features column</span></span>
<span id="cb12-214"><a href="#cb12-214"></a>unlabeledDataDF <span class="op">=</span> assembler.transform(unlabeledData)</span>
<span id="cb12-215"><a href="#cb12-215"></a></span>
<span id="cb12-216"><a href="#cb12-216"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="cb12-217"><a href="#cb12-217"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="cb12-218"><a href="#cb12-218"></a><span class="co">## to perform the predictions</span></span>
<span id="cb12-219"><a href="#cb12-219"></a>predictionsDF <span class="op">=</span> classificationModel.transform(unlabeledDataDF)</span>
<span id="cb12-220"><a href="#cb12-220"></a></span>
<span id="cb12-221"><a href="#cb12-221"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb12-222"><a href="#cb12-222"></a><span class="co">## - attr1</span></span>
<span id="cb12-223"><a href="#cb12-223"></a><span class="co">## - attr2</span></span>
<span id="cb12-224"><a href="#cb12-224"></a><span class="co">## - attr3</span></span>
<span id="cb12-225"><a href="#cb12-225"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="cb12-226"><a href="#cb12-226"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="cb12-227"><a href="#cb12-227"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="cb12-228"><a href="#cb12-228"></a><span class="co">## - probability: vector (The i-th cell contains the probability that </span></span>
<span id="cb12-229"><a href="#cb12-229"></a><span class="co">## the current record belongs to the i-th class</span></span>
<span id="cb12-230"><a href="#cb12-230"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="cb12-231"><a href="#cb12-231"></a></span>
<span id="cb12-232"><a href="#cb12-232"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="cb12-233"><a href="#cb12-233"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="cb12-234"><a href="#cb12-234"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>, <span class="st">"prediction"</span>)</span>
<span id="cb12-235"><a href="#cb12-235"></a></span>
<span id="cb12-236"><a href="#cb12-236"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb12-237"><a href="#cb12-237"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb12-238"><a href="#cb12-238"></a><span class="in">```</span></span>
<span id="cb12-239"><a href="#cb12-239"></a></span>
<span id="cb12-240"><a href="#cb12-240"></a><span class="fu">### Pipelines</span></span>
<span id="cb12-241"><a href="#cb12-241"></a>In the previous solution the same preprocessing steps were applied on both training and unlabeled data (the same assembler on both input data). It is possible to use a pipeline to specify the common phases we apply on both input data sets.</span>
<span id="cb12-242"><a href="#cb12-242"></a></span>
<span id="cb12-243"><a href="#cb12-243"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-244"><a href="#cb12-244"></a><span class="fu">### Example</span></span>
<span id="cb12-245"><a href="#cb12-245"></a></span>
<span id="cb12-246"><a href="#cb12-246"></a><span class="in">```python</span></span>
<span id="cb12-247"><a href="#cb12-247"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-248"><a href="#cb12-248"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-249"><a href="#cb12-249"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb12-250"><a href="#cb12-250"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb12-251"><a href="#cb12-251"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb12-252"><a href="#cb12-252"></a></span>
<span id="cb12-253"><a href="#cb12-253"></a><span class="co">## input and output folders</span></span>
<span id="cb12-254"><a href="#cb12-254"></a>trainingData <span class="op">=</span> <span class="st">"ex_data/trainingData.csv"</span></span>
<span id="cb12-255"><a href="#cb12-255"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_data/unlabeledData.csv"</span></span>
<span id="cb12-256"><a href="#cb12-256"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLR/"</span></span>
<span id="cb12-257"><a href="#cb12-257"></a></span>
<span id="cb12-258"><a href="#cb12-258"></a><span class="co">## *************************</span></span>
<span id="cb12-259"><a href="#cb12-259"></a><span class="co">## Training step</span></span>
<span id="cb12-260"><a href="#cb12-260"></a><span class="co">## *************************</span></span>
<span id="cb12-261"><a href="#cb12-261"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb12-262"><a href="#cb12-262"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-263"><a href="#cb12-263"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-264"><a href="#cb12-264"></a>    trainingData,</span>
<span id="cb12-265"><a href="#cb12-265"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-266"><a href="#cb12-266"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-267"><a href="#cb12-267"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-268"><a href="#cb12-268"></a>)</span>
<span id="cb12-269"><a href="#cb12-269"></a></span>
<span id="cb12-270"><a href="#cb12-270"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb12-271"><a href="#cb12-271"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb12-272"><a href="#cb12-272"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb12-273"><a href="#cb12-273"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="cb12-274"><a href="#cb12-274"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb12-275"><a href="#cb12-275"></a>)</span>
<span id="cb12-276"><a href="#cb12-276"></a></span>
<span id="cb12-277"><a href="#cb12-277"></a><span class="co">## Create a LogisticRegression object</span></span>
<span id="cb12-278"><a href="#cb12-278"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="cb12-279"><a href="#cb12-279"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="cb12-280"><a href="#cb12-280"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-281"><a href="#cb12-281"></a></span>
<span id="cb12-282"><a href="#cb12-282"></a><span class="co">## Set the values of the parameters of the</span></span>
<span id="cb12-283"><a href="#cb12-283"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="cb12-284"><a href="#cb12-284"></a><span class="co">## There is one set method for each parameter</span></span>
<span id="cb12-285"><a href="#cb12-285"></a><span class="co">## For example, we are setting the number of maximum iterations to 10</span></span>
<span id="cb12-286"><a href="#cb12-286"></a><span class="co">## and the regularization parameter to 0.01</span></span>
<span id="cb12-287"><a href="#cb12-287"></a>lr.setMaxIter(<span class="dv">10</span>)</span>
<span id="cb12-288"><a href="#cb12-288"></a>lr.setRegParam(<span class="fl">0.01</span>)</span>
<span id="cb12-289"><a href="#cb12-289"></a></span>
<span id="cb12-290"><a href="#cb12-290"></a><span class="co">## Define a pipeline that is used to create the logistic regression</span></span>
<span id="cb12-291"><a href="#cb12-291"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="cb12-292"><a href="#cb12-292"></a><span class="co">## the preprocessing step</span></span>
<span id="cb12-293"><a href="#cb12-293"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, lr]) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-294"><a href="#cb12-294"></a></span>
<span id="cb12-295"><a href="#cb12-295"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="cb12-296"><a href="#cb12-296"></a><span class="co">## classification model</span></span>
<span id="cb12-297"><a href="#cb12-297"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="cb12-298"><a href="#cb12-298"></a></span>
<span id="cb12-299"><a href="#cb12-299"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="cb12-300"><a href="#cb12-300"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb12-301"><a href="#cb12-301"></a></span>
<span id="cb12-302"><a href="#cb12-302"></a><span class="co">## *************************</span></span>
<span id="cb12-303"><a href="#cb12-303"></a><span class="co">## Prediction step</span></span>
<span id="cb12-304"><a href="#cb12-304"></a><span class="co">## *************************</span></span>
<span id="cb12-305"><a href="#cb12-305"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb12-306"><a href="#cb12-306"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb12-307"><a href="#cb12-307"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-308"><a href="#cb12-308"></a>    unlabeledData,</span>
<span id="cb12-309"><a href="#cb12-309"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-310"><a href="#cb12-310"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-311"><a href="#cb12-311"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-312"><a href="#cb12-312"></a>)</span>
<span id="cb12-313"><a href="#cb12-313"></a></span>
<span id="cb12-314"><a href="#cb12-314"></a><span class="co">## Make predictions on the unlabled data using the transform() </span></span>
<span id="cb12-315"><a href="#cb12-315"></a><span class="co">## method of the trained classification model transform uses only </span></span>
<span id="cb12-316"><a href="#cb12-316"></a><span class="co">## the content of 'features' to perform the predictions. The model </span></span>
<span id="cb12-317"><a href="#cb12-317"></a><span class="co">## is associated with the pipeline and hence also the assembler is executed</span></span>
<span id="cb12-318"><a href="#cb12-318"></a>predictions <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="cb12-319"><a href="#cb12-319"></a></span>
<span id="cb12-320"><a href="#cb12-320"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb12-321"><a href="#cb12-321"></a><span class="co">## - attr1</span></span>
<span id="cb12-322"><a href="#cb12-322"></a><span class="co">## - attr2</span></span>
<span id="cb12-323"><a href="#cb12-323"></a><span class="co">## - attr3</span></span>
<span id="cb12-324"><a href="#cb12-324"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="cb12-325"><a href="#cb12-325"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="cb12-326"><a href="#cb12-326"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="cb12-327"><a href="#cb12-327"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the current</span></span>
<span id="cb12-328"><a href="#cb12-328"></a><span class="co">## record belongs to the i-th class</span></span>
<span id="cb12-329"><a href="#cb12-329"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="cb12-330"><a href="#cb12-330"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="cb12-331"><a href="#cb12-331"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="cb12-332"><a href="#cb12-332"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>,<span class="st">"prediction"</span>)</span>
<span id="cb12-333"><a href="#cb12-333"></a></span>
<span id="cb12-334"><a href="#cb12-334"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb12-335"><a href="#cb12-335"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb12-336"><a href="#cb12-336"></a><span class="in">```</span></span>
<span id="cb12-337"><a href="#cb12-337"></a><span class="ss">1. </span><span class="in">`assembler`</span>: the sequence of transformers and estimators to apply on the input data</span>
<span id="cb12-338"><a href="#cb12-338"></a></span>
<span id="cb12-339"><a href="#cb12-339"></a>:::</span>
<span id="cb12-340"><a href="#cb12-340"></a></span>
<span id="cb12-341"><a href="#cb12-341"></a><span class="fu">### Decision trees and structured data</span></span>
<span id="cb12-342"><a href="#cb12-342"></a>The following paragraphs show how to</span>
<span id="cb12-343"><a href="#cb12-343"></a></span>
<span id="cb12-344"><a href="#cb12-344"></a><span class="ss">- </span>Create a classification model based on the decision tree algorithm on structured data: the model is inferred by analyzing the training data, i.e., the example records/data points for which the value of the class label is known;</span>
<span id="cb12-345"><a href="#cb12-345"></a><span class="ss">- </span>Apply the model to new unlabeled data: the inferred model is applied to predict the value of the class label of new unlabeled records/data points.</span>
<span id="cb12-346"><a href="#cb12-346"></a></span>
<span id="cb12-347"><a href="#cb12-347"></a>The same example structured data already used in the running example related to the logistic regression algorithm are used also in this example related to the decision tree algorithm. The main steps are the same of the previous example, the only difference is the definition and configuration of the used classification algorithm.</span>
<span id="cb12-348"><a href="#cb12-348"></a></span>
<span id="cb12-349"><a href="#cb12-349"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-350"><a href="#cb12-350"></a><span class="fu">### Example</span></span>
<span id="cb12-351"><a href="#cb12-351"></a></span>
<span id="cb12-352"><a href="#cb12-352"></a><span class="in">```python</span></span>
<span id="cb12-353"><a href="#cb12-353"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-354"><a href="#cb12-354"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-355"><a href="#cb12-355"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb12-356"><a href="#cb12-356"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb12-357"><a href="#cb12-357"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb12-358"><a href="#cb12-358"></a></span>
<span id="cb12-359"><a href="#cb12-359"></a><span class="co">## input and output folders</span></span>
<span id="cb12-360"><a href="#cb12-360"></a>trainingData <span class="op">=</span> <span class="st">"ex_data/trainingData.csv"</span></span>
<span id="cb12-361"><a href="#cb12-361"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_data/unlabeledData.csv"</span></span>
<span id="cb12-362"><a href="#cb12-362"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLR/"</span></span>
<span id="cb12-363"><a href="#cb12-363"></a></span>
<span id="cb12-364"><a href="#cb12-364"></a><span class="co">## *************************</span></span>
<span id="cb12-365"><a href="#cb12-365"></a><span class="co">## Training step</span></span>
<span id="cb12-366"><a href="#cb12-366"></a><span class="co">## *************************</span></span>
<span id="cb12-367"><a href="#cb12-367"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb12-368"><a href="#cb12-368"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-369"><a href="#cb12-369"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-370"><a href="#cb12-370"></a>    trainingData,</span>
<span id="cb12-371"><a href="#cb12-371"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-372"><a href="#cb12-372"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-373"><a href="#cb12-373"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-374"><a href="#cb12-374"></a>)</span>
<span id="cb12-375"><a href="#cb12-375"></a></span>
<span id="cb12-376"><a href="#cb12-376"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb12-377"><a href="#cb12-377"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb12-378"><a href="#cb12-378"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb12-379"><a href="#cb12-379"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="cb12-380"><a href="#cb12-380"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb12-381"><a href="#cb12-381"></a>)</span>
<span id="cb12-382"><a href="#cb12-382"></a></span>
<span id="cb12-383"><a href="#cb12-383"></a><span class="co">## Create a DecisionTreeClassifier object.</span></span>
<span id="cb12-384"><a href="#cb12-384"></a><span class="co">## DecisionTreeClassifier is an Estimator that is used to</span></span>
<span id="cb12-385"><a href="#cb12-385"></a><span class="co">## create a classification model based on decision trees.</span></span>
<span id="cb12-386"><a href="#cb12-386"></a>dt <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb12-387"><a href="#cb12-387"></a></span>
<span id="cb12-388"><a href="#cb12-388"></a><span class="co">## We can set the values of the parameters of the Decision Tree</span></span>
<span id="cb12-389"><a href="#cb12-389"></a><span class="co">## For example we can set the measure that is used to decide if a</span></span>
<span id="cb12-390"><a href="#cb12-390"></a><span class="co">## node must be split. In this case we set gini index</span></span>
<span id="cb12-391"><a href="#cb12-391"></a>dt.setImpurity(<span class="st">"gini"</span>)</span>
<span id="cb12-392"><a href="#cb12-392"></a></span>
<span id="cb12-393"><a href="#cb12-393"></a><span class="co">## Define a pipeline that is used to create the decision tree</span></span>
<span id="cb12-394"><a href="#cb12-394"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="cb12-395"><a href="#cb12-395"></a><span class="co">## the preprocessing step</span></span>
<span id="cb12-396"><a href="#cb12-396"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, dt]) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-397"><a href="#cb12-397"></a></span>
<span id="cb12-398"><a href="#cb12-398"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="cb12-399"><a href="#cb12-399"></a><span class="co">## classification model</span></span>
<span id="cb12-400"><a href="#cb12-400"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="cb12-401"><a href="#cb12-401"></a></span>
<span id="cb12-402"><a href="#cb12-402"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="cb12-403"><a href="#cb12-403"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb12-404"><a href="#cb12-404"></a></span>
<span id="cb12-405"><a href="#cb12-405"></a><span class="co">## *************************</span></span>
<span id="cb12-406"><a href="#cb12-406"></a><span class="co">## Prediction step</span></span>
<span id="cb12-407"><a href="#cb12-407"></a><span class="co">## *************************</span></span>
<span id="cb12-408"><a href="#cb12-408"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb12-409"><a href="#cb12-409"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb12-410"><a href="#cb12-410"></a>unlabeledData <span class="op">=</span> spark.read.load(unlabeledData,<span class="op">\</span></span>
<span id="cb12-411"><a href="#cb12-411"></a><span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>, header<span class="op">=</span><span class="va">True</span>, inferSchema<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-412"><a href="#cb12-412"></a></span>
<span id="cb12-413"><a href="#cb12-413"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="cb12-414"><a href="#cb12-414"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="cb12-415"><a href="#cb12-415"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="cb12-416"><a href="#cb12-416"></a><span class="co">## also the assembler is executed</span></span>
<span id="cb12-417"><a href="#cb12-417"></a>predictions <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="cb12-418"><a href="#cb12-418"></a></span>
<span id="cb12-419"><a href="#cb12-419"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb12-420"><a href="#cb12-420"></a><span class="co">## - attr1</span></span>
<span id="cb12-421"><a href="#cb12-421"></a><span class="co">## - attr2</span></span>
<span id="cb12-422"><a href="#cb12-422"></a><span class="co">## - attr3</span></span>
<span id="cb12-423"><a href="#cb12-423"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="cb12-424"><a href="#cb12-424"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="cb12-425"><a href="#cb12-425"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="cb12-426"><a href="#cb12-426"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the current</span></span>
<span id="cb12-427"><a href="#cb12-427"></a><span class="co">## record belongs to the i-th class</span></span>
<span id="cb12-428"><a href="#cb12-428"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="cb12-429"><a href="#cb12-429"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="cb12-430"><a href="#cb12-430"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="cb12-431"><a href="#cb12-431"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>,<span class="st">"prediction"</span>)</span>
<span id="cb12-432"><a href="#cb12-432"></a></span>
<span id="cb12-433"><a href="#cb12-433"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb12-434"><a href="#cb12-434"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb12-435"><a href="#cb12-435"></a><span class="in">```</span></span>
<span id="cb12-436"><a href="#cb12-436"></a><span class="ss">1. </span><span class="in">`assembler`</span>: the sequence of transformers and estimators to apply on the input data. A decision tree algorithm is used in this case.</span>
<span id="cb12-437"><a href="#cb12-437"></a></span>
<span id="cb12-438"><a href="#cb12-438"></a>:::</span>
<span id="cb12-439"><a href="#cb12-439"></a></span>
<span id="cb12-440"><a href="#cb12-440"></a><span class="fu">## Categorical class labels</span></span>
<span id="cb12-441"><a href="#cb12-441"></a>Usually the class label is a categorical value (i.e., a string). However, as reported before, Spark MLlib works only with numerical values and hence categorical class label values must be mapped to integer (and then double) values: processing and postprocessing steps are used to manage this transformation.</span>
<span id="cb12-442"><a href="#cb12-442"></a></span>
<span id="cb12-443"><a href="#cb12-443"></a>Consider the following input training data</span>
<span id="cb12-444"><a href="#cb12-444"></a></span>
<span id="cb12-445"><a href="#cb12-445"></a>|categoricalLabel|Attr1|Attr2|Attr3|</span>
<span id="cb12-446"><a href="#cb12-446"></a>|--|-|-|-|</span>
<span id="cb12-447"><a href="#cb12-447"></a>|Positive|$0.0$|$1.1$|$0.1$|</span>
<span id="cb12-448"><a href="#cb12-448"></a>|Negative|$2.0$|$1.0$|$-1.0$|</span>
<span id="cb12-449"><a href="#cb12-449"></a>|Negative|$2.0$|$1.3$|$1.0$|</span>
<span id="cb12-450"><a href="#cb12-450"></a></span>
<span id="cb12-451"><a href="#cb12-451"></a>A modified input DataFrame must be generated as input for the MLlib classification algorithms</span>
<span id="cb12-452"><a href="#cb12-452"></a></span>
<span id="cb12-453"><a href="#cb12-453"></a>|label|features|</span>
<span id="cb12-454"><a href="#cb12-454"></a>|-|--|</span>
<span id="cb12-455"><a href="#cb12-455"></a>|$1.0$|$<span class="co">[</span><span class="ot">0.0,1.1,0.1</span><span class="co">]</span>$|</span>
<span id="cb12-456"><a href="#cb12-456"></a>|$1.0$|$<span class="co">[</span><span class="ot">2.0,1.0,-1.0</span><span class="co">]</span>$|</span>
<span id="cb12-457"><a href="#cb12-457"></a>|$0.0$|$<span class="co">[</span><span class="ot">2.0,1.3,1.0</span><span class="co">]</span>$|</span>
<span id="cb12-458"><a href="#cb12-458"></a></span>
<span id="cb12-459"><a href="#cb12-459"></a>Notice that the categorical values of "categoricalLabel" (the class label column) must mapped to integer data values (finally casted to doubles).</span>
<span id="cb12-460"><a href="#cb12-460"></a></span>
<span id="cb12-461"><a href="#cb12-461"></a><span class="fu">### `StringIndexer` and `IndexToString`</span></span>
<span id="cb12-462"><a href="#cb12-462"></a>The Estimator <span class="in">`StringIndexer`</span> and the Transformer <span class="in">`IndexToString`</span> support the transformation of categorical class label into numerical one and vice versa:</span>
<span id="cb12-463"><a href="#cb12-463"></a></span>
<span id="cb12-464"><a href="#cb12-464"></a><span class="ss">- </span><span class="in">`StringIndexer`</span> maps each categorical value of the class label to an integer (then casted to a double);</span>
<span id="cb12-465"><a href="#cb12-465"></a><span class="ss">- </span><span class="in">`IndexToString`</span> is used to perform the opposite operation.</span>
<span id="cb12-466"><a href="#cb12-466"></a></span>
<span id="cb12-467"><a href="#cb12-467"></a>All in all, these are the main steps</span>
<span id="cb12-468"><a href="#cb12-468"></a></span>
<span id="cb12-469"><a href="#cb12-469"></a><span class="ss">1. </span>Use <span class="in">`StringIndexer`</span> to extend the input DataFrame with a new column, called "label", containing the numerical representation of the class label column;</span>
<span id="cb12-470"><a href="#cb12-470"></a><span class="ss">2. </span>Create a column, called "features", of type vector containing the predictive features;</span>
<span id="cb12-471"><a href="#cb12-471"></a><span class="ss">3. </span>Infer a classification model by using a classification algorithm (e.g., Decision Tree, Logistic regression);</span>
<span id="cb12-472"><a href="#cb12-472"></a><span class="ss">4. </span>Apply the model on a set of unlabeled data to predict their numerical class label;</span>
<span id="cb12-473"><a href="#cb12-473"></a><span class="ss">5. </span>Use <span class="in">`IndexToString`</span> to convert the predicted numerical class label values to the original categorical values.</span>
<span id="cb12-474"><a href="#cb12-474"></a></span>
<span id="cb12-475"><a href="#cb12-475"></a>Notice that the model is built by considering only the values of features and label. All the other columns are not considered by the classification algorithm during the generation of the prediction model.</span>
<span id="cb12-476"><a href="#cb12-476"></a></span>
<span id="cb12-477"><a href="#cb12-477"></a><span class="fu">#### Training data</span></span>
<span id="cb12-478"><a href="#cb12-478"></a>Given the following input training file</span>
<span id="cb12-479"><a href="#cb12-479"></a></span>
<span id="cb12-480"><a href="#cb12-480"></a><span class="in">```</span></span>
<span id="cb12-481"><a href="#cb12-481"></a><span class="in">categoricalLabel,attr1,attr2,attr3</span></span>
<span id="cb12-482"><a href="#cb12-482"></a><span class="in">Positive,0.0,1.1,0.1</span></span>
<span id="cb12-483"><a href="#cb12-483"></a><span class="in">Negative,2.0,1.0,-1.0</span></span>
<span id="cb12-484"><a href="#cb12-484"></a><span class="in">Negative,2.0,1.3,1.0</span></span>
<span id="cb12-485"><a href="#cb12-485"></a><span class="in">```</span></span>
<span id="cb12-486"><a href="#cb12-486"></a></span>
<span id="cb12-487"><a href="#cb12-487"></a>The initial training DataFrame will be</span>
<span id="cb12-488"><a href="#cb12-488"></a></span>
<span id="cb12-489"><a href="#cb12-489"></a>|categoricalLabel|features|</span>
<span id="cb12-490"><a href="#cb12-490"></a>|-|-|</span>
<span id="cb12-491"><a href="#cb12-491"></a>|Positive|$<span class="co">[</span><span class="ot">0.0,1.1,0.1</span><span class="co">]</span>$|</span>
<span id="cb12-492"><a href="#cb12-492"></a>|Negative|$<span class="co">[</span><span class="ot">2.0,1.0,-1.0</span><span class="co">]</span>$|</span>
<span id="cb12-493"><a href="#cb12-493"></a>|Negative|$<span class="co">[</span><span class="ot">2.0,1.3,1.0</span><span class="co">]</span>$|</span>
<span id="cb12-494"><a href="#cb12-494"></a></span>
<span id="cb12-495"><a href="#cb12-495"></a><span class="ss">- </span>The type of "categoricalLabel" is String</span>
<span id="cb12-496"><a href="#cb12-496"></a><span class="ss">- </span>The type of "features" is Vector</span>
<span id="cb12-497"><a href="#cb12-497"></a></span>
<span id="cb12-498"><a href="#cb12-498"></a>After applying <span class="in">`StringIndexer`</span>, the training DataFrame will be</span>
<span id="cb12-499"><a href="#cb12-499"></a></span>
<span id="cb12-500"><a href="#cb12-500"></a>|categoricalLabel|features|label!</span>
<span id="cb12-501"><a href="#cb12-501"></a>|-|-|-|</span>
<span id="cb12-502"><a href="#cb12-502"></a>|Positive|$<span class="co">[</span><span class="ot">0.0,1.1,0.1</span><span class="co">]</span>$|$1.0$|</span>
<span id="cb12-503"><a href="#cb12-503"></a>|Negative|$<span class="co">[</span><span class="ot">2.0,1.0,-1.0</span><span class="co">]</span>$|$0.0$|</span>
<span id="cb12-504"><a href="#cb12-504"></a>|Negative|$<span class="co">[</span><span class="ot">2.0,1.3,1.0</span><span class="co">]</span>$|$0.0$|</span>
<span id="cb12-505"><a href="#cb12-505"></a></span>
<span id="cb12-506"><a href="#cb12-506"></a>"label" contains the mapping generated by <span class="in">`StringIndexer`</span>:</span>
<span id="cb12-507"><a href="#cb12-507"></a></span>
<span id="cb12-508"><a href="#cb12-508"></a><span class="ss">- </span>"Positive": $1.0$</span>
<span id="cb12-509"><a href="#cb12-509"></a><span class="ss">- </span>"Negative": $0.0$</span>
<span id="cb12-510"><a href="#cb12-510"></a></span>
<span id="cb12-511"><a href="#cb12-511"></a><span class="fu">#### Unalabeled data</span></span>
<span id="cb12-512"><a href="#cb12-512"></a>Given the input unlabeled data file</span>
<span id="cb12-513"><a href="#cb12-513"></a></span>
<span id="cb12-514"><a href="#cb12-514"></a><span class="in">```</span></span>
<span id="cb12-515"><a href="#cb12-515"></a><span class="in">categoricalLabel,attr1,attr2,attr3</span></span>
<span id="cb12-516"><a href="#cb12-516"></a><span class="in">,-1.0,1.5,1.3</span></span>
<span id="cb12-517"><a href="#cb12-517"></a><span class="in">,3.0,2.0,-0.1</span></span>
<span id="cb12-518"><a href="#cb12-518"></a><span class="in">,0.0,2.2,-1.5</span></span>
<span id="cb12-519"><a href="#cb12-519"></a><span class="in">```</span></span>
<span id="cb12-520"><a href="#cb12-520"></a></span>
<span id="cb12-521"><a href="#cb12-521"></a>The initial unlabeled DataFrame will be</span>
<span id="cb12-522"><a href="#cb12-522"></a></span>
<span id="cb12-523"><a href="#cb12-523"></a>|categoricalLabel|features|</span>
<span id="cb12-524"><a href="#cb12-524"></a>|-|-|</span>
<span id="cb12-525"><a href="#cb12-525"></a>|null|$<span class="co">[</span><span class="ot">-1.0,1.5,1.3</span><span class="co">]</span>$|</span>
<span id="cb12-526"><a href="#cb12-526"></a>|null|$<span class="co">[</span><span class="ot">3.0,2.0,-0.1</span><span class="co">]</span>$|</span>
<span id="cb12-527"><a href="#cb12-527"></a>|null|$<span class="co">[</span><span class="ot">0.0,2.2,-1.5</span><span class="co">]</span>$|</span>
<span id="cb12-528"><a href="#cb12-528"></a></span>
<span id="cb12-529"><a href="#cb12-529"></a>After performing the prediction, and applying <span class="in">`IndexToString`</span>, the output DataFrame will be</span>
<span id="cb12-530"><a href="#cb12-530"></a></span>
<span id="cb12-531"><a href="#cb12-531"></a>|categoricalLabel|features|label|prediction|predictedLabel|...|</span>
<span id="cb12-532"><a href="#cb12-532"></a>|-|-|-|-|-|-|</span>
<span id="cb12-533"><a href="#cb12-533"></a>|...|$<span class="co">[</span><span class="ot">-1.0,1.5,1.3</span><span class="co">]</span>$|...|$1.0$|Positive||</span>
<span id="cb12-534"><a href="#cb12-534"></a>|...|$<span class="co">[</span><span class="ot">3.0,2.0,-0.1</span><span class="co">]</span>$|...|$0.0$|Negative||</span>
<span id="cb12-535"><a href="#cb12-535"></a>|...|$<span class="co">[</span><span class="ot">0.0,2.2,-1.5</span><span class="co">]</span>$|...|$1.0$|Negative||</span>
<span id="cb12-536"><a href="#cb12-536"></a></span>
<span id="cb12-537"><a href="#cb12-537"></a><span class="ss">- </span>"prediction" contains the predicted label, expressed as a number</span>
<span id="cb12-538"><a href="#cb12-538"></a><span class="ss">- </span>"predictedLabel" contains the predicted label, expressed as a category (original name)</span>
<span id="cb12-539"><a href="#cb12-539"></a></span>
<span id="cb12-540"><a href="#cb12-540"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-541"><a href="#cb12-541"></a><span class="fu">### Example</span></span>
<span id="cb12-542"><a href="#cb12-542"></a>In this example, the input training data is stored in a text file that contains one record/data point per line, and the records/data points are structured data with a fixed number of attributes (four)</span>
<span id="cb12-543"><a href="#cb12-543"></a></span>
<span id="cb12-544"><a href="#cb12-544"></a><span class="ss">- </span>One attribute is the class label ("categoricalLabel"): this is a categorical attribute that can assume two values, "Positive" or "Negative";</span>
<span id="cb12-545"><a href="#cb12-545"></a><span class="ss">- </span>The other three attributes ("attr1", "attr2", "attr3") are the predictive attributes that are used to predict the value of the class label.</span>
<span id="cb12-546"><a href="#cb12-546"></a></span>
<span id="cb12-547"><a href="#cb12-547"></a>The input file has the header line.</span>
<span id="cb12-548"><a href="#cb12-548"></a></span>
<span id="cb12-549"><a href="#cb12-549"></a>The file containing the unlabeled data has the same format of the training data file, however the first column is empty because the class label is unknown.</span>
<span id="cb12-550"><a href="#cb12-550"></a></span>
<span id="cb12-551"><a href="#cb12-551"></a>The goal is to predict the class label value of each unlabeled data by applying the classification model that has been inferred on the training data.</span>
<span id="cb12-552"><a href="#cb12-552"></a></span>
<span id="cb12-553"><a href="#cb12-553"></a><span class="in">```python</span></span>
<span id="cb12-554"><a href="#cb12-554"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-555"><a href="#cb12-555"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-556"><a href="#cb12-556"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> StringIndexer</span>
<span id="cb12-557"><a href="#cb12-557"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> IndexToString</span>
<span id="cb12-558"><a href="#cb12-558"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb12-559"><a href="#cb12-559"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb12-560"><a href="#cb12-560"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb12-561"><a href="#cb12-561"></a></span>
<span id="cb12-562"><a href="#cb12-562"></a><span class="co">## input and output folders</span></span>
<span id="cb12-563"><a href="#cb12-563"></a>trainingData <span class="op">=</span> <span class="st">"ex_dataCategorical/trainingData.csv"</span></span>
<span id="cb12-564"><a href="#cb12-564"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_dataCategorical/unlabeledData.csv"</span></span>
<span id="cb12-565"><a href="#cb12-565"></a>outputPath <span class="op">=</span> <span class="st">"predictionsDTCategoricalPipeline/"</span></span>
<span id="cb12-566"><a href="#cb12-566"></a></span>
<span id="cb12-567"><a href="#cb12-567"></a><span class="co">## *************************</span></span>
<span id="cb12-568"><a href="#cb12-568"></a><span class="co">## Training step</span></span>
<span id="cb12-569"><a href="#cb12-569"></a><span class="co">## *************************</span></span>
<span id="cb12-570"><a href="#cb12-570"></a></span>
<span id="cb12-571"><a href="#cb12-571"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb12-572"><a href="#cb12-572"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-573"><a href="#cb12-573"></a>trainingData <span class="op">=</span> spark.read.load(trainingData,<span class="op">\</span></span>
<span id="cb12-574"><a href="#cb12-574"></a><span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>, header<span class="op">=</span><span class="va">True</span>, inferSchema<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-575"><a href="#cb12-575"></a></span>
<span id="cb12-576"><a href="#cb12-576"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb12-577"><a href="#cb12-577"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb12-578"><a href="#cb12-578"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb12-579"><a href="#cb12-579"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>,<span class="st">"attr2"</span>,<span class="st">"attr3"</span>],</span>
<span id="cb12-580"><a href="#cb12-580"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb12-581"><a href="#cb12-581"></a>)</span>
<span id="cb12-582"><a href="#cb12-582"></a></span>
<span id="cb12-583"><a href="#cb12-583"></a><span class="co">## The StringIndexer Estimator is used to map each class label</span></span>
<span id="cb12-584"><a href="#cb12-584"></a><span class="co">## value to an integer value (casted to a double).</span></span>
<span id="cb12-585"><a href="#cb12-585"></a><span class="co">## A new attribute called label is generated by applying</span></span>
<span id="cb12-586"><a href="#cb12-586"></a><span class="co">## transforming the content of the categoricalLabel attribute.</span></span>
<span id="cb12-587"><a href="#cb12-587"></a>labelIndexer <span class="op">=</span> StringIndexer( <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-588"><a href="#cb12-588"></a>    inputCol<span class="op">=</span><span class="st">"categoricalLabel"</span></span>
<span id="cb12-589"><a href="#cb12-589"></a>    outputCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb12-590"><a href="#cb12-590"></a>    handleInvalid<span class="op">=</span><span class="st">"keep"</span></span>
<span id="cb12-591"><a href="#cb12-591"></a>).fit(trainingData) </span>
<span id="cb12-592"><a href="#cb12-592"></a></span>
<span id="cb12-593"><a href="#cb12-593"></a><span class="co">## Create a DecisionTreeClassifier object.</span></span>
<span id="cb12-594"><a href="#cb12-594"></a><span class="co">## DecisionTreeClassifier is an Estimator that is used to</span></span>
<span id="cb12-595"><a href="#cb12-595"></a><span class="co">## create a classification model based on decision trees.</span></span>
<span id="cb12-596"><a href="#cb12-596"></a>dt <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb12-597"><a href="#cb12-597"></a></span>
<span id="cb12-598"><a href="#cb12-598"></a><span class="co">## Set the values of the parameters of the Decision Tree</span></span>
<span id="cb12-599"><a href="#cb12-599"></a><span class="co">## For example set the measure that is used to decide if a</span></span>
<span id="cb12-600"><a href="#cb12-600"></a><span class="co">## node must be split.</span></span>
<span id="cb12-601"><a href="#cb12-601"></a><span class="co">## In this case we set gini index</span></span>
<span id="cb12-602"><a href="#cb12-602"></a>dt.setImpurity(<span class="st">"gini"</span>)</span>
<span id="cb12-603"><a href="#cb12-603"></a></span>
<span id="cb12-604"><a href="#cb12-604"></a><span class="co">## At the end of the pipeline we must convert indexed labels back</span></span>
<span id="cb12-605"><a href="#cb12-605"></a><span class="co">## to original labels (from numerical to string).</span></span>
<span id="cb12-606"><a href="#cb12-606"></a><span class="co">## The content of the prediction attribute is the index of the predicted class</span></span>
<span id="cb12-607"><a href="#cb12-607"></a><span class="co">## The original name of the predicted class is stored in the predictedLabel</span></span>
<span id="cb12-608"><a href="#cb12-608"></a><span class="co">## attribute.</span></span>
<span id="cb12-609"><a href="#cb12-609"></a><span class="co">## IndexToString creates a new column (called predictedLabel in</span></span>
<span id="cb12-610"><a href="#cb12-610"></a><span class="co">## this example) that is based on the content of the prediction column.</span></span>
<span id="cb12-611"><a href="#cb12-611"></a><span class="co">## prediction is a double while predictedLabel is a string</span></span>
<span id="cb12-612"><a href="#cb12-612"></a>labelConverter <span class="op">=</span> IndexToString( <span class="co"># &lt;2&gt;</span></span>
<span id="cb12-613"><a href="#cb12-613"></a>    inputCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="cb12-614"><a href="#cb12-614"></a>    outputCol<span class="op">=</span><span class="st">"predictedLabel"</span>,</span>
<span id="cb12-615"><a href="#cb12-615"></a>    labels<span class="op">=</span>labelIndexer.labels</span>
<span id="cb12-616"><a href="#cb12-616"></a>)</span>
<span id="cb12-617"><a href="#cb12-617"></a></span>
<span id="cb12-618"><a href="#cb12-618"></a><span class="co">## Define a pipeline that is used to create the decision tree</span></span>
<span id="cb12-619"><a href="#cb12-619"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="cb12-620"><a href="#cb12-620"></a><span class="co">## the preprocessing and postprocessing steps</span></span>
<span id="cb12-621"><a href="#cb12-621"></a>pipeline <span class="op">=</span> Pipeline() \ </span>
<span id="cb12-622"><a href="#cb12-622"></a>    .setStages([assembler, labelIndexer, dt, labelConverter]) <span class="co"># &lt;3&gt;</span></span>
<span id="cb12-623"><a href="#cb12-623"></a></span>
<span id="cb12-624"><a href="#cb12-624"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="cb12-625"><a href="#cb12-625"></a><span class="co">## classification model</span></span>
<span id="cb12-626"><a href="#cb12-626"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="cb12-627"><a href="#cb12-627"></a></span>
<span id="cb12-628"><a href="#cb12-628"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="cb12-629"><a href="#cb12-629"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb12-630"><a href="#cb12-630"></a></span>
<span id="cb12-631"><a href="#cb12-631"></a><span class="co">## *************************</span></span>
<span id="cb12-632"><a href="#cb12-632"></a><span class="co">## Prediction step</span></span>
<span id="cb12-633"><a href="#cb12-633"></a><span class="co">## *************************</span></span>
<span id="cb12-634"><a href="#cb12-634"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb12-635"><a href="#cb12-635"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb12-636"><a href="#cb12-636"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-637"><a href="#cb12-637"></a>    unlabeledData,</span>
<span id="cb12-638"><a href="#cb12-638"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-639"><a href="#cb12-639"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-640"><a href="#cb12-640"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-641"><a href="#cb12-641"></a>)</span>
<span id="cb12-642"><a href="#cb12-642"></a></span>
<span id="cb12-643"><a href="#cb12-643"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="cb12-644"><a href="#cb12-644"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="cb12-645"><a href="#cb12-645"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="cb12-646"><a href="#cb12-646"></a><span class="co">## also the assembler is executed</span></span>
<span id="cb12-647"><a href="#cb12-647"></a>predictions <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="cb12-648"><a href="#cb12-648"></a></span>
<span id="cb12-649"><a href="#cb12-649"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb12-650"><a href="#cb12-650"></a><span class="co">## - attr1: double (nullable = true)</span></span>
<span id="cb12-651"><a href="#cb12-651"></a><span class="co">## - attr2: double (nullable = true)</span></span>
<span id="cb12-652"><a href="#cb12-652"></a><span class="co">## - attr3: double (nullable = true)</span></span>
<span id="cb12-653"><a href="#cb12-653"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="cb12-654"><a href="#cb12-654"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="cb12-655"><a href="#cb12-655"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="cb12-656"><a href="#cb12-656"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the</span></span>
<span id="cb12-657"><a href="#cb12-657"></a><span class="co">##   current record belongs to the i-th class</span></span>
<span id="cb12-658"><a href="#cb12-658"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="cb12-659"><a href="#cb12-659"></a><span class="co">## - predictedLabel: string (nullable = true)</span></span>
<span id="cb12-660"><a href="#cb12-660"></a></span>
<span id="cb12-661"><a href="#cb12-661"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="cb12-662"><a href="#cb12-662"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="cb12-663"><a href="#cb12-663"></a>predictions <span class="op">=</span> predictionsDF <span class="op">\</span></span>
<span id="cb12-664"><a href="#cb12-664"></a>    .select(<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>, <span class="st">"predictedLabel"</span>) <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-665"><a href="#cb12-665"></a></span>
<span id="cb12-666"><a href="#cb12-666"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb12-667"><a href="#cb12-667"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb12-668"><a href="#cb12-668"></a><span class="in">```</span></span>
<span id="cb12-669"><a href="#cb12-669"></a><span class="ss">1. </span>This <span class="in">`StringIndexer`</span> estimator is used to infer a transformer that maps the categorical values of column "categoricalLabel" to a set of integer values stored in the new column called "label". The list of valid label values are extracted from the training data.</span>
<span id="cb12-670"><a href="#cb12-670"></a><span class="ss">2. </span>This <span class="in">`IndexToString`</span> component is used to remap the numerical predictions available in the "prediction" column to the original categorical values that are stored in the new column called "predictedLabel". The mapping of integer to original string value is the one of "labelIndexer".</span>
<span id="cb12-671"><a href="#cb12-671"></a><span class="ss">3. </span>This <span class="in">`Pipeline`</span> is composed of four steps.</span>
<span id="cb12-672"><a href="#cb12-672"></a><span class="ss">4. </span>The "predictedLabel" field is the column containing the predicted categorical class label for the unlabeled data. </span>
<span id="cb12-673"><a href="#cb12-673"></a></span>
<span id="cb12-674"><a href="#cb12-674"></a>:::</span>
<span id="cb12-675"><a href="#cb12-675"></a></span>
<span id="cb12-676"><a href="#cb12-676"></a><span class="fu">## Textual data management and classification</span></span>
<span id="cb12-677"><a href="#cb12-677"></a>The following paragraphs show how to</span>
<span id="cb12-678"><a href="#cb12-678"></a></span>
<span id="cb12-679"><a href="#cb12-679"></a><span class="ss">- </span>Create a classification model based on the logistic regression algorithm for textual documents: a set of specific preprocessing estimators and transformers are used to preprocess textual data.</span>
<span id="cb12-680"><a href="#cb12-680"></a><span class="ss">- </span>Apply the model to new textual documents</span>
<span id="cb12-681"><a href="#cb12-681"></a></span>
<span id="cb12-682"><a href="#cb12-682"></a>The input training dataset represents a textual document collection, where each line contains one document and its class</span>
<span id="cb12-683"><a href="#cb12-683"></a></span>
<span id="cb12-684"><a href="#cb12-684"></a><span class="ss">- </span>The class label</span>
<span id="cb12-685"><a href="#cb12-685"></a><span class="ss">- </span>A list of words (the text of the document)</span>
<span id="cb12-686"><a href="#cb12-686"></a></span>
<span id="cb12-687"><a href="#cb12-687"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-688"><a href="#cb12-688"></a><span class="fu">### Example</span></span>
<span id="cb12-689"><a href="#cb12-689"></a>Given the following example training file </span>
<span id="cb12-690"><a href="#cb12-690"></a></span>
<span id="cb12-691"><a href="#cb12-691"></a><span class="in">```</span></span>
<span id="cb12-692"><a href="#cb12-692"></a><span class="in">Label,Text</span></span>
<span id="cb12-693"><a href="#cb12-693"></a><span class="in">1,The Spark system is based on scala</span></span>
<span id="cb12-694"><a href="#cb12-694"></a><span class="in">1,Spark is a new distributed system</span></span>
<span id="cb12-695"><a href="#cb12-695"></a><span class="in">0,Turin is a beautiful city</span></span>
<span id="cb12-696"><a href="#cb12-696"></a><span class="in">0,Turin is in the north of Italy</span></span>
<span id="cb12-697"><a href="#cb12-697"></a><span class="in">```</span></span>
<span id="cb12-698"><a href="#cb12-698"></a></span>
<span id="cb12-699"><a href="#cb12-699"></a>It contains four textual documents, and each line contains two attributes, that are the class label (first attribute) and the text of the document (second attribute).</span>
<span id="cb12-700"><a href="#cb12-700"></a></span>
<span id="cb12-701"><a href="#cb12-701"></a>The input data before preprocessing, represented as a DataFrame, is </span>
<span id="cb12-702"><a href="#cb12-702"></a></span>
<span id="cb12-703"><a href="#cb12-703"></a>|Label|Text|</span>
<span id="cb12-704"><a href="#cb12-704"></a>|-|-|</span>
<span id="cb12-705"><a href="#cb12-705"></a>|1|The Spark system is based on scala|</span>
<span id="cb12-706"><a href="#cb12-706"></a>|1|Spark is a new distributed system|</span>
<span id="cb12-707"><a href="#cb12-707"></a>|0|Turin is a beautiful city|</span>
<span id="cb12-708"><a href="#cb12-708"></a>|0|Turin is in the north of Italy|</span>
<span id="cb12-709"><a href="#cb12-709"></a></span>
<span id="cb12-710"><a href="#cb12-710"></a>:::</span>
<span id="cb12-711"><a href="#cb12-711"></a></span>
<span id="cb12-712"><a href="#cb12-712"></a>A set of preprocessing steps must be applied on the textual attribute before generating a classification model.</span>
<span id="cb12-713"><a href="#cb12-713"></a></span>
<span id="cb12-714"><a href="#cb12-714"></a><span class="ss">1. </span>Since Spark ML algorithms work only on "Tables" and double values, the textual part of the input data must be translated in a set of attributes to represent the data as a table: usually a table with an attribute for each word is generated.</span>
<span id="cb12-715"><a href="#cb12-715"></a><span class="ss">2. </span>Many words are useless (e.g., conjunctions): stopwords are usually removed. In general, </span>
<span id="cb12-716"><a href="#cb12-716"></a><span class="ss">    - </span>the words appearing in almost all documents are not characterizing the data, and so they are not very important for the classification problem;</span>
<span id="cb12-717"><a href="#cb12-717"></a><span class="ss">    - </span>the words appearing in few documents allow to distinguish the content of those documents (and hence the class label) with respect to the others, and so they are very important for the classification problem.</span>
<span id="cb12-718"><a href="#cb12-718"></a><span class="ss">3. </span>Traditionally a weight, based on the TF-IDF measure, is used to assign a difference importance to the words based on their frequency in the collection.</span>
<span id="cb12-719"><a href="#cb12-719"></a></span>
<span id="cb12-720"><a href="#cb12-720"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-721"><a href="#cb12-721"></a><span class="fu">### Example</span></span>
<span id="cb12-722"><a href="#cb12-722"></a>Input data after the preprocessing transformations (tokenization, stopword removal, TF-IDF computation)</span>
<span id="cb12-723"><a href="#cb12-723"></a></span>
<span id="cb12-724"><a href="#cb12-724"></a>|Label|Spark|system|scala|...|</span>
<span id="cb12-725"><a href="#cb12-725"></a>|-|-|-|-|-|</span>
<span id="cb12-726"><a href="#cb12-726"></a>|1|$0.5$|$0.3$|$0.75$|...|</span>
<span id="cb12-727"><a href="#cb12-727"></a>|1|$0.5$|$0.3$|$0$|...|</span>
<span id="cb12-728"><a href="#cb12-728"></a>|0|$0$|$0$|$0$|...|</span>
<span id="cb12-729"><a href="#cb12-729"></a>|0|$0$|$0$|$0$|...|</span>
<span id="cb12-730"><a href="#cb12-730"></a></span>
<span id="cb12-731"><a href="#cb12-731"></a>:::</span>
<span id="cb12-732"><a href="#cb12-732"></a></span>
<span id="cb12-733"><a href="#cb12-733"></a>The DataFrame associated with the input data after the preprocessing transformations must contain, as usual, the columns</span>
<span id="cb12-734"><a href="#cb12-734"></a></span>
<span id="cb12-735"><a href="#cb12-735"></a><span class="ss">- </span>label: class label value</span>
<span id="cb12-736"><a href="#cb12-736"></a><span class="ss">- </span>features: the preprocessed version of the input text</span>
<span id="cb12-737"><a href="#cb12-737"></a></span>
<span id="cb12-738"><a href="#cb12-738"></a>There are also some other intermediate columns, related to applied transformations, but they are not considered by the classification algorithm.</span>
<span id="cb12-739"><a href="#cb12-739"></a></span>
<span id="cb12-740"><a href="#cb12-740"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-741"><a href="#cb12-741"></a><span class="fu">### Example</span></span>
<span id="cb12-742"><a href="#cb12-742"></a>The DataFrame associated with the input data after the preprocessing transformations</span>
<span id="cb12-743"><a href="#cb12-743"></a></span>
<span id="cb12-744"><a href="#cb12-744"></a>|label|features|text|...|...|</span>
<span id="cb12-745"><a href="#cb12-745"></a>|-|-|--|-|-|</span>
<span id="cb12-746"><a href="#cb12-746"></a>|1|$<span class="co">[</span><span class="ot">0.5,0.3,0.75,...</span><span class="co">]</span>$|The Spark system is based on scala|...|...|</span>
<span id="cb12-747"><a href="#cb12-747"></a>|1|$<span class="co">[</span><span class="ot">0.5,0.3,0,...</span><span class="co">]</span>$|Spark is a new distributed system|...|...|</span>
<span id="cb12-748"><a href="#cb12-748"></a>|0|$<span class="co">[</span><span class="ot">0,0,0,...</span><span class="co">]</span>$|Turin is a beautiful city|...|...|</span>
<span id="cb12-749"><a href="#cb12-749"></a>|0|$<span class="co">[</span><span class="ot">0,0,0,...</span><span class="co">]</span>$|Turin is in the north of Italy|...|...|</span>
<span id="cb12-750"><a href="#cb12-750"></a></span>
<span id="cb12-751"><a href="#cb12-751"></a>Only "label" and "features" are considered by the classification algorithm.</span>
<span id="cb12-752"><a href="#cb12-752"></a></span>
<span id="cb12-753"><a href="#cb12-753"></a>:::</span>
<span id="cb12-754"><a href="#cb12-754"></a></span>
<span id="cb12-755"><a href="#cb12-755"></a>In the following solution we will use a set of new Transformers to prepare input data</span>
<span id="cb12-756"><a href="#cb12-756"></a></span>
<span id="cb12-757"><a href="#cb12-757"></a><span class="ss">- </span><span class="in">`Tokenizer`</span>: to split the input text in words;</span>
<span id="cb12-758"><a href="#cb12-758"></a><span class="ss">- </span><span class="in">`StopWordsRemover`</span>: to remove stopwords;</span>
<span id="cb12-759"><a href="#cb12-759"></a><span class="ss">- </span><span class="in">`HashingTF`</span>: to compute the (approximate) term frequency of each input term;</span>
<span id="cb12-760"><a href="#cb12-760"></a><span class="ss">- </span><span class="in">`IDF`</span>: to compute the inverse document frequency of each input word.</span>
<span id="cb12-761"><a href="#cb12-761"></a></span>
<span id="cb12-762"><a href="#cb12-762"></a>The input data (training and unlabeled data) are stored in input csv files. Each line contains two attributes:</span>
<span id="cb12-763"><a href="#cb12-763"></a></span>
<span id="cb12-764"><a href="#cb12-764"></a><span class="ss">- </span>The class label (label)</span>
<span id="cb12-765"><a href="#cb12-765"></a><span class="ss">- </span>The text of the document (text)</span>
<span id="cb12-766"><a href="#cb12-766"></a></span>
<span id="cb12-767"><a href="#cb12-767"></a>We infer a linear regression model on the training data and apply the model on the unlabeled data.</span>
<span id="cb12-768"><a href="#cb12-768"></a></span>
<span id="cb12-769"><a href="#cb12-769"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-770"><a href="#cb12-770"></a><span class="fu">### Example</span></span>
<span id="cb12-771"><a href="#cb12-771"></a></span>
<span id="cb12-772"><a href="#cb12-772"></a><span class="in">```python</span></span>
<span id="cb12-773"><a href="#cb12-773"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-774"><a href="#cb12-774"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-775"><a href="#cb12-775"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> Tokenizer</span>
<span id="cb12-776"><a href="#cb12-776"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> StopWordsRemover</span>
<span id="cb12-777"><a href="#cb12-777"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> HashingTF</span>
<span id="cb12-778"><a href="#cb12-778"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> IDF</span>
<span id="cb12-779"><a href="#cb12-779"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb12-780"><a href="#cb12-780"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb12-781"><a href="#cb12-781"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb12-782"><a href="#cb12-782"></a></span>
<span id="cb12-783"><a href="#cb12-783"></a><span class="co">## input and output folders</span></span>
<span id="cb12-784"><a href="#cb12-784"></a>trainingData <span class="op">=</span> <span class="st">"ex_dataText/trainingData.csv"</span></span>
<span id="cb12-785"><a href="#cb12-785"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_dataText/unlabeledData.csv"</span></span>
<span id="cb12-786"><a href="#cb12-786"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLRPipelineText/"</span></span>
<span id="cb12-787"><a href="#cb12-787"></a></span>
<span id="cb12-788"><a href="#cb12-788"></a><span class="co">## *************************</span></span>
<span id="cb12-789"><a href="#cb12-789"></a><span class="co">## Training step</span></span>
<span id="cb12-790"><a href="#cb12-790"></a><span class="co">## *************************</span></span>
<span id="cb12-791"><a href="#cb12-791"></a><span class="co">## Create a DataFrame from trainingData.csv</span></span>
<span id="cb12-792"><a href="#cb12-792"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-793"><a href="#cb12-793"></a>trainingData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-794"><a href="#cb12-794"></a>    trainingData,</span>
<span id="cb12-795"><a href="#cb12-795"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-796"><a href="#cb12-796"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-797"><a href="#cb12-797"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-798"><a href="#cb12-798"></a>)</span>
<span id="cb12-799"><a href="#cb12-799"></a></span>
<span id="cb12-800"><a href="#cb12-800"></a><span class="co">## Configure an ML pipeline, which consists of five stages:</span></span>
<span id="cb12-801"><a href="#cb12-801"></a><span class="co">## tokenizer -&gt; split sentences in set of words</span></span>
<span id="cb12-802"><a href="#cb12-802"></a><span class="co">## remover -&gt; remove stopwords</span></span>
<span id="cb12-803"><a href="#cb12-803"></a><span class="co">## hashingTF -&gt; map set of words to a fixed-length feature vectors (each</span></span>
<span id="cb12-804"><a href="#cb12-804"></a><span class="co">## word becomes a feature and the value of the feature is the frequency of</span></span>
<span id="cb12-805"><a href="#cb12-805"></a><span class="co">## the word in the sentence)</span></span>
<span id="cb12-806"><a href="#cb12-806"></a><span class="co">## idf -&gt; compute the idf component of the TF-IDF measure</span></span>
<span id="cb12-807"><a href="#cb12-807"></a><span class="co">## lr -&gt; logistic regression classification algorithm</span></span>
<span id="cb12-808"><a href="#cb12-808"></a><span class="co">## The Tokenizer splits each sentence in a set of words.</span></span>
<span id="cb12-809"><a href="#cb12-809"></a><span class="co">## It analyzes the content of column "text" and adds the</span></span>
<span id="cb12-810"><a href="#cb12-810"></a><span class="co">## new column "words" in the returned DataFrame</span></span>
<span id="cb12-811"><a href="#cb12-811"></a>tokenizer <span class="op">=</span> Tokenizer() <span class="op">\</span></span>
<span id="cb12-812"><a href="#cb12-812"></a>    .setInputCol(<span class="st">"text"</span>) <span class="op">\</span></span>
<span id="cb12-813"><a href="#cb12-813"></a>    .setOutputCol(<span class="st">"words"</span>)</span>
<span id="cb12-814"><a href="#cb12-814"></a></span>
<span id="cb12-815"><a href="#cb12-815"></a><span class="co">## Remove stopwords.</span></span>
<span id="cb12-816"><a href="#cb12-816"></a><span class="co">## The StopWordsRemover component returns a new DataFrame with</span></span>
<span id="cb12-817"><a href="#cb12-817"></a><span class="co">## a new column called "filteredWords". "filteredWords" is generated</span></span>
<span id="cb12-818"><a href="#cb12-818"></a><span class="co">## by removing the stopwords from the content of column "words"</span></span>
<span id="cb12-819"><a href="#cb12-819"></a>remover <span class="op">=</span> StopWordsRemover() <span class="op">\</span></span>
<span id="cb12-820"><a href="#cb12-820"></a>    .setInputCol(<span class="st">"words"</span>) <span class="op">\</span></span>
<span id="cb12-821"><a href="#cb12-821"></a>    .setOutputCol(<span class="st">"filteredWords"</span>)</span>
<span id="cb12-822"><a href="#cb12-822"></a></span>
<span id="cb12-823"><a href="#cb12-823"></a><span class="co">## Map words to a features</span></span>
<span id="cb12-824"><a href="#cb12-824"></a><span class="co">## Each word in filteredWords must become a feature in a Vector object</span></span>
<span id="cb12-825"><a href="#cb12-825"></a><span class="co">## The HashingTF Transformer can be used to perform this operation.</span></span>
<span id="cb12-826"><a href="#cb12-826"></a><span class="co">## This operations is based on a hash function and can potentially</span></span>
<span id="cb12-827"><a href="#cb12-827"></a><span class="co">## map two different words to the same "feature". The number of conflicts</span></span>
<span id="cb12-828"><a href="#cb12-828"></a><span class="co">## in influenced by the value of the numFeatures parameter.</span></span>
<span id="cb12-829"><a href="#cb12-829"></a><span class="co">## The "feature" version of the words is stored in Column "rawFeatures".</span></span>
<span id="cb12-830"><a href="#cb12-830"></a><span class="co">## Each feature, for a document, contains the number of occurrences</span></span>
<span id="cb12-831"><a href="#cb12-831"></a><span class="co">## of that feature in the document (TF component of the TF-IDF measure)</span></span>
<span id="cb12-832"><a href="#cb12-832"></a>hashingTF <span class="op">=</span> HashingTF() <span class="op">\</span></span>
<span id="cb12-833"><a href="#cb12-833"></a>    .setNumFeatures(<span class="dv">1000</span>) <span class="op">\</span></span>
<span id="cb12-834"><a href="#cb12-834"></a>    .setInputCol(<span class="st">"filteredWords"</span>) <span class="op">\</span></span>
<span id="cb12-835"><a href="#cb12-835"></a>    .setOutputCol(<span class="st">"rawFeatures"</span>)</span>
<span id="cb12-836"><a href="#cb12-836"></a></span>
<span id="cb12-837"><a href="#cb12-837"></a><span class="co">## Apply the IDF transformation/computation.</span></span>
<span id="cb12-838"><a href="#cb12-838"></a><span class="co">## Update the weight associated with each feature by considering also the</span></span>
<span id="cb12-839"><a href="#cb12-839"></a><span class="co">## inverse document frequency component. The returned new column</span></span>
<span id="cb12-840"><a href="#cb12-840"></a><span class="co">## is called "features", that is the standard name for the column that</span></span>
<span id="cb12-841"><a href="#cb12-841"></a><span class="co">## contains the predictive features used to create a classification model</span></span>
<span id="cb12-842"><a href="#cb12-842"></a>idf <span class="op">=</span> IDF() <span class="op">\</span></span>
<span id="cb12-843"><a href="#cb12-843"></a>    .setInputCol(<span class="st">"rawFeatures"</span>) <span class="op">\</span></span>
<span id="cb12-844"><a href="#cb12-844"></a>    .setOutputCol(<span class="st">"features"</span>)</span>
<span id="cb12-845"><a href="#cb12-845"></a></span>
<span id="cb12-846"><a href="#cb12-846"></a><span class="co">## Create a classification model based on the logistic regression algorithm</span></span>
<span id="cb12-847"><a href="#cb12-847"></a><span class="co">## We can set the values of the parameters of the</span></span>
<span id="cb12-848"><a href="#cb12-848"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="cb12-849"><a href="#cb12-849"></a>lr <span class="op">=</span> LogisticRegression() <span class="op">\</span></span>
<span id="cb12-850"><a href="#cb12-850"></a>    .setMaxIter(<span class="dv">10</span>) <span class="op">\</span></span>
<span id="cb12-851"><a href="#cb12-851"></a>    .setRegParam(<span class="fl">0.01</span>)</span>
<span id="cb12-852"><a href="#cb12-852"></a></span>
<span id="cb12-853"><a href="#cb12-853"></a><span class="co">## Define the pipeline that is used to create the logistic regression</span></span>
<span id="cb12-854"><a href="#cb12-854"></a><span class="co">## model on the training data.</span></span>
<span id="cb12-855"><a href="#cb12-855"></a><span class="co">## In this case the pipeline is composed of five steps</span></span>
<span id="cb12-856"><a href="#cb12-856"></a><span class="co">## - text tokenizer</span></span>
<span id="cb12-857"><a href="#cb12-857"></a><span class="co">## - stopword removal</span></span>
<span id="cb12-858"><a href="#cb12-858"></a><span class="co">## - TF-IDF computation (performed in two steps)</span></span>
<span id="cb12-859"><a href="#cb12-859"></a><span class="co">## - Logistic regression model generation</span></span>
<span id="cb12-860"><a href="#cb12-860"></a>pipeline <span class="op">=</span> Pipeline()<span class="op">\</span></span>
<span id="cb12-861"><a href="#cb12-861"></a>    .setStages([tokenizer, remover, hashingTF, idf, lr])</span>
<span id="cb12-862"><a href="#cb12-862"></a></span>
<span id="cb12-863"><a href="#cb12-863"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="cb12-864"><a href="#cb12-864"></a><span class="co">## classification model</span></span>
<span id="cb12-865"><a href="#cb12-865"></a>classificationModel <span class="op">=</span> pipeline.fit(trainingData)</span>
<span id="cb12-866"><a href="#cb12-866"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="cb12-867"><a href="#cb12-867"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb12-868"><a href="#cb12-868"></a></span>
<span id="cb12-869"><a href="#cb12-869"></a><span class="co">## *************************</span></span>
<span id="cb12-870"><a href="#cb12-870"></a><span class="co">## Prediction step</span></span>
<span id="cb12-871"><a href="#cb12-871"></a><span class="co">## *************************</span></span>
<span id="cb12-872"><a href="#cb12-872"></a><span class="co">## Read unlabeled data</span></span>
<span id="cb12-873"><a href="#cb12-873"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb12-874"><a href="#cb12-874"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb12-875"><a href="#cb12-875"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-876"><a href="#cb12-876"></a>    unlabeledData,</span>
<span id="cb12-877"><a href="#cb12-877"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-878"><a href="#cb12-878"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-879"><a href="#cb12-879"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-880"><a href="#cb12-880"></a>)</span>
<span id="cb12-881"><a href="#cb12-881"></a></span>
<span id="cb12-882"><a href="#cb12-882"></a><span class="co">## Make predictions on unlabeled documents by using the</span></span>
<span id="cb12-883"><a href="#cb12-883"></a><span class="co">## Transformer.transform() method.</span></span>
<span id="cb12-884"><a href="#cb12-884"></a><span class="co">## The transform will only use the 'features' columns</span></span>
<span id="cb12-885"><a href="#cb12-885"></a>predictionsDF <span class="op">=</span> classificationModel.transform(unlabeledData)</span>
<span id="cb12-886"><a href="#cb12-886"></a></span>
<span id="cb12-887"><a href="#cb12-887"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb12-888"><a href="#cb12-888"></a><span class="co">## |-- label: string (nullable = true)</span></span>
<span id="cb12-889"><a href="#cb12-889"></a><span class="co">## |-- text: string (nullable = true)</span></span>
<span id="cb12-890"><a href="#cb12-890"></a><span class="co">## |-- words: array (nullable = true)</span></span>
<span id="cb12-891"><a href="#cb12-891"></a><span class="co">## | |-- element: string (containsNull = true)</span></span>
<span id="cb12-892"><a href="#cb12-892"></a><span class="co">## |-- filteredWords: array (nullable = true)</span></span>
<span id="cb12-893"><a href="#cb12-893"></a><span class="co">## | |-- element: string (containsNull = true)</span></span>
<span id="cb12-894"><a href="#cb12-894"></a><span class="co">## |-- rawFeatures: vector (nullable = true)</span></span>
<span id="cb12-895"><a href="#cb12-895"></a><span class="co">## |-- features: vector (nullable = true)</span></span>
<span id="cb12-896"><a href="#cb12-896"></a><span class="co">## |-- rawPrediction: vector (nullable = true)</span></span>
<span id="cb12-897"><a href="#cb12-897"></a><span class="co">## |-- probability: vector (nullable = true)</span></span>
<span id="cb12-898"><a href="#cb12-898"></a><span class="co">## |-- prediction: double (nullable = false)</span></span>
<span id="cb12-899"><a href="#cb12-899"></a></span>
<span id="cb12-900"><a href="#cb12-900"></a><span class="co">## Select only the original features (i.e., the value of the original text attribute) and</span></span>
<span id="cb12-901"><a href="#cb12-901"></a><span class="co">## the predicted class for each record</span></span>
<span id="cb12-902"><a href="#cb12-902"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"text"</span>, <span class="st">"prediction"</span>)</span>
<span id="cb12-903"><a href="#cb12-903"></a></span>
<span id="cb12-904"><a href="#cb12-904"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb12-905"><a href="#cb12-905"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb12-906"><a href="#cb12-906"></a><span class="in">```</span></span>
<span id="cb12-907"><a href="#cb12-907"></a></span>
<span id="cb12-908"><a href="#cb12-908"></a>:::</span>
<span id="cb12-909"><a href="#cb12-909"></a></span>
<span id="cb12-910"><a href="#cb12-910"></a><span class="fu">## Performance evaluation</span></span>
<span id="cb12-911"><a href="#cb12-911"></a>In order to test the goodness of algorithms there are some evaluators. The Evaluator can be</span>
<span id="cb12-912"><a href="#cb12-912"></a></span>
<span id="cb12-913"><a href="#cb12-913"></a><span class="ss">- </span>a <span class="in">`BinaryClassificationEvaluator`</span> for binary data</span>
<span id="cb12-914"><a href="#cb12-914"></a><span class="ss">- </span>a <span class="in">`MulticlassClassificationEvaluator`</span> for multiclass problems</span>
<span id="cb12-915"><a href="#cb12-915"></a></span>
<span id="cb12-916"><a href="#cb12-916"></a>Provided metrics are:</span>
<span id="cb12-917"><a href="#cb12-917"></a></span>
<span id="cb12-918"><a href="#cb12-918"></a><span class="ss">- </span>Accuracy</span>
<span id="cb12-919"><a href="#cb12-919"></a><span class="ss">- </span>Precision</span>
<span id="cb12-920"><a href="#cb12-920"></a><span class="ss">- </span>Recall</span>
<span id="cb12-921"><a href="#cb12-921"></a><span class="ss">- </span>F-measure</span>
<span id="cb12-922"><a href="#cb12-922"></a></span>
<span id="cb12-923"><a href="#cb12-923"></a>Use the <span class="in">`MulticlassClassificationEvaluator`</span> estimator from <span class="in">`pyspark.ml.evaluator`</span> on a DataFrame. The instantiated estimator has the method <span class="in">`.evaluate()`</span> that is applied on a DataFrame: it compares the predictions with the true label values, and the output is the double value of the computed performance metric.</span>
<span id="cb12-924"><a href="#cb12-924"></a></span>
<span id="cb12-925"><a href="#cb12-925"></a>The parameters of <span class="in">`MulticlassClassificationEvaluator`</span> are</span>
<span id="cb12-926"><a href="#cb12-926"></a></span>
<span id="cb12-927"><a href="#cb12-927"></a><span class="ss">- </span><span class="in">`metricName`</span>: type of metric to compute. It can assume the following values</span>
<span id="cb12-928"><a href="#cb12-928"></a><span class="ss">    - </span><span class="in">`"accuracy"`</span></span>
<span id="cb12-929"><a href="#cb12-929"></a><span class="ss">    - </span><span class="in">`"f1"`</span></span>
<span id="cb12-930"><a href="#cb12-930"></a><span class="ss">    - </span><span class="in">`"weightedPrecision"`</span></span>
<span id="cb12-931"><a href="#cb12-931"></a><span class="ss">    - </span><span class="in">`"weightedRecall"`</span></span>
<span id="cb12-932"><a href="#cb12-932"></a><span class="ss">- </span><span class="in">`labelCol`</span>: input column with the true label/class value</span>
<span id="cb12-933"><a href="#cb12-933"></a><span class="ss">- </span><span class="in">`predictionCol`</span>: input column with the predicted class/label value</span>
<span id="cb12-934"><a href="#cb12-934"></a></span>
<span id="cb12-935"><a href="#cb12-935"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-936"><a href="#cb12-936"></a><span class="fu">### Example</span></span>
<span id="cb12-937"><a href="#cb12-937"></a>In this example, the set of labeled data is read from a text file that contains one record/data point per line, and the records/data points are structured data with a fixed number of attributes (four)</span>
<span id="cb12-938"><a href="#cb12-938"></a></span>
<span id="cb12-939"><a href="#cb12-939"></a><span class="ss">- </span>One attribute is the class label ("label");</span>
<span id="cb12-940"><a href="#cb12-940"></a><span class="ss">- </span>The other three attributes ("attr1", "attr2", "attr3") are the predictive attributes that are used to predict the value of the class label.</span>
<span id="cb12-941"><a href="#cb12-941"></a></span>
<span id="cb12-942"><a href="#cb12-942"></a>All attributes are already double attributes, and the input file has the header line.</span>
<span id="cb12-943"><a href="#cb12-943"></a></span>
<span id="cb12-944"><a href="#cb12-944"></a>Consider the following example input labeled data file</span>
<span id="cb12-945"><a href="#cb12-945"></a></span>
<span id="cb12-946"><a href="#cb12-946"></a><span class="in">```</span></span>
<span id="cb12-947"><a href="#cb12-947"></a><span class="in">label,attr1,attr2,attr3</span></span>
<span id="cb12-948"><a href="#cb12-948"></a><span class="in">1,0.0,1.1,0.1</span></span>
<span id="cb12-949"><a href="#cb12-949"></a><span class="in">0,2.0,1.0,-1.0</span></span>
<span id="cb12-950"><a href="#cb12-950"></a><span class="in">0,2.0,1.3,1.0</span></span>
<span id="cb12-951"><a href="#cb12-951"></a><span class="in">1,0.0,1.2,-0.5</span></span>
<span id="cb12-952"><a href="#cb12-952"></a><span class="in">```</span></span>
<span id="cb12-953"><a href="#cb12-953"></a></span>
<span id="cb12-954"><a href="#cb12-954"></a>Follow these steps</span>
<span id="cb12-955"><a href="#cb12-955"></a></span>
<span id="cb12-956"><a href="#cb12-956"></a><span class="ss">1. </span>Split the labeled data set in two subsets</span>
<span id="cb12-957"><a href="#cb12-957"></a><span class="ss">    - </span>Training set: $75\%$ of the labeled data</span>
<span id="cb12-958"><a href="#cb12-958"></a><span class="ss">    - </span>Test set: $25\%$ of the labeled data</span>
<span id="cb12-959"><a href="#cb12-959"></a><span class="ss">2. </span>Infer/train a logistic regression model on the training set</span>
<span id="cb12-960"><a href="#cb12-960"></a><span class="ss">3. </span>Evaluate the prediction quality of the inferred model on both the test set and the training set</span>
<span id="cb12-961"><a href="#cb12-961"></a></span>
<span id="cb12-962"><a href="#cb12-962"></a><span class="in">```python</span></span>
<span id="cb12-963"><a href="#cb12-963"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-964"><a href="#cb12-964"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-965"><a href="#cb12-965"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb12-966"><a href="#cb12-966"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> MulticlassClassificationEvaluator</span>
<span id="cb12-967"><a href="#cb12-967"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb12-968"><a href="#cb12-968"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb12-969"><a href="#cb12-969"></a></span>
<span id="cb12-970"><a href="#cb12-970"></a><span class="co">## input and output folders</span></span>
<span id="cb12-971"><a href="#cb12-971"></a>labeledData <span class="op">=</span> <span class="st">"ex_dataValidation/labeledData.csv"</span></span>
<span id="cb12-972"><a href="#cb12-972"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLRPipelineValidation/"</span></span>
<span id="cb12-973"><a href="#cb12-973"></a></span>
<span id="cb12-974"><a href="#cb12-974"></a><span class="co">## Create a DataFrame from labeledData.csv</span></span>
<span id="cb12-975"><a href="#cb12-975"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-976"><a href="#cb12-976"></a>labeledDataDF <span class="op">=</span> spark.read.load(</span>
<span id="cb12-977"><a href="#cb12-977"></a>    labeledData,</span>
<span id="cb12-978"><a href="#cb12-978"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-979"><a href="#cb12-979"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-980"><a href="#cb12-980"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-981"><a href="#cb12-981"></a>)</span>
<span id="cb12-982"><a href="#cb12-982"></a></span>
<span id="cb12-983"><a href="#cb12-983"></a><span class="co">## Split labeled data in training and test set</span></span>
<span id="cb12-984"><a href="#cb12-984"></a><span class="co">## training data : 75%</span></span>
<span id="cb12-985"><a href="#cb12-985"></a><span class="co">## test data: 25%</span></span>
<span id="cb12-986"><a href="#cb12-986"></a>trainDF, testDF <span class="op">=</span> labeledDataDF.randomSplit([<span class="fl">0.75</span>, <span class="fl">0.25</span>], seed<span class="op">=</span><span class="dv">10</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-987"><a href="#cb12-987"></a></span>
<span id="cb12-988"><a href="#cb12-988"></a><span class="co">## *************************</span></span>
<span id="cb12-989"><a href="#cb12-989"></a><span class="co">## Training step</span></span>
<span id="cb12-990"><a href="#cb12-990"></a><span class="co">## *************************</span></span>
<span id="cb12-991"><a href="#cb12-991"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb12-992"><a href="#cb12-992"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb12-993"><a href="#cb12-993"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb12-994"><a href="#cb12-994"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>],</span>
<span id="cb12-995"><a href="#cb12-995"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb12-996"><a href="#cb12-996"></a>)</span>
<span id="cb12-997"><a href="#cb12-997"></a></span>
<span id="cb12-998"><a href="#cb12-998"></a><span class="co">## Create a LogisticRegression object.</span></span>
<span id="cb12-999"><a href="#cb12-999"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="cb12-1000"><a href="#cb12-1000"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="cb12-1001"><a href="#cb12-1001"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-1002"><a href="#cb12-1002"></a></span>
<span id="cb12-1003"><a href="#cb12-1003"></a><span class="co">## Set the values of the parameters of the</span></span>
<span id="cb12-1004"><a href="#cb12-1004"></a><span class="co">## Logistic Regression algorithm using the setter methods.</span></span>
<span id="cb12-1005"><a href="#cb12-1005"></a><span class="co">## There is one set method for each parameter</span></span>
<span id="cb12-1006"><a href="#cb12-1006"></a><span class="co">## For example, we are setting the number of maximum iterations to 10</span></span>
<span id="cb12-1007"><a href="#cb12-1007"></a><span class="co">## and the regularization parameter to 0.01</span></span>
<span id="cb12-1008"><a href="#cb12-1008"></a>lr.setMaxIter(<span class="dv">10</span>)</span>
<span id="cb12-1009"><a href="#cb12-1009"></a>lr.setRegParam(<span class="fl">0.01</span>)</span>
<span id="cb12-1010"><a href="#cb12-1010"></a></span>
<span id="cb12-1011"><a href="#cb12-1011"></a><span class="co">## Define a pipeline that is used to create the logistic regression</span></span>
<span id="cb12-1012"><a href="#cb12-1012"></a><span class="co">## model on the training data. The pipeline includes also</span></span>
<span id="cb12-1013"><a href="#cb12-1013"></a><span class="co">## the preprocessing step</span></span>
<span id="cb12-1014"><a href="#cb12-1014"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, lr])</span>
<span id="cb12-1015"><a href="#cb12-1015"></a></span>
<span id="cb12-1016"><a href="#cb12-1016"></a><span class="co">## Execute the pipeline on the training data to build the</span></span>
<span id="cb12-1017"><a href="#cb12-1017"></a><span class="co">## classification model</span></span>
<span id="cb12-1018"><a href="#cb12-1018"></a>classificationModel <span class="op">=</span> pipeline.fit(trainDF)</span>
<span id="cb12-1019"><a href="#cb12-1019"></a><span class="co">## Now, the classification model can be used to predict the class label</span></span>
<span id="cb12-1020"><a href="#cb12-1020"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb12-1021"><a href="#cb12-1021"></a></span>
<span id="cb12-1022"><a href="#cb12-1022"></a><span class="co">## Make predictions on the test data using the transform() method of the</span></span>
<span id="cb12-1023"><a href="#cb12-1023"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="cb12-1024"><a href="#cb12-1024"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="cb12-1025"><a href="#cb12-1025"></a><span class="co">## also the assembler is executed</span></span>
<span id="cb12-1026"><a href="#cb12-1026"></a>predictionsDF <span class="op">=</span> classificationModel.transform(testDF)</span>
<span id="cb12-1027"><a href="#cb12-1027"></a></span>
<span id="cb12-1028"><a href="#cb12-1028"></a><span class="co">## The predicted value is column prediction</span></span>
<span id="cb12-1029"><a href="#cb12-1029"></a><span class="co">## The actual label is column label</span></span>
<span id="cb12-1030"><a href="#cb12-1030"></a><span class="co">## Define a set of evaluators</span></span>
<span id="cb12-1031"><a href="#cb12-1031"></a>myEvaluatorAcc <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="cb12-1032"><a href="#cb12-1032"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb12-1033"><a href="#cb12-1033"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="cb12-1034"><a href="#cb12-1034"></a>    metricName<span class="op">=</span><span class="st">'accuracy'</span></span>
<span id="cb12-1035"><a href="#cb12-1035"></a>)</span>
<span id="cb12-1036"><a href="#cb12-1036"></a></span>
<span id="cb12-1037"><a href="#cb12-1037"></a>myEvaluatorF1 <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="cb12-1038"><a href="#cb12-1038"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb12-1039"><a href="#cb12-1039"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="cb12-1040"><a href="#cb12-1040"></a>    metricName<span class="op">=</span><span class="st">'f1'</span></span>
<span id="cb12-1041"><a href="#cb12-1041"></a>)</span>
<span id="cb12-1042"><a href="#cb12-1042"></a></span>
<span id="cb12-1043"><a href="#cb12-1043"></a>myEvaluatorWeightedPrecision <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="cb12-1044"><a href="#cb12-1044"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb12-1045"><a href="#cb12-1045"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="cb12-1046"><a href="#cb12-1046"></a>    metricName<span class="op">=</span><span class="st">'weightedPrecision'</span></span>
<span id="cb12-1047"><a href="#cb12-1047"></a>)</span>
<span id="cb12-1048"><a href="#cb12-1048"></a></span>
<span id="cb12-1049"><a href="#cb12-1049"></a>myEvaluatorWeightedRecall <span class="op">=</span> MulticlassClassificationEvaluator(</span>
<span id="cb12-1050"><a href="#cb12-1050"></a>    labelCol<span class="op">=</span><span class="st">"label"</span>,</span>
<span id="cb12-1051"><a href="#cb12-1051"></a>    predictionCol<span class="op">=</span><span class="st">"prediction"</span>,</span>
<span id="cb12-1052"><a href="#cb12-1052"></a>    metricName<span class="op">=</span><span class="st">'weightedRecall'</span></span>
<span id="cb12-1053"><a href="#cb12-1053"></a>)</span>
<span id="cb12-1054"><a href="#cb12-1054"></a></span>
<span id="cb12-1055"><a href="#cb12-1055"></a><span class="co">## Apply the evaluators on the predictions associated with the test data</span></span>
<span id="cb12-1056"><a href="#cb12-1056"></a><span class="co">## Print the results on the standard output</span></span>
<span id="cb12-1057"><a href="#cb12-1057"></a><span class="bu">print</span>(</span>
<span id="cb12-1058"><a href="#cb12-1058"></a>    <span class="st">"Accuracy on test data "</span>, </span>
<span id="cb12-1059"><a href="#cb12-1059"></a>    myEvaluatorAcc.evaluate(predictionsDF)</span>
<span id="cb12-1060"><a href="#cb12-1060"></a>)</span>
<span id="cb12-1061"><a href="#cb12-1061"></a></span>
<span id="cb12-1062"><a href="#cb12-1062"></a><span class="bu">print</span>(</span>
<span id="cb12-1063"><a href="#cb12-1063"></a>    <span class="st">"F1 on test data "</span>, </span>
<span id="cb12-1064"><a href="#cb12-1064"></a>    myEvaluatorF1.evaluate(predictionsDF)</span>
<span id="cb12-1065"><a href="#cb12-1065"></a>)</span>
<span id="cb12-1066"><a href="#cb12-1066"></a></span>
<span id="cb12-1067"><a href="#cb12-1067"></a><span class="bu">print</span>(</span>
<span id="cb12-1068"><a href="#cb12-1068"></a>    <span class="st">"Weighted recall on test data "</span>,</span>
<span id="cb12-1069"><a href="#cb12-1069"></a>    myEvaluatorWeightedRecall.evaluate(predictionsDF)</span>
<span id="cb12-1070"><a href="#cb12-1070"></a>)</span>
<span id="cb12-1071"><a href="#cb12-1071"></a></span>
<span id="cb12-1072"><a href="#cb12-1072"></a><span class="bu">print</span>(</span>
<span id="cb12-1073"><a href="#cb12-1073"></a>    <span class="st">"Weighted precision on test data "</span>,</span>
<span id="cb12-1074"><a href="#cb12-1074"></a>    myEvaluatorWeightedPrecision.evaluate(predictionsDF)</span>
<span id="cb12-1075"><a href="#cb12-1075"></a>)</span>
<span id="cb12-1076"><a href="#cb12-1076"></a></span>
<span id="cb12-1077"><a href="#cb12-1077"></a><span class="co">## Compute the prediction quality also for the training data.</span></span>
<span id="cb12-1078"><a href="#cb12-1078"></a><span class="co">## To check if the model is overfitted on the training data</span></span>
<span id="cb12-1079"><a href="#cb12-1079"></a></span>
<span id="cb12-1080"><a href="#cb12-1080"></a><span class="co">## Make predictions on the training data using the transform() method of the</span></span>
<span id="cb12-1081"><a href="#cb12-1081"></a><span class="co">## trained classification model transform uses only the content of 'features'</span></span>
<span id="cb12-1082"><a href="#cb12-1082"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="cb12-1083"><a href="#cb12-1083"></a><span class="co">## also the assembler is executed</span></span>
<span id="cb12-1084"><a href="#cb12-1084"></a>predictionsTrainingDF <span class="op">=</span> classificationModel.transform(trainDF)</span>
<span id="cb12-1085"><a href="#cb12-1085"></a></span>
<span id="cb12-1086"><a href="#cb12-1086"></a><span class="co">## Apply the evaluators on the predictions associated with the test data</span></span>
<span id="cb12-1087"><a href="#cb12-1087"></a><span class="co">## Print the results on the standard output</span></span>
<span id="cb12-1088"><a href="#cb12-1088"></a></span>
<span id="cb12-1089"><a href="#cb12-1089"></a><span class="bu">print</span>(</span>
<span id="cb12-1090"><a href="#cb12-1090"></a>    <span class="st">"Accuracy on training data "</span>,</span>
<span id="cb12-1091"><a href="#cb12-1091"></a>    myEvaluatorAcc.evaluate(predictionsTrainingDF)</span>
<span id="cb12-1092"><a href="#cb12-1092"></a>)</span>
<span id="cb12-1093"><a href="#cb12-1093"></a></span>
<span id="cb12-1094"><a href="#cb12-1094"></a><span class="bu">print</span>(</span>
<span id="cb12-1095"><a href="#cb12-1095"></a>    <span class="st">"F1 on training data "</span>,</span>
<span id="cb12-1096"><a href="#cb12-1096"></a>    myEvaluatorF1.evaluate(predictionsTrainingDF)</span>
<span id="cb12-1097"><a href="#cb12-1097"></a>)</span>
<span id="cb12-1098"><a href="#cb12-1098"></a></span>
<span id="cb12-1099"><a href="#cb12-1099"></a><span class="bu">print</span>(</span>
<span id="cb12-1100"><a href="#cb12-1100"></a>    <span class="st">"Weighted recall on training data "</span>,</span>
<span id="cb12-1101"><a href="#cb12-1101"></a>    myEvaluatorWeightedRecall.evaluate(predictionsTrainingDF)</span>
<span id="cb12-1102"><a href="#cb12-1102"></a>)</span>
<span id="cb12-1103"><a href="#cb12-1103"></a></span>
<span id="cb12-1104"><a href="#cb12-1104"></a><span class="bu">print</span>(</span>
<span id="cb12-1105"><a href="#cb12-1105"></a>    <span class="st">"Weighted precision on training data "</span>,</span>
<span id="cb12-1106"><a href="#cb12-1106"></a>    myEvaluatorWeightedPrecision.evaluate(predictionsTrainingDF)</span>
<span id="cb12-1107"><a href="#cb12-1107"></a>)</span>
<span id="cb12-1108"><a href="#cb12-1108"></a><span class="in">```</span></span>
<span id="cb12-1109"><a href="#cb12-1109"></a><span class="ss">1. </span><span class="in">`randomSplit`</span> can be used to split the content of an input DataFrame in subsets</span>
<span id="cb12-1110"><a href="#cb12-1110"></a></span>
<span id="cb12-1111"><a href="#cb12-1111"></a>:::</span>
<span id="cb12-1112"><a href="#cb12-1112"></a></span>
<span id="cb12-1113"><a href="#cb12-1113"></a><span class="fu">## Hyperparameter tuning</span></span>
<span id="cb12-1114"><a href="#cb12-1114"></a>The setting of the parameters of an algorithm is always a difficult task. A brute force approach can be used to find the setting optimizing a quality index, by splitting the training data in two subsets:</span>
<span id="cb12-1115"><a href="#cb12-1115"></a></span>
<span id="cb12-1116"><a href="#cb12-1116"></a><span class="ss">- </span>The first set is used to build a model</span>
<span id="cb12-1117"><a href="#cb12-1117"></a><span class="ss">- </span>The second one is used to evaluate the quality of the model</span>
<span id="cb12-1118"><a href="#cb12-1118"></a></span>
<span id="cb12-1119"><a href="#cb12-1119"></a>The setting that maximizes a quality index (e.g., the prediction accuracy) is used to build the final model on the whole training dataset.</span>
<span id="cb12-1120"><a href="#cb12-1120"></a></span>
<span id="cb12-1121"><a href="#cb12-1121"></a>Using one single split of the training set usually leads to biased results, so the cross-validation approach is normally used </span>
<span id="cb12-1122"><a href="#cb12-1122"></a></span>
<span id="cb12-1123"><a href="#cb12-1123"></a><span class="ss">- </span>Create $k$ splits and $k$ models</span>
<span id="cb12-1124"><a href="#cb12-1124"></a><span class="ss">- </span>The parameter setting that achieves, on the average, the best result on the $k$ models is selected as final setting of the algorithm parameters</span>
<span id="cb12-1125"><a href="#cb12-1125"></a></span>
<span id="cb12-1126"><a href="#cb12-1126"></a>Spark supports a brute-force grid-based approach to evaluate a set of possible parameter settings on a pipeline</span>
<span id="cb12-1127"><a href="#cb12-1127"></a></span>
<span id="cb12-1128"><a href="#cb12-1128"></a><span class="ss">- </span>Input</span>
<span id="cb12-1129"><a href="#cb12-1129"></a><span class="ss">    - </span>An MLlib pipeline</span>
<span id="cb12-1130"><a href="#cb12-1130"></a><span class="ss">    - </span>A set of values to be evaluated for each input parameter of the pipeline: all the possible combinations of the specified parameter values are considered and the related models are automatically generated and evaluated by Spark</span>
<span id="cb12-1131"><a href="#cb12-1131"></a><span class="ss">    - </span>A quality evaluation metric to evaluate the result of the input pipeline</span>
<span id="cb12-1132"><a href="#cb12-1132"></a><span class="ss">- </span>Output: the model associated with the best parameter setting, in term of quality evaluation metric</span>
<span id="cb12-1133"><a href="#cb12-1133"></a></span>
<span id="cb12-1134"><a href="#cb12-1134"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-1135"><a href="#cb12-1135"></a><span class="fu">### Example</span></span>
<span id="cb12-1136"><a href="#cb12-1136"></a>This example shows how a grid-based approach can be used to tune a logistic regression classifier on a structured dataset: the pipeline that is repeated multiple times is based on the cross validation component. The input data set is the same structured dataset used for the example of the evaluators.</span>
<span id="cb12-1137"><a href="#cb12-1137"></a></span>
<span id="cb12-1138"><a href="#cb12-1138"></a>The following parameters of the logistic regression algorithm are considered in the brute-force search/parameter tuning</span>
<span id="cb12-1139"><a href="#cb12-1139"></a></span>
<span id="cb12-1140"><a href="#cb12-1140"></a><span class="ss">- </span>Maximum iteration: $<span class="co">[</span><span class="ot">10, 100, 1000</span><span class="co">]</span>$</span>
<span id="cb12-1141"><a href="#cb12-1141"></a><span class="ss">- </span>Regulation parameter: $<span class="co">[</span><span class="ot">0.1, 0.01</span><span class="co">]</span>$</span>
<span id="cb12-1142"><a href="#cb12-1142"></a></span>
<span id="cb12-1143"><a href="#cb12-1143"></a>In total, 6 parameter configurations are evaluated ($3*2$).</span>
<span id="cb12-1144"><a href="#cb12-1144"></a></span>
<span id="cb12-1145"><a href="#cb12-1145"></a><span class="in">```python</span></span>
<span id="cb12-1146"><a href="#cb12-1146"></a><span class="im">from</span> pyspark.mllib.linalg <span class="im">import</span> Vectors</span>
<span id="cb12-1147"><a href="#cb12-1147"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb12-1148"><a href="#cb12-1148"></a><span class="im">from</span> pyspark.ml.classification <span class="im">import</span> LogisticRegression</span>
<span id="cb12-1149"><a href="#cb12-1149"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> MulticlassClassificationEvaluator</span>
<span id="cb12-1150"><a href="#cb12-1150"></a><span class="im">from</span> pyspark.ml.evaluation <span class="im">import</span> BinaryClassificationEvaluator</span>
<span id="cb12-1151"><a href="#cb12-1151"></a><span class="im">from</span> pyspark.ml.tuning <span class="im">import</span> ParamGridBuilder</span>
<span id="cb12-1152"><a href="#cb12-1152"></a><span class="im">from</span> pyspark.ml.tuning <span class="im">import</span> CrossValidator</span>
<span id="cb12-1153"><a href="#cb12-1153"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb12-1154"><a href="#cb12-1154"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> PipelineModel</span>
<span id="cb12-1155"><a href="#cb12-1155"></a></span>
<span id="cb12-1156"><a href="#cb12-1156"></a><span class="co">## input and output folders</span></span>
<span id="cb12-1157"><a href="#cb12-1157"></a>labeledData <span class="op">=</span> <span class="st">"ex_dataValidation/labeledData.csv"</span></span>
<span id="cb12-1158"><a href="#cb12-1158"></a>unlabeledData <span class="op">=</span> <span class="st">"ex_dataValidation/unlabeledData.csv"</span></span>
<span id="cb12-1159"><a href="#cb12-1159"></a>outputPath <span class="op">=</span> <span class="st">"predictionsLRPipelineTuning/"</span></span>
<span id="cb12-1160"><a href="#cb12-1160"></a></span>
<span id="cb12-1161"><a href="#cb12-1161"></a><span class="co">## Create a DataFrame from labeledData.csv</span></span>
<span id="cb12-1162"><a href="#cb12-1162"></a><span class="co">## Training data in raw format</span></span>
<span id="cb12-1163"><a href="#cb12-1163"></a>labeledDataDF <span class="op">=</span> spark.read.load(</span>
<span id="cb12-1164"><a href="#cb12-1164"></a>    labeledData,</span>
<span id="cb12-1165"><a href="#cb12-1165"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-1166"><a href="#cb12-1166"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-1167"><a href="#cb12-1167"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-1168"><a href="#cb12-1168"></a>)</span>
<span id="cb12-1169"><a href="#cb12-1169"></a></span>
<span id="cb12-1170"><a href="#cb12-1170"></a><span class="co">## *************************</span></span>
<span id="cb12-1171"><a href="#cb12-1171"></a><span class="co">## Training step</span></span>
<span id="cb12-1172"><a href="#cb12-1172"></a><span class="co">## *************************</span></span>
<span id="cb12-1173"><a href="#cb12-1173"></a><span class="co">## Define an assembler to create a column (features) of type Vector</span></span>
<span id="cb12-1174"><a href="#cb12-1174"></a><span class="co">## containing the double values associated with columns attr1, attr2, attr3</span></span>
<span id="cb12-1175"><a href="#cb12-1175"></a>assembler <span class="op">=</span> VectorAssembler(</span>
<span id="cb12-1176"><a href="#cb12-1176"></a>    inputCols<span class="op">=</span>[<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>],</span>
<span id="cb12-1177"><a href="#cb12-1177"></a>    outputCol<span class="op">=</span><span class="st">"features"</span></span>
<span id="cb12-1178"><a href="#cb12-1178"></a>)</span>
<span id="cb12-1179"><a href="#cb12-1179"></a></span>
<span id="cb12-1180"><a href="#cb12-1180"></a><span class="co">## Create a LogisticRegression object.</span></span>
<span id="cb12-1181"><a href="#cb12-1181"></a><span class="co">## LogisticRegression is an Estimator that is used to</span></span>
<span id="cb12-1182"><a href="#cb12-1182"></a><span class="co">## create a classification model based on logistic regression.</span></span>
<span id="cb12-1183"><a href="#cb12-1183"></a>lr <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-1184"><a href="#cb12-1184"></a></span>
<span id="cb12-1185"><a href="#cb12-1185"></a><span class="co">## Define a pipeline that is used to create the logistic regression</span></span>
<span id="cb12-1186"><a href="#cb12-1186"></a><span class="co">## model on the training data. The pipeline includes also the preprocessing step</span></span>
<span id="cb12-1187"><a href="#cb12-1187"></a>pipeline <span class="op">=</span> Pipeline().setStages([assembler, lr])</span>
<span id="cb12-1188"><a href="#cb12-1188"></a></span>
<span id="cb12-1189"><a href="#cb12-1189"></a><span class="co">## We use a ParamGridBuilder to construct a grid of parameter values to</span></span>
<span id="cb12-1190"><a href="#cb12-1190"></a><span class="co">## search over.</span></span>
<span id="cb12-1191"><a href="#cb12-1191"></a><span class="co">## We set 3 values for lr.setMaxIter and 2 values for lr.regParam.</span></span>
<span id="cb12-1192"><a href="#cb12-1192"></a><span class="co">## This grid will evaluate 3 x 2 = 6 parameter settings for</span></span>
<span id="cb12-1193"><a href="#cb12-1193"></a><span class="co">## the input pipeline.</span></span>
<span id="cb12-1194"><a href="#cb12-1194"></a>paramGrid <span class="op">=</span> ParamGridBuilder() <span class="op">\</span></span>
<span id="cb12-1195"><a href="#cb12-1195"></a>    .addGrid(lr.maxIter, [<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>]) <span class="op">\</span></span>
<span id="cb12-1196"><a href="#cb12-1196"></a>    .addGrid(lr.regParam, [<span class="fl">0.1</span>,<span class="fl">0.01</span>]) <span class="op">\</span></span>
<span id="cb12-1197"><a href="#cb12-1197"></a>    .build() <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-1198"><a href="#cb12-1198"></a></span>
<span id="cb12-1199"><a href="#cb12-1199"></a><span class="co">## We now treat the Pipeline as an Estimator, wrapping it in a</span></span>
<span id="cb12-1200"><a href="#cb12-1200"></a><span class="co">## CrossValidator instance. This allows us to jointly choose parameters</span></span>
<span id="cb12-1201"><a href="#cb12-1201"></a><span class="co">## for all Pipeline stages.</span></span>
<span id="cb12-1202"><a href="#cb12-1202"></a><span class="co">## CrossValidator requires</span></span>
<span id="cb12-1203"><a href="#cb12-1203"></a><span class="co">## - an Estimator</span></span>
<span id="cb12-1204"><a href="#cb12-1204"></a><span class="co">## - a set of Estimator ParamMaps</span></span>
<span id="cb12-1205"><a href="#cb12-1205"></a><span class="co">## - an Evaluator.</span></span>
<span id="cb12-1206"><a href="#cb12-1206"></a>cv <span class="op">=</span> CrossValidator() <span class="op">\</span></span>
<span id="cb12-1207"><a href="#cb12-1207"></a>    .setEstimator(pipeline) <span class="op">\</span></span>
<span id="cb12-1208"><a href="#cb12-1208"></a>    .setEstimatorParamMaps(paramGrid) <span class="op">\</span></span>
<span id="cb12-1209"><a href="#cb12-1209"></a>    .setEvaluator(BinaryClassificationEvaluator()) <span class="op">\</span></span>
<span id="cb12-1210"><a href="#cb12-1210"></a>    .setNumFolds(<span class="dv">3</span>) <span class="co"># &lt;2&gt;</span></span>
<span id="cb12-1211"><a href="#cb12-1211"></a></span>
<span id="cb12-1212"><a href="#cb12-1212"></a><span class="co">## Run cross-validation. The result is the logistic regression model</span></span>
<span id="cb12-1213"><a href="#cb12-1213"></a><span class="co">## based on the best set of parameters (based on the results of the</span></span>
<span id="cb12-1214"><a href="#cb12-1214"></a><span class="co">## cross-validation operation).</span></span>
<span id="cb12-1215"><a href="#cb12-1215"></a>tunedLRmodel <span class="op">=</span> cv.fit(labeledDataDF) <span class="co"># &lt;3&gt;</span></span>
<span id="cb12-1216"><a href="#cb12-1216"></a></span>
<span id="cb12-1217"><a href="#cb12-1217"></a><span class="co">## Now, the tuned classification model can be used to predict the class label</span></span>
<span id="cb12-1218"><a href="#cb12-1218"></a><span class="co">## of new unlabeled data</span></span>
<span id="cb12-1219"><a href="#cb12-1219"></a></span>
<span id="cb12-1220"><a href="#cb12-1220"></a><span class="co">## *************************</span></span>
<span id="cb12-1221"><a href="#cb12-1221"></a><span class="co">## Prediction step</span></span>
<span id="cb12-1222"><a href="#cb12-1222"></a><span class="co">## *************************</span></span>
<span id="cb12-1223"><a href="#cb12-1223"></a><span class="co">## Create a DataFrame from unlabeledData.csv</span></span>
<span id="cb12-1224"><a href="#cb12-1224"></a><span class="co">## Unlabeled data in raw format</span></span>
<span id="cb12-1225"><a href="#cb12-1225"></a>unlabeledData <span class="op">=</span> spark.read.load(</span>
<span id="cb12-1226"><a href="#cb12-1226"></a>    unlabeledData,</span>
<span id="cb12-1227"><a href="#cb12-1227"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"csv"</span>,</span>
<span id="cb12-1228"><a href="#cb12-1228"></a>    header<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-1229"><a href="#cb12-1229"></a>    inferSchema<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-1230"><a href="#cb12-1230"></a>)</span>
<span id="cb12-1231"><a href="#cb12-1231"></a></span>
<span id="cb12-1232"><a href="#cb12-1232"></a><span class="co">## Make predictions on the unlabled data using the transform() method of the</span></span>
<span id="cb12-1233"><a href="#cb12-1233"></a><span class="co">## trained tuned classification model transform uses only the content of 'features'</span></span>
<span id="cb12-1234"><a href="#cb12-1234"></a><span class="co">## to perform the predictions. The model is associated with the pipeline and hence</span></span>
<span id="cb12-1235"><a href="#cb12-1235"></a><span class="co">## also the assembler is executed</span></span>
<span id="cb12-1236"><a href="#cb12-1236"></a>predictionsDF <span class="op">=</span> tunedLRmodel.transform(unlabeledData)</span>
<span id="cb12-1237"><a href="#cb12-1237"></a></span>
<span id="cb12-1238"><a href="#cb12-1238"></a><span class="co">## The returned DataFrame has the following schema (attributes)</span></span>
<span id="cb12-1239"><a href="#cb12-1239"></a><span class="co">## - features: vector (values of the attributes)</span></span>
<span id="cb12-1240"><a href="#cb12-1240"></a><span class="co">## - label: double (value of the class label)</span></span>
<span id="cb12-1241"><a href="#cb12-1241"></a><span class="co">## - rawPrediction: vector (nullable = true)</span></span>
<span id="cb12-1242"><a href="#cb12-1242"></a><span class="co">## - probability: vector (The i-th cell contains the probability that the current</span></span>
<span id="cb12-1243"><a href="#cb12-1243"></a><span class="co">## record belongs to the i-th class</span></span>
<span id="cb12-1244"><a href="#cb12-1244"></a><span class="co">## - prediction: double (the predicted class label)</span></span>
<span id="cb12-1245"><a href="#cb12-1245"></a><span class="co">## Select only the original features (i.e., the value of the original attributes</span></span>
<span id="cb12-1246"><a href="#cb12-1246"></a><span class="co">## attr1, attr2, attr3) and the predicted class for each record</span></span>
<span id="cb12-1247"><a href="#cb12-1247"></a>predictions <span class="op">=</span> predictionsDF.select(<span class="st">"attr1"</span>, <span class="st">"attr2"</span>, <span class="st">"attr3"</span>, <span class="st">"prediction"</span>)</span>
<span id="cb12-1248"><a href="#cb12-1248"></a></span>
<span id="cb12-1249"><a href="#cb12-1249"></a><span class="co">## Save the result in an HDFS output folder</span></span>
<span id="cb12-1250"><a href="#cb12-1250"></a>predictions.write.csv(outputPath, header<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb12-1251"><a href="#cb12-1251"></a><span class="in">```</span></span>
<span id="cb12-1252"><a href="#cb12-1252"></a><span class="ss">1. </span>There is one call to the addGrid method for each parameter that we want to set: each call to the addGrid method is characterized by the parameter we want to consider, and the list of values to test/to consider.</span>
<span id="cb12-1253"><a href="#cb12-1253"></a><span class="ss">2. </span>Here the characteristics of the cross validation are set: the pipeline to be evaluated, the set of parameters to be considered, the evaluator (i.e., the object that is used to evaluate the quality of the model), and the number of folds to consider (i.e., the number of repetitions).</span>
<span id="cb12-1254"><a href="#cb12-1254"></a><span class="ss">3. </span>The returned model is the one associated with the best parameter setting, based on the result of the cross-validation test</span>
<span id="cb12-1255"><a href="#cb12-1255"></a></span>
<span id="cb12-1256"><a href="#cb12-1256"></a>:::</span>
<span id="cb12-1257"><a href="#cb12-1257"></a></span>
<span id="cb12-1258"><a href="#cb12-1258"></a><span class="fu">## Sparse labeled data</span></span>
<span id="cb12-1259"><a href="#cb12-1259"></a>Frequently the training data are sparse (e.g., textual data are sparse: each document contains only a subset of the possible words), so sparse vectors are frequently used. MLlib supports reading training examples stored in the LIBSVM format: this is a commonly used textual format that is used to represent sparse documents/data points.</span>
<span id="cb12-1260"><a href="#cb12-1260"></a></span>
<span id="cb12-1261"><a href="#cb12-1261"></a>The LIBSVM format is a textual format in which each line represents an input record/data point by using a sparse feature vector: each line has the format </span>
<span id="cb12-1262"><a href="#cb12-1262"></a></span>
<span id="cb12-1263"><a href="#cb12-1263"></a><span class="in">```</span></span>
<span id="cb12-1264"><a href="#cb12-1264"></a><span class="in">label index1:value1 index2:value2 ...</span></span>
<span id="cb12-1265"><a href="#cb12-1265"></a><span class="in">```</span></span>
<span id="cb12-1266"><a href="#cb12-1266"></a></span>
<span id="cb12-1267"><a href="#cb12-1267"></a>where</span>
<span id="cb12-1268"><a href="#cb12-1268"></a></span>
<span id="cb12-1269"><a href="#cb12-1269"></a><span class="ss">- </span><span class="in">`label`</span> is an integer associated with the class label. It is the first value of each line.</span>
<span id="cb12-1270"><a href="#cb12-1270"></a><span class="ss">- </span><span class="in">`index#`</span> are integer values representing the features</span>
<span id="cb12-1271"><a href="#cb12-1271"></a><span class="ss">- </span><span class="in">`value#`</span> are the (double) values of the features</span>
<span id="cb12-1272"><a href="#cb12-1272"></a></span>
<span id="cb12-1273"><a href="#cb12-1273"></a>Consider the following two records/data points characterized by 4 predictive features and a class label</span>
<span id="cb12-1274"><a href="#cb12-1274"></a></span>
<span id="cb12-1275"><a href="#cb12-1275"></a>|||</span>
<span id="cb12-1276"><a href="#cb12-1276"></a>|--|-|</span>
<span id="cb12-1277"><a href="#cb12-1277"></a>|$\textbf{Features} = <span class="co">[</span><span class="ot">5.8, 1.7, 0 , 0 </span><span class="co">]</span>$|$\textbf{Label} = 1$|</span>
<span id="cb12-1278"><a href="#cb12-1278"></a>|$\textbf{Features} = <span class="co">[</span><span class="ot">4.1, 0 , 2.5, 1.2</span><span class="co">]</span>$|$\textbf{Label} = 0$|</span>
<span id="cb12-1279"><a href="#cb12-1279"></a></span>
<span id="cb12-1280"><a href="#cb12-1280"></a>Their LIBSVM format-based representation is the following</span>
<span id="cb12-1281"><a href="#cb12-1281"></a></span>
<span id="cb12-1282"><a href="#cb12-1282"></a><span class="in">```</span></span>
<span id="cb12-1283"><a href="#cb12-1283"></a><span class="in">1 1:5.8 2:1.7</span></span>
<span id="cb12-1284"><a href="#cb12-1284"></a><span class="in">0 1:4.1 3:2.5 4:1.2</span></span>
<span id="cb12-1285"><a href="#cb12-1285"></a><span class="in">```</span></span>
<span id="cb12-1286"><a href="#cb12-1286"></a></span>
<span id="cb12-1287"><a href="#cb12-1287"></a>LIBSVM files can be loaded into DataFrames by combining the following methods</span>
<span id="cb12-1288"><a href="#cb12-1288"></a></span>
<span id="cb12-1289"><a href="#cb12-1289"></a><span class="ss">- </span><span class="in">`read()`</span></span>
<span id="cb12-1290"><a href="#cb12-1290"></a><span class="ss">- </span><span class="in">`format("libsvm")`</span></span>
<span id="cb12-1291"><a href="#cb12-1291"></a><span class="ss">- </span><span class="in">`load(inputpath)`</span></span>
<span id="cb12-1292"><a href="#cb12-1292"></a></span>
<span id="cb12-1293"><a href="#cb12-1293"></a>The returned DataFrame has two columns:</span>
<span id="cb12-1294"><a href="#cb12-1294"></a></span>
<span id="cb12-1295"><a href="#cb12-1295"></a><span class="ss">- </span>label: the double value associated with the label</span>
<span id="cb12-1296"><a href="#cb12-1296"></a><span class="ss">- </span>features: the sparse vector associated with the predictive features</span>
<span id="cb12-1297"><a href="#cb12-1297"></a></span>
<span id="cb12-1298"><a href="#cb12-1298"></a>:::{.callout-note collapse="true"}</span>
<span id="cb12-1299"><a href="#cb12-1299"></a><span class="fu">### Example </span></span>
<span id="cb12-1300"><a href="#cb12-1300"></a></span>
<span id="cb12-1301"><a href="#cb12-1301"></a><span class="in">```python</span></span>
<span id="cb12-1302"><a href="#cb12-1302"></a>spark.read.<span class="bu">format</span>(<span class="st">"libsvm"</span>).load(<span class="st">"sample_libsvm_data.txt"</span>)</span>
<span id="cb12-1303"><a href="#cb12-1303"></a><span class="in">```</span></span>
<span id="cb12-1304"><a href="#cb12-1304"></a></span>
<span id="cb12-1305"><a href="#cb12-1305"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>