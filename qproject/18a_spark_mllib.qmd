---
title: "Spark MLlib"
---

Spark MLlib is the Spark component providing the machine learning/data mining algorithms

- Pre-processing techniques
- Classification (supervised learning)
- Clustering (unsupervised learning)
- Itemset mining

MLlib APIs are divided into two packages:

- `pyspark.mllib`: It contains the original APIs built on top of RDDs. This version of the APIs is in maintenance mode and will be probably deprecated in the next releases of Spark.
- `pyspark.ml`: It provides higher-level API built on top of DataFrames (i.e, Dataset\<Row\>) for constructing ML pipelines. It is recommended because the DataFrame-based API is more versatile and flexible, also providing the pipeline concept. This is the one explained in this course.

# Data types
Spark MLlib is based on a set of basic local and distributed data types:

- Local vector
- Local matrix
- Distributed matrix
- ...

DataFrames for ML-based applications contain objects based on these basic data types.

## Local vectors
Local `pyspark.ml.linalg.Vector` objects in MLlib are used to store vectors (in dense and sparse representations) of double values. The MLlib algorithms work on vectors of doubles, used to represent the input records/data (one vector for each input record). Non double attributes/values must be mapped to double values before applying MLlib algorithms.

:::{.callout-note collapse="true"}
## Example
Consider the vector of doubles `[1.0, 0.0, 3.0]`. It can be represented

- In dense format as `[1.0, 0.0, 3.0]`
- In sparse format as `(3, [0, 2], [1.0, 3.0])`, where 
    - 3 is the size of the vector
    - The array `[0, 2]` contains the indexes of the non-zero cells
    - The array `[1.0, 3.0]` contains the values of the non-zero cells

The following code shows how dense and sparse vectors can be created in Spark 

```python
from pyspark.ml.linalg import Vectors

# Create a dense vector [1.0, 0.0, 3.0]
dv = Vectors.dense([1.0, 0.0, 3.0])

# Create a sparse vector [1.0, 0.0, 3.0] by specifying
# its indices and values corresponding to non-zero entries
# by means of a dictionary
sv = Vectors.sparse(3, { 0:1.0, 2:3.0 })
```

In the sparse vector

|||
|-|---|
| `3` | Size of the vector |
| `2:3.0` | Index and value of a non-empty cell |
| `{ 0:1.0, 2:3.0 }` | Dictionary of $index:value$ pairs |

:::

## Local matrices
Local `pyspark.ml.linalg.Matrix` objects in MLlib are used to store matrices (in dense and sparse representations) of double values. The column-major order is used to store the content of the matrix in a linear way.

![Local matrices](images/18a_spark_mllib/matrix_example.png){width=80%}

:::{.callout-note collapse="true"}
## Example
The following code shows how dense and sparse matrices can be created in Spark.

```python
from pyspark.ml.linalg import Matrices

# Create a dense matrix with two rows and three columns
# 3.0 0.0 0.0
# 1.0 1.5 2.0
dm =Matrices.dense(2,3,[3.0, 1.0, 0.0, 1.5, 0.0, 2.0])

# Create a sparse version of the same matrix
sm = Matrices.sparse(2,3, [0, 2, 3, 4], [0, 1, 1, 1] , [3, 1, 1.5, 2])
```

In the dense matrix vector

|||
|-|---|
| `2` | Number of rows |
| `3` | Number of columns |
| `[3.0, 1.0, 0.0, 1.5, 0.0, 2.0]` | Values in column/major order |

In the sparse matrix vector

|||
|-|---|
| `2` | Number of rows |
| `3` | Number of columns |
| `[0, 2, 3, 4]` | One element per column that encodes the offset in the array of non-zero values where the values of the given column start. The last element is the number of non-zero values. |
| `[0, 1, 1, 1]` | Row index of each non-zero value |
| `[3, 1, 1.5, 2]` | Array of non-zero values of the represented matrix |

:::

# Main concepts
Spark MLlib uses DataFrames as input data: the input of the MLlib algorithms are structured data (i.e., tables), and all input data must be represented by means of tables before applying the MLlib algorithms; also document collections must be transformed in a tabular format before applying the MLlib algorithms.

The DataFrames used and created by the MLlib algorithms are characterized by several columns, and each column is associated with a different role/meaning

- **label**: the target of a classification/regression analysis;
- **features**: the vector containing the values of the attributes/features of the input record/data points;
- **text**: the original text of a document before being transformed in a tabular format;
- **prediction**: the predicted value of a classification/regression analysis.

## Transformer
A Transformer is an ML algorithm/procedure that transforms one DataFrame into another DataFrame by means of the method `transform(inputDataFrame)`.

:::{.callout-note collapse="true"}
## Example 1
A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.
:::

:::{.callout-note collapse="true"}
## Example 2
A classification model is a Transformer that can be applied on a DataFrame with features and transforms it into a DataFrame with also the prediction column.
:::

## Estimator
An Estimator is an ML algorithm/procedure that is fit on an input (training) DataFrame to produce a Transformer: each Estimator implements a `fit()` method, which accepts a DataFrame and produces a Model of type `Transformer`.

An Estimator abstracts the concept of a learning algorithm or any algorithm that fits/trains on an input dataset and returns a model

:::{.callout-note collapse="true"}
## Example
The Logistic Regression classification algorithm is an Estimator: calling `fit(input training DataFrame)` on it a Logistic Regression Model is built, which is a Model/a Transformer.
:::

## Pipeline
A Pipeline chains multiple Transformers and Estimators together to specify a Machine learning/Data Mining workflow. In a pipeline, the output of a transformer/estimator is the input of the next one.

:::{.callout-note collapse="true"}
## Example
A simple text document processing workflow aiming at building a classification model includes several steps

1. Split each document into a set of words;
2. Convert each set of words into a numerical feature vector;
3. Learn a prediction model using the feature vectors and the associated class labels.
:::

## Parameters
Transformers and Estimators share common APIs for specifying the values of their parameters.

In the new APIs of Spark MLlib the use of the pipeline approach is preferred/recommended. This approach is based on the following steps

1. The set of Transformers and Estimators that are needed are instantiated;
2. A pipeline object is created and the sequence of transformers and estimators associated with the pipeline are specified;
3. The pipeline is executed and a model is trained;
4. (optional) The model is applied on new data.

# Data preprocessing
Input data must be preprocessed before applying machine learning and data mining algorithms

- To organize data in a format consistent with the one expected by the applied algorithms;
- To define good (predictive) features;
- To remove bias (e.g., normalization);
- To remove noise and missing values.

## Extracting, transformings, and selecting features
MLlib provides a set of transformers than can be used to extract, transform and select features from DataFrames

- Feature Extractors (e.g., TF-IDF, Word2Vec)
- Feature Transformers (e.g., Tokenizer, StopWordsRemover, StringIndexer, IndexToString, OneHotEncoderEstimator, Normalizer)
- Feature Selectors (e.g., VectorSlicer)

See the up-to-date list [here](https://spark.apache.org/docs/latest/ml-features.html).

# Feature transformations
Several algorithms are provided by MLlib to transform features. They are used to create new columns/features by combining or transforming other features
It is possible to perform feature transformations and feature creations by using the standard methods for DataFrames and RDDs.

## Vector Assembler
VectorAssembler (`pyspark.ml.feature.VectorAssembler`) is a transformer that combines a given list of columns into a single vector column. It is useful for combining features into a single feature vector before applying ML algorithms.

`VectorAssembler(inputCols, outputCol)`

- `inputCols`: the list of original columns to include in the new column of type `Vector`. The following input column types are accepted
    - all numeric types, boolean type, and vector type
    - Boolean values are mapped to 1 (True) and 0 (False)
- `outputCol`: the name of the new output column

When the transform method of VectorAssembler is invoked on a DataFrame the returned DataFrame has a new column (outputCol): for each record, the value of the new column is the concatenation of the values of the input columns. It has also all the columns of the input DataFrame.

:::{.callout-note collapse="true"}
## Example
Consider an input DataFrame with three columns: create a new DataFrame with a new column containing the concatenation of colB and colC in a new vector column (the name of the new column is set to features).

**Original DataFrame**

|colA|colB|colC|
|-|-|-|
|$1$|$4.5$|True|
|$2$|$0.6$|True|
|$3$|$1.5$|False|
|$4$|$12.1$|True|
|$5$|$0.0$|True|

**Transformed DataFrame**

|colA|colB|colC|features|
|-|-|-|-|-|
|$1$|$4.5$|True|$[4.5,1.0]$|
|$2$|$0.6$|True|$[0.6,1.0]$|
|$3$|$1.5$|False|$[1.5,0.0]$|
|$4$|$12.1$|True|$[12.1,1.0]$|
|$5$|$0.0$|True|$[0.0,1.0]$|

Notice that columns of DataFrames can also be vectors.

```python
from pyspark.mllib.linalg import Vectors
from pyspark.ml.feature import VectorAssembler

# input and output folders
inputPath = "data/exampleDataAssembler.csv“

# Create a DataFrame from the input data
inputDF = spark.read.load(
    inputPath,
    format="csv",
    header=True,
    inferSchema=True
)

# Create a VectorAssembler that combines columns colB and colC
# The new vetor column is called features
myVectorAssembler = VectorAssembler(
    inputCols = ['colB', 'colC'],
    outputCol = 'features'
)

# Apply myVectorAssembler on the input DataFrame
transformedDF = myVectorAssembler.transform(inputDF)
```

:::

## Data Normalization
MLlib provides a set of normalization algorithms (called scalers)

- StandardScaler
- MinMaxScaler
- Normalizer
- MaxAbsScaler

### StandardScaler
StandardScaler (`pyspark.ml.feature.StandardScaler`) is an Estimator that returns a Transformer (`pyspark.ml.feature.StandardScalerModel`). `StandardScalerModel` transforms a vector column of an input DataFrame normalizing each feature of the input vector column to have unit standard deviation and/or zero mean.

`StandardScaler(inputCol, outputCol)`

- `inputCol`: the name of the input vector column (of doubles) to normalize
- `outputCol`: the name of the new output normalized vector column

Invoke the fit method of `StandardScaler` on the input DataFrame to infer a `StandardScalerModel`. The returned model is a Transformer.

Invoke the transform method of `StandardScalerModel` on the input DataFrame to create a new DataFrame that has a new column (`outputCol`): for each record, the value of the new column is the normalized version of the input vector column. It has also all the columns of the input DataFrame.

:::{.callout-note collapse="true"}
## Example
Consider an input DataFrame with four columns: create a new DataFrame with a new column containing the normalized version of the vector column features. Set the name of the new column to scaledFeatures.

**Original DataFrame**

|colA|colB|colC|features|
|-|-|-|-|-|
|$1$|$4.5$|True|$[4.5,1.0]$|
|$2$|$0.6$|True|$[0.6,1.0]$|
|$3$|$1.5$|False|$[1.5,0.0]$|
|$4$|$12.1$|True|$[12.1,1.0]$|
|$5$|$0.0$|True|$[0.0,1.0]$|

**Transformed DataFrame**

|colA|colB|colC|features|scaledFeatures|
|-|-|-|-|-|-|
|$1$|$4.5$|True|$[4.5,1.0]$|$[0.903,2.236]$|
|$2$|$0.6$|True|$[0.6,1.0]$|$[0.120,2.236]$|
|$3$|$1.5$|False|$[1.5,0.0]$|$[0.301,0.0]$|
|$4$|$12.1$|True|$[12.1,1.0]$|$[2.428,2.236]$|
|$5$|$0.0$|True|$[0.0,1.0]$|$[0.0,2.236]$|

```python
from pyspark.mllib.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StandardScaler
# input and output folders
inputPath = "data/exampleDataAssembler.csv“

# Create a DataFrame from the input data
inputDF = spark.read.load(
    inputPath,
    format="csv",
    header=True,
    inferSchema=True
)

# Create a VectorAssembler that combines columns colB and colC
# The new vetor column is called features
myVectorAssembler = VectorAssembler(
    inputCols = ['colB', 'colC'],
    outputCol = 'features'
)

# Apply myVectorAssembler on the input DataFrame
transformedDF = myVectorAssembler.transform(inputDF)

# Create a Standard Scaler to scale the content of features
myScaler = StandardScaler(
    inputCol="features",
    outputCol="scaledFeatures"
)

# Compute summary statistics by fitting the StandardScaler
# Before normalizing the content of the data we need to compute mean and
# standard deviation of the analyzed data
scalerModel = myScaler.fit(transformedDF)

# Apply myScaler on the input column features
scaledDF = scalerModel.transform(transformedDF)
```

:::

## Categorical columns
Frequently the input data are characterized by categorical attributes (i.e., string columns), and the class label of the classification problem is a categorical attribute. The Spark MLlib classification and regression algorithms work only with numerical values, so categorical columns must be mapped to double values.