---
title: "How to submit/execute a Spark application"
---

# Spark submit
Spark programs are executed (submitted) by using the `spark-submit` command. It is a command line program, characterized by a set of parameters (e.g., the name of the jar file containing all the classes of the Spark application we want to execute, the name of the Driver class, the parameters of the Spark application).

`spark-submit` has also two parameters that are used to specify where the application is executed

## Options
### `--master`

```bash
--master
```

It specifies which environment/scheduler is used to execute the application

| | |
|---|---|
| `spark://host:port` | The spark scheduler is used |
| `mesos://host:port` | The memos scheduler is used |
| `yarn` | The YARN scheduler (i.e., the one of Hadoop) |
| `local` | The application is executed exclusively on the local PC |
: {tbl-colwidths="[20,80]"}

### `--deploy-mode`

```bash
--deploy-mode
```

It specifies where the Driver is launched/executed

| | |
|---|---|
| `client` | The driver is launched locally (in the "local" PC executing spark-submit) |
| `cluster` | The driver is launched on one node of the cluster |
: {tbl-colwidths="[20,80]"}

# Cluster deplyment mode

![Deployment mode](images/10_intro_spark/cluster_deployment_mode.png){width=80%}